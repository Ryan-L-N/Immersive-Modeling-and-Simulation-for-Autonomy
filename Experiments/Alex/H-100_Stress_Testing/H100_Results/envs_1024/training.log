[3gH    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H   [INFO] Using python from: /home/t2user/miniconda3/envs/env_isaaclab/bin/python
[INFO][AppLauncher]: Using device: cuda:0
[INFO][AppLauncher]: Loading experience file: /home/t2user/IsaacLab/apps/isaaclab.python.headless.kit
Loading user config located at: '/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/isaacsim/kit/data/Kit/Isaac-Sim/5.1/user.config.json'
[Info] [carb] Logging to file: /home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/isaacsim/kit/logs/Kit/Isaac-Sim/5.1/kit_20260212_221346.log
2026-02-13T03:13:46Z [153ms] [Warning] [omni.usd_config.extension] Enable omni.materialx.libs extension to use MaterialX
2026-02-13T03:13:46Z [601ms] [Warning] [omni.platforminfo.plugin] failed to open the default display.  Can't verify X Server version.
2026-02-13T03:13:47Z [850ms] [Warning] [carb] Acquiring non optional plugin interface which is not listed as dependency: [omni::physx::IPhysxBenchmarks v1.0] (plugin: <default plugin>), by client: omni.physics.physx.plugin. Add it to CARB_PLUGIN_IMPL_DEPS() macro of a client.
2026-02-13T03:13:47Z [864ms] [Warning] [omni.isaac.dynamic_control] omni.isaac.dynamic_control is deprecated as of Isaac Sim 4.5. No action is needed from end-users.

|---------------------------------------------------------------------------------------------|
| Driver Version: 580.126.16    | Graphics API: Vulkan
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|     |                                  |        |     |            | Bus-ID    |            |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA H100 NVL                  | Yes: 0 |     | 95830   MB | 10de      | 0          |
|     |                                  |        |     |            | 2321      | 64de321d.. |
|     |                                  |        |     |            | 40        |            |
|=============================================================================================|
| OS: 22.04.5 LTS (Jammy Jellyfish) ubuntu, Version: 22.04.5, Kernel: 5.15.0-170-generic
| Processor: INTEL(R) XEON(R) PLATINUM 8581V
| Cores: 60 | Logical Cores: 120
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 1031732 | Free Memory: 1022644
| Total Page/Swap (MB): 2047 | Free Page/Swap: 2047
|---------------------------------------------------------------------------------------------|
2026-02-13T03:13:50Z [4,119ms] [Warning] [gpu.foundation.plugin] ECC is enabled on physical device 0
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.anymal_c.rough_env_cfg:AnymalCRoughEnvCfg
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.anymal_c.agents.rsl_rl_ppo_cfg:AnymalCRoughPPORunnerCfg
[INFO] Logging experiment in directory: /home/t2user/IsaacLab/logs/rsl_rl/anymal_c_rough
Exact experiment name requested from command line: 2026-02-12_22-13-53

[36m======================================================================================[0m
[36m[1m[INFO][IsaacLab]: Logging to file: /tmp/isaaclab/logs/isaaclab_2026-02-12_22-13-53.log[0m
[36m======================================================================================[0m

[33m22:13:53 [simulation_context.py] WARNING: The `enable_external_forces_every_iteration` parameter in the PhysxCfg is set to False. If you are experiencing noisy velocities, consider enabling this flag. You may need to slightly increase the number of velocity iterations (setting it to 1 or 2 rather than 0), together with this flag, to improve the accuracy of velocity updates.[0m
[INFO]: Base environment:
	Environment device    : cuda:0
	Environment seed      : 42
	Physics step-size     : 0.005
	Rendering step-size   : 0.02
	Environment step-size : 0.02
[INFO] Generating terrains based on curriculum took : 1.213249 seconds
[INFO]: Time taken for scene creation : 3.185006 seconds
[INFO]: Scene manager:  <class InteractiveScene>
	Number of environments: 1024
	Environment spacing   : 2.5
	Source prim name      : /World/envs/env_0
	Global prim paths     : ['/World/ground']
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
[INFO]: Time taken for simulation start : 2.952133 seconds
[INFO] Command Manager:  <CommandManager> contains 1 active terms.
+------------------------------------------------+
|              Active Command Terms              |
+-------+---------------+------------------------+
| Index | Name          |          Type          |
+-------+---------------+------------------------+
|   0   | base_velocity | UniformVelocityCommand |
+-------+---------------+------------------------+

[INFO] Event Manager:  <EventManager> contains 3 active terms.
+--------------------------------------+
| Active Event Terms in Mode: 'startup' |
+----------+---------------------------+
|  Index   | Name                      |
+----------+---------------------------+
|    0     | physics_material          |
|    1     | add_base_mass             |
|    2     | base_com                  |
+----------+---------------------------+
+---------------------------------------+
|  Active Event Terms in Mode: 'reset'  |
+--------+------------------------------+
| Index  | Name                         |
+--------+------------------------------+
|   0    | base_external_force_torque   |
|   1    | reset_base                   |
|   2    | reset_robot_joints           |
+--------+------------------------------+
+----------------------------------------------+
|    Active Event Terms in Mode: 'interval'    |
+-------+------------+-------------------------+
| Index | Name       | Interval time range (s) |
+-------+------------+-------------------------+
|   0   | push_robot |       (10.0, 15.0)      |
+-------+------------+-------------------------+

[INFO] Recorder Manager:  <RecorderManager> contains 0 active terms.
+---------------------+
| Active Recorder Terms |
+-----------+---------+
|   Index   | Name    |
+-----------+---------+
+-----------+---------+

[INFO] Action Manager:  <ActionManager> contains 1 active terms.
+------------------------------------+
|  Active Action Terms (shape: 12)   |
+--------+-------------+-------------+
| Index  | Name        |   Dimension |
+--------+-------------+-------------+
|   0    | joint_pos   |          12 |
+--------+-------------+-------------+

[INFO] Observation Manager: <ObservationManager> contains 1 groups.
+----------------------------------------------------------+
| Active Observation Terms in Group: 'policy' (shape: (235,)) |
+-----------+--------------------------------+-------------+
|   Index   | Name                           |    Shape    |
+-----------+--------------------------------+-------------+
|     0     | base_lin_vel                   |     (3,)    |
|     1     | base_ang_vel                   |     (3,)    |
|     2     | projected_gravity              |     (3,)    |
|     3     | velocity_commands              |     (3,)    |
|     4     | joint_pos                      |    (12,)    |
|     5     | joint_vel                      |    (12,)    |
|     6     | actions                        |    (12,)    |
|     7     | height_scan                    |    (187,)   |
+-----------+--------------------------------+-------------+

[INFO] Termination Manager:  <TerminationManager> contains 2 active terms.
+---------------------------------+
|     Active Termination Terms    |
+-------+--------------+----------+
| Index | Name         | Time Out |
+-------+--------------+----------+
|   0   | time_out     |   True   |
|   1   | base_contact |  False   |
+-------+--------------+----------+

[INFO] Reward Manager:  <RewardManager> contains 11 active terms.
+-----------------------------------------+
|           Active Reward Terms           |
+-------+----------------------+----------+
| Index | Name                 |   Weight |
+-------+----------------------+----------+
|   0   | track_lin_vel_xy_exp |      1.0 |
|   1   | track_ang_vel_z_exp  |      0.5 |
|   2   | lin_vel_z_l2         |     -2.0 |
|   3   | ang_vel_xy_l2        |    -0.05 |
|   4   | dof_torques_l2       |   -1e-05 |
|   5   | dof_acc_l2           | -2.5e-07 |
|   6   | action_rate_l2       |    -0.01 |
|   7   | feet_air_time        |    0.125 |
|   8   | undesired_contacts   |     -1.0 |
|   9   | flat_orientation_l2  |      0.0 |
|   10  | dof_pos_limits       |      0.0 |
+-------+----------------------+----------+

[INFO] Curriculum Manager:  <CurriculumManager> contains 1 active terms.
+---------------------------+
|  Active Curriculum Terms  |
+--------+------------------+
| Index  | Name             |
+--------+------------------+
|   0    | terrain_levels   |
+--------+------------------+

[INFO]: Completed setting up the environment...
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
2026-02-13T03:14:00Z [14,330ms] [Warning] [omni.physx.fabric.plugin] FabricManager::initializePointInstancer mismatched prototypes on point instancer: /Visuals/Command/velocity_goal.
2026-02-13T03:14:00Z [14,330ms] [Warning] [omni.physx.fabric.plugin] FabricManager::initializePointInstancer mismatched prototypes on point instancer: /Visuals/Command/velocity_current.
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/utils/utils.py:245: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'policy' key. As an observation group with the name 'policy' was found, this is assumed to be the observation set. Consider adding the 'policy' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/utils/utils.py:291: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'critic' key. As the configuration for 'critic' is missing, the observations from the 'policy' set are used. Consider adding the 'critic' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
--------------------------------------------------------------------------------
Resolved observation sets: 
	 policy :  ['policy']
	 critic :  ['policy']
--------------------------------------------------------------------------------
Actor MLP: MLP(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=12, bias=True)
)
Critic MLP: MLP(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
################################################################################
                       [1m Learning iteration 0/10 [0m                        

                       Computation: 5255 steps/s (collection: 4.220s, learning 0.457s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0536
               Mean surrogate loss: 0.0314
                 Mean entropy loss: 17.0065
                       Mean reward: -0.46
               Mean episode length: 10.95
Episode_Reward/track_lin_vel_xy_exp: 0.0030
Episode_Reward/track_ang_vel_z_exp: 0.0022
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0027
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0105
     Episode_Reward/action_rate_l2: -0.0026
      Episode_Reward/feet_air_time: -0.0003
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5452
Metrics/base_velocity/error_vel_xy: 0.0163
Metrics/base_velocity/error_vel_yaw: 0.0135
      Episode_Termination/time_out: 0.0107
  Episode_Termination/base_contact: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 4.68s
                      Time elapsed: 00:00:04
                               ETA: 00:00:46

Could not find git repository in /home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/__init__.py. Skipping.
Storing git diff for 'IsaacLab' in: /home/t2user/IsaacLab/logs/rsl_rl/anymal_c_rough/2026-02-12_22-13-53/git/IsaacLab.diff
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
################################################################################
                       [1m Learning iteration 1/10 [0m                        

                       Computation: 9072 steps/s (collection: 2.445s, learning 0.264s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0297
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 16.9916
                       Mean reward: -0.93
               Mean episode length: 27.51
Episode_Reward/track_lin_vel_xy_exp: 0.0054
Episode_Reward/track_ang_vel_z_exp: 0.0057
       Episode_Reward/lin_vel_z_l2: -0.0197
      Episode_Reward/ang_vel_xy_l2: -0.0082
     Episode_Reward/dof_torques_l2: -0.0048
         Episode_Reward/dof_acc_l2: -0.0195
     Episode_Reward/action_rate_l2: -0.0084
      Episode_Reward/feet_air_time: -0.0009
 Episode_Reward/undesired_contacts: -0.0071
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5267
Metrics/base_velocity/error_vel_xy: 0.0628
Metrics/base_velocity/error_vel_yaw: 0.0546
      Episode_Termination/time_out: 0.0255
  Episode_Termination/base_contact: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 2.71s
                      Time elapsed: 00:00:07
                               ETA: 00:00:33

################################################################################
                       [1m Learning iteration 2/10 [0m                        

                       Computation: 8764 steps/s (collection: 2.540s, learning 0.264s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 16.9116
                       Mean reward: -1.35
               Mean episode length: 42.90
Episode_Reward/track_lin_vel_xy_exp: 0.0121
Episode_Reward/track_ang_vel_z_exp: 0.0096
       Episode_Reward/lin_vel_z_l2: -0.0220
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.0079
         Episode_Reward/dof_acc_l2: -0.0313
     Episode_Reward/action_rate_l2: -0.0144
      Episode_Reward/feet_air_time: -0.0014
 Episode_Reward/undesired_contacts: -0.0173
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4862
Metrics/base_velocity/error_vel_xy: 0.1015
Metrics/base_velocity/error_vel_yaw: 0.0989
      Episode_Termination/time_out: 0.0449
  Episode_Termination/base_contact: 0.0267
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 2.80s
                      Time elapsed: 00:00:10
                               ETA: 00:00:27

################################################################################
                       [1m Learning iteration 3/10 [0m                        

                       Computation: 8877 steps/s (collection: 2.505s, learning 0.263s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 16.7929
                       Mean reward: -1.96
               Mean episode length: 67.06
Episode_Reward/track_lin_vel_xy_exp: 0.0109
Episode_Reward/track_ang_vel_z_exp: 0.0122
       Episode_Reward/lin_vel_z_l2: -0.0256
      Episode_Reward/ang_vel_xy_l2: -0.0226
     Episode_Reward/dof_torques_l2: -0.0115
         Episode_Reward/dof_acc_l2: -0.0413
     Episode_Reward/action_rate_l2: -0.0204
      Episode_Reward/feet_air_time: -0.0022
 Episode_Reward/undesired_contacts: -0.0241
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4519
Metrics/base_velocity/error_vel_xy: 0.1590
Metrics/base_velocity/error_vel_yaw: 0.1420
      Episode_Termination/time_out: 0.0635
  Episode_Termination/base_contact: 0.0437
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 2.77s
                      Time elapsed: 00:00:12
                               ETA: 00:00:22

################################################################################
                       [1m Learning iteration 4/10 [0m                        

                       Computation: 8749 steps/s (collection: 2.540s, learning 0.268s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 16.6799
                       Mean reward: -2.27
               Mean episode length: 83.95
Episode_Reward/track_lin_vel_xy_exp: 0.0191
Episode_Reward/track_ang_vel_z_exp: 0.0154
       Episode_Reward/lin_vel_z_l2: -0.0246
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.0134
         Episode_Reward/dof_acc_l2: -0.0413
     Episode_Reward/action_rate_l2: -0.0238
      Episode_Reward/feet_air_time: -0.0026
 Episode_Reward/undesired_contacts: -0.0341
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4163
Metrics/base_velocity/error_vel_xy: 0.1682
Metrics/base_velocity/error_vel_yaw: 0.1591
      Episode_Termination/time_out: 0.0841
  Episode_Termination/base_contact: 0.0581
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 2.81s
                      Time elapsed: 00:00:15
                               ETA: 00:00:18

################################################################################
                       [1m Learning iteration 5/10 [0m                        

                       Computation: 8836 steps/s (collection: 2.519s, learning 0.262s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 16.5621
                       Mean reward: -2.70
               Mean episode length: 108.55
Episode_Reward/track_lin_vel_xy_exp: 0.0198
Episode_Reward/track_ang_vel_z_exp: 0.0185
       Episode_Reward/lin_vel_z_l2: -0.0226
      Episode_Reward/ang_vel_xy_l2: -0.0269
     Episode_Reward/dof_torques_l2: -0.0173
         Episode_Reward/dof_acc_l2: -0.0503
     Episode_Reward/action_rate_l2: -0.0305
      Episode_Reward/feet_air_time: -0.0032
 Episode_Reward/undesired_contacts: -0.0574
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3843
Metrics/base_velocity/error_vel_xy: 0.2236
Metrics/base_velocity/error_vel_yaw: 0.2058
      Episode_Termination/time_out: 0.0997
  Episode_Termination/base_contact: 0.0702
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 2.78s
                      Time elapsed: 00:00:18
                               ETA: 00:00:15

################################################################################
                       [1m Learning iteration 6/10 [0m                        

                       Computation: 8801 steps/s (collection: 2.529s, learning 0.263s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 16.4184
                       Mean reward: -3.00
               Mean episode length: 128.69
Episode_Reward/track_lin_vel_xy_exp: 0.0321
Episode_Reward/track_ang_vel_z_exp: 0.0226
       Episode_Reward/lin_vel_z_l2: -0.0279
      Episode_Reward/ang_vel_xy_l2: -0.0311
     Episode_Reward/dof_torques_l2: -0.0192
         Episode_Reward/dof_acc_l2: -0.0506
     Episode_Reward/action_rate_l2: -0.0326
      Episode_Reward/feet_air_time: -0.0033
 Episode_Reward/undesired_contacts: -0.0491
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3435
Metrics/base_velocity/error_vel_xy: 0.2137
Metrics/base_velocity/error_vel_yaw: 0.2093
      Episode_Termination/time_out: 0.1242
  Episode_Termination/base_contact: 0.0803
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 2.79s
                      Time elapsed: 00:00:21
                               ETA: 00:00:12

################################################################################
                       [1m Learning iteration 7/10 [0m                        

                       Computation: 8962 steps/s (collection: 2.481s, learning 0.261s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0083
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 16.2741
                       Mean reward: -3.36
               Mean episode length: 147.69
Episode_Reward/track_lin_vel_xy_exp: 0.0265
Episode_Reward/track_ang_vel_z_exp: 0.0279
       Episode_Reward/lin_vel_z_l2: -0.0298
      Episode_Reward/ang_vel_xy_l2: -0.0367
     Episode_Reward/dof_torques_l2: -0.0215
         Episode_Reward/dof_acc_l2: -0.0617
     Episode_Reward/action_rate_l2: -0.0369
      Episode_Reward/feet_air_time: -0.0040
 Episode_Reward/undesired_contacts: -0.0534
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3104
Metrics/base_velocity/error_vel_xy: 0.2672
Metrics/base_velocity/error_vel_yaw: 0.2280
      Episode_Termination/time_out: 0.1434
  Episode_Termination/base_contact: 0.0906
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.74s
                      Time elapsed: 00:00:24
                               ETA: 00:00:09

################################################################################
                       [1m Learning iteration 8/10 [0m                        

                       Computation: 8968 steps/s (collection: 2.485s, learning 0.256s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0075
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 16.1028
                       Mean reward: -3.63
               Mean episode length: 164.14
Episode_Reward/track_lin_vel_xy_exp: 0.0414
Episode_Reward/track_ang_vel_z_exp: 0.0266
       Episode_Reward/lin_vel_z_l2: -0.0319
      Episode_Reward/ang_vel_xy_l2: -0.0353
     Episode_Reward/dof_torques_l2: -0.0237
         Episode_Reward/dof_acc_l2: -0.0588
     Episode_Reward/action_rate_l2: -0.0389
      Episode_Reward/feet_air_time: -0.0037
 Episode_Reward/undesired_contacts: -0.0596
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2778
Metrics/base_velocity/error_vel_xy: 0.2667
Metrics/base_velocity/error_vel_yaw: 0.2581
      Episode_Termination/time_out: 0.1670
  Episode_Termination/base_contact: 0.0961
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 2.74s
                      Time elapsed: 00:00:26
                               ETA: 00:00:05

################################################################################
                       [1m Learning iteration 9/10 [0m                        

                       Computation: 8965 steps/s (collection: 2.474s, learning 0.268s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 15.9578
                       Mean reward: -3.82
               Mean episode length: 183.76
Episode_Reward/track_lin_vel_xy_exp: 0.0602
Episode_Reward/track_ang_vel_z_exp: 0.0343
       Episode_Reward/lin_vel_z_l2: -0.0302
      Episode_Reward/ang_vel_xy_l2: -0.0426
     Episode_Reward/dof_torques_l2: -0.0291
         Episode_Reward/dof_acc_l2: -0.0674
     Episode_Reward/action_rate_l2: -0.0479
      Episode_Reward/feet_air_time: -0.0046
 Episode_Reward/undesired_contacts: -0.0703
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2498
Metrics/base_velocity/error_vel_xy: 0.2799
Metrics/base_velocity/error_vel_yaw: 0.3141
      Episode_Termination/time_out: 0.1848
  Episode_Termination/base_contact: 0.1025
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 2.74s
                      Time elapsed: 00:00:29
                               ETA: 00:00:02

Training time: 30.88 seconds
Exit code: 0

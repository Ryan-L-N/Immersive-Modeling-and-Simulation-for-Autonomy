[3gH    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H   [INFO] Using python from: /home/t2user/miniconda3/envs/env_isaaclab/bin/python
[INFO][AppLauncher]: Using device: cuda:0
[INFO][AppLauncher]: Loading experience file: /home/t2user/IsaacLab/apps/isaaclab.python.headless.kit
Loading user config located at: '/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/isaacsim/kit/data/Kit/Isaac-Sim/5.1/user.config.json'
[Info] [carb] Logging to file: /home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/isaacsim/kit/logs/Kit/Isaac-Sim/5.1/kit_20260212_221443.log
2026-02-13T03:14:43Z [164ms] [Warning] [omni.usd_config.extension] Enable omni.materialx.libs extension to use MaterialX
2026-02-13T03:14:44Z [675ms] [Warning] [omni.platforminfo.plugin] failed to open the default display.  Can't verify X Server version.
2026-02-13T03:14:44Z [928ms] [Warning] [carb] Acquiring non optional plugin interface which is not listed as dependency: [omni::physx::IPhysxBenchmarks v1.0] (plugin: <default plugin>), by client: omni.physics.physx.plugin. Add it to CARB_PLUGIN_IMPL_DEPS() macro of a client.
2026-02-13T03:14:44Z [943ms] [Warning] [omni.isaac.dynamic_control] omni.isaac.dynamic_control is deprecated as of Isaac Sim 4.5. No action is needed from end-users.

|---------------------------------------------------------------------------------------------|
| Driver Version: 580.126.16    | Graphics API: Vulkan
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|     |                                  |        |     |            | Bus-ID    |            |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA H100 NVL                  | Yes: 0 |     | 95830   MB | 10de      | 0          |
|     |                                  |        |     |            | 2321      | 64de321d.. |
|     |                                  |        |     |            | 40        |            |
|=============================================================================================|
| OS: 22.04.5 LTS (Jammy Jellyfish) ubuntu, Version: 22.04.5, Kernel: 5.15.0-170-generic
| Processor: INTEL(R) XEON(R) PLATINUM 8581V
| Cores: 60 | Logical Cores: 120
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 1031732 | Free Memory: 1022634
| Total Page/Swap (MB): 2047 | Free Page/Swap: 2047
|---------------------------------------------------------------------------------------------|
2026-02-13T03:14:48Z [5,381ms] [Warning] [gpu.foundation.plugin] ECC is enabled on physical device 0
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.anymal_c.rough_env_cfg:AnymalCRoughEnvCfg
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.anymal_c.agents.rsl_rl_ppo_cfg:AnymalCRoughPPORunnerCfg
[INFO] Logging experiment in directory: /home/t2user/IsaacLab/logs/rsl_rl/anymal_c_rough
Exact experiment name requested from command line: 2026-02-12_22-14-50

[36m======================================================================================[0m
[36m[1m[INFO][IsaacLab]: Logging to file: /tmp/isaaclab/logs/isaaclab_2026-02-12_22-14-50.log[0m
[36m======================================================================================[0m

[33m22:14:50 [simulation_context.py] WARNING: The `enable_external_forces_every_iteration` parameter in the PhysxCfg is set to False. If you are experiencing noisy velocities, consider enabling this flag. You may need to slightly increase the number of velocity iterations (setting it to 1 or 2 rather than 0), together with this flag, to improve the accuracy of velocity updates.[0m
[INFO]: Base environment:
	Environment device    : cuda:0
	Environment seed      : 42
	Physics step-size     : 0.005
	Rendering step-size   : 0.02
	Environment step-size : 0.02
[INFO] Generating terrains based on curriculum took : 1.230467 seconds
[INFO]: Time taken for scene creation : 4.819807 seconds
[INFO]: Scene manager:  <class InteractiveScene>
	Number of environments: 2048
	Environment spacing   : 2.5
	Source prim name      : /World/envs/env_0
	Global prim paths     : ['/World/ground']
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
[INFO]: Time taken for simulation start : 5.299026 seconds
[INFO] Command Manager:  <CommandManager> contains 1 active terms.
+------------------------------------------------+
|              Active Command Terms              |
+-------+---------------+------------------------+
| Index | Name          |          Type          |
+-------+---------------+------------------------+
|   0   | base_velocity | UniformVelocityCommand |
+-------+---------------+------------------------+

[INFO] Event Manager:  <EventManager> contains 3 active terms.
+--------------------------------------+
| Active Event Terms in Mode: 'startup' |
+----------+---------------------------+
|  Index   | Name                      |
+----------+---------------------------+
|    0     | physics_material          |
|    1     | add_base_mass             |
|    2     | base_com                  |
+----------+---------------------------+
+---------------------------------------+
|  Active Event Terms in Mode: 'reset'  |
+--------+------------------------------+
| Index  | Name                         |
+--------+------------------------------+
|   0    | base_external_force_torque   |
|   1    | reset_base                   |
|   2    | reset_robot_joints           |
+--------+------------------------------+
+----------------------------------------------+
|    Active Event Terms in Mode: 'interval'    |
+-------+------------+-------------------------+
| Index | Name       | Interval time range (s) |
+-------+------------+-------------------------+
|   0   | push_robot |       (10.0, 15.0)      |
+-------+------------+-------------------------+

[INFO] Recorder Manager:  <RecorderManager> contains 0 active terms.
+---------------------+
| Active Recorder Terms |
+-----------+---------+
|   Index   | Name    |
+-----------+---------+
+-----------+---------+

[INFO] Action Manager:  <ActionManager> contains 1 active terms.
+------------------------------------+
|  Active Action Terms (shape: 12)   |
+--------+-------------+-------------+
| Index  | Name        |   Dimension |
+--------+-------------+-------------+
|   0    | joint_pos   |          12 |
+--------+-------------+-------------+

[INFO] Observation Manager: <ObservationManager> contains 1 groups.
+----------------------------------------------------------+
| Active Observation Terms in Group: 'policy' (shape: (235,)) |
+-----------+--------------------------------+-------------+
|   Index   | Name                           |    Shape    |
+-----------+--------------------------------+-------------+
|     0     | base_lin_vel                   |     (3,)    |
|     1     | base_ang_vel                   |     (3,)    |
|     2     | projected_gravity              |     (3,)    |
|     3     | velocity_commands              |     (3,)    |
|     4     | joint_pos                      |    (12,)    |
|     5     | joint_vel                      |    (12,)    |
|     6     | actions                        |    (12,)    |
|     7     | height_scan                    |    (187,)   |
+-----------+--------------------------------+-------------+

[INFO] Termination Manager:  <TerminationManager> contains 2 active terms.
+---------------------------------+
|     Active Termination Terms    |
+-------+--------------+----------+
| Index | Name         | Time Out |
+-------+--------------+----------+
|   0   | time_out     |   True   |
|   1   | base_contact |  False   |
+-------+--------------+----------+

[INFO] Reward Manager:  <RewardManager> contains 11 active terms.
+-----------------------------------------+
|           Active Reward Terms           |
+-------+----------------------+----------+
| Index | Name                 |   Weight |
+-------+----------------------+----------+
|   0   | track_lin_vel_xy_exp |      1.0 |
|   1   | track_ang_vel_z_exp  |      0.5 |
|   2   | lin_vel_z_l2         |     -2.0 |
|   3   | ang_vel_xy_l2        |    -0.05 |
|   4   | dof_torques_l2       |   -1e-05 |
|   5   | dof_acc_l2           | -2.5e-07 |
|   6   | action_rate_l2       |    -0.01 |
|   7   | feet_air_time        |    0.125 |
|   8   | undesired_contacts   |     -1.0 |
|   9   | flat_orientation_l2  |      0.0 |
|   10  | dof_pos_limits       |      0.0 |
+-------+----------------------+----------+

[INFO] Curriculum Manager:  <CurriculumManager> contains 1 active terms.
+---------------------------+
|  Active Curriculum Terms  |
+--------+------------------+
| Index  | Name             |
+--------+------------------+
|   0    | terrain_levels   |
+--------+------------------+

[INFO]: Completed setting up the environment...
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
2026-02-13T03:15:01Z [18,570ms] [Warning] [omni.physx.fabric.plugin] FabricManager::initializePointInstancer mismatched prototypes on point instancer: /Visuals/Command/velocity_current.
2026-02-13T03:15:01Z [18,570ms] [Warning] [omni.physx.fabric.plugin] FabricManager::initializePointInstancer mismatched prototypes on point instancer: /Visuals/Command/velocity_goal.
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/utils/utils.py:245: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'policy' key. As an observation group with the name 'policy' was found, this is assumed to be the observation set. Consider adding the 'policy' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/utils/utils.py:291: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'critic' key. As the configuration for 'critic' is missing, the observations from the 'policy' set are used. Consider adding the 'critic' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
--------------------------------------------------------------------------------
Resolved observation sets: 
	 policy :  ['policy']
	 critic :  ['policy']
--------------------------------------------------------------------------------
Actor MLP: MLP(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=12, bias=True)
)
Critic MLP: MLP(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
################################################################################
                       [1m Learning iteration 0/10 [0m                        

                       Computation: 9784 steps/s (collection: 4.573s, learning 0.451s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0505
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 17.0107
                       Mean reward: -0.52
               Mean episode length: 12.45
Episode_Reward/track_lin_vel_xy_exp: 0.0028
Episode_Reward/track_ang_vel_z_exp: 0.0022
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0029
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0110
     Episode_Reward/action_rate_l2: -0.0030
      Episode_Reward/feet_air_time: -0.0003
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4983
Metrics/base_velocity/error_vel_xy: 0.0178
Metrics/base_velocity/error_vel_yaw: 0.0151
      Episode_Termination/time_out: 0.0099
  Episode_Termination/base_contact: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 5.02s
                      Time elapsed: 00:00:05
                               ETA: 00:00:50

Could not find git repository in /home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/__init__.py. Skipping.
Storing git diff for 'IsaacLab' in: /home/t2user/IsaacLab/logs/rsl_rl/anymal_c_rough/2026-02-12_22-14-50/git/IsaacLab.diff
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
################################################################################
                       [1m Learning iteration 1/10 [0m                        

                       Computation: 15554 steps/s (collection: 2.890s, learning 0.270s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 16.9901
                       Mean reward: -1.19
               Mean episode length: 34.47
Episode_Reward/track_lin_vel_xy_exp: 0.0063
Episode_Reward/track_ang_vel_z_exp: 0.0055
       Episode_Reward/lin_vel_z_l2: -0.0195
      Episode_Reward/ang_vel_xy_l2: -0.0108
     Episode_Reward/dof_torques_l2: -0.0048
         Episode_Reward/dof_acc_l2: -0.0222
     Episode_Reward/action_rate_l2: -0.0086
      Episode_Reward/feet_air_time: -0.0009
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4674
Metrics/base_velocity/error_vel_xy: 0.0651
Metrics/base_velocity/error_vel_yaw: 0.0581
      Episode_Termination/time_out: 0.0280
  Episode_Termination/base_contact: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 3.16s
                      Time elapsed: 00:00:08
                               ETA: 00:00:36

################################################################################
                       [1m Learning iteration 2/10 [0m                        

                       Computation: 15184 steps/s (collection: 2.965s, learning 0.272s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0192
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 16.8382
                       Mean reward: -1.62
               Mean episode length: 56.57
Episode_Reward/track_lin_vel_xy_exp: 0.0110
Episode_Reward/track_ang_vel_z_exp: 0.0100
       Episode_Reward/lin_vel_z_l2: -0.0205
      Episode_Reward/ang_vel_xy_l2: -0.0162
     Episode_Reward/dof_torques_l2: -0.0082
         Episode_Reward/dof_acc_l2: -0.0311
     Episode_Reward/action_rate_l2: -0.0147
      Episode_Reward/feet_air_time: -0.0016
 Episode_Reward/undesired_contacts: -0.0102
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4245
Metrics/base_velocity/error_vel_xy: 0.1030
Metrics/base_velocity/error_vel_yaw: 0.0928
      Episode_Termination/time_out: 0.0485
  Episode_Termination/base_contact: 0.0358
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 3.24s
                      Time elapsed: 00:00:11
                               ETA: 00:00:30

################################################################################
                       [1m Learning iteration 3/10 [0m                        

                       Computation: 15164 steps/s (collection: 2.973s, learning 0.268s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 16.6586
                       Mean reward: -1.97
               Mean episode length: 77.87
Episode_Reward/track_lin_vel_xy_exp: 0.0137
Episode_Reward/track_ang_vel_z_exp: 0.0122
       Episode_Reward/lin_vel_z_l2: -0.0232
      Episode_Reward/ang_vel_xy_l2: -0.0204
     Episode_Reward/dof_torques_l2: -0.0109
         Episode_Reward/dof_acc_l2: -0.0363
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0022
 Episode_Reward/undesired_contacts: -0.0164
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3901
Metrics/base_velocity/error_vel_xy: 0.1334
Metrics/base_velocity/error_vel_yaw: 0.1309
      Episode_Termination/time_out: 0.0701
  Episode_Termination/base_contact: 0.0486
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 3.24s
                      Time elapsed: 00:00:14
                               ETA: 00:00:25

################################################################################
                       [1m Learning iteration 4/10 [0m                        

                       Computation: 15278 steps/s (collection: 2.950s, learning 0.267s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 16.4866
                       Mean reward: -2.40
               Mean episode length: 99.19
Episode_Reward/track_lin_vel_xy_exp: 0.0191
Episode_Reward/track_ang_vel_z_exp: 0.0155
       Episode_Reward/lin_vel_z_l2: -0.0233
      Episode_Reward/ang_vel_xy_l2: -0.0243
     Episode_Reward/dof_torques_l2: -0.0145
         Episode_Reward/dof_acc_l2: -0.0426
     Episode_Reward/action_rate_l2: -0.0250
      Episode_Reward/feet_air_time: -0.0028
 Episode_Reward/undesired_contacts: -0.0255
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3584
Metrics/base_velocity/error_vel_xy: 0.1744
Metrics/base_velocity/error_vel_yaw: 0.1734
      Episode_Termination/time_out: 0.0926
  Episode_Termination/base_contact: 0.0566
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 3.22s
                      Time elapsed: 00:00:17
                               ETA: 00:00:21

################################################################################
                       [1m Learning iteration 5/10 [0m                        

                       Computation: 15293 steps/s (collection: 2.944s, learning 0.269s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0097
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 16.3131
                       Mean reward: -2.77
               Mean episode length: 123.32
Episode_Reward/track_lin_vel_xy_exp: 0.0212
Episode_Reward/track_ang_vel_z_exp: 0.0194
       Episode_Reward/lin_vel_z_l2: -0.0243
      Episode_Reward/ang_vel_xy_l2: -0.0284
     Episode_Reward/dof_torques_l2: -0.0184
         Episode_Reward/dof_acc_l2: -0.0486
     Episode_Reward/action_rate_l2: -0.0303
      Episode_Reward/feet_air_time: -0.0033
 Episode_Reward/undesired_contacts: -0.0342
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3276
Metrics/base_velocity/error_vel_xy: 0.2221
Metrics/base_velocity/error_vel_yaw: 0.2075
      Episode_Termination/time_out: 0.1147
  Episode_Termination/base_contact: 0.0639
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 3.21s
                      Time elapsed: 00:00:21
                               ETA: 00:00:17

################################################################################
                       [1m Learning iteration 6/10 [0m                        

                       Computation: 15357 steps/s (collection: 2.933s, learning 0.268s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 16.1358
                       Mean reward: -3.15
               Mean episode length: 143.06
Episode_Reward/track_lin_vel_xy_exp: 0.0287
Episode_Reward/track_ang_vel_z_exp: 0.0226
       Episode_Reward/lin_vel_z_l2: -0.0276
      Episode_Reward/ang_vel_xy_l2: -0.0321
     Episode_Reward/dof_torques_l2: -0.0206
         Episode_Reward/dof_acc_l2: -0.0549
     Episode_Reward/action_rate_l2: -0.0343
      Episode_Reward/feet_air_time: -0.0034
 Episode_Reward/undesired_contacts: -0.0449
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2969
Metrics/base_velocity/error_vel_xy: 0.2451
Metrics/base_velocity/error_vel_yaw: 0.2332
      Episode_Termination/time_out: 0.1367
  Episode_Termination/base_contact: 0.0720
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 3.20s
                      Time elapsed: 00:00:24
                               ETA: 00:00:13

################################################################################
                       [1m Learning iteration 7/10 [0m                        

                       Computation: 15331 steps/s (collection: 2.936s, learning 0.269s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0075
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 15.9398
                       Mean reward: -3.44
               Mean episode length: 163.50
Episode_Reward/track_lin_vel_xy_exp: 0.0382
Episode_Reward/track_ang_vel_z_exp: 0.0289
       Episode_Reward/lin_vel_z_l2: -0.0255
      Episode_Reward/ang_vel_xy_l2: -0.0344
     Episode_Reward/dof_torques_l2: -0.0244
         Episode_Reward/dof_acc_l2: -0.0629
     Episode_Reward/action_rate_l2: -0.0397
      Episode_Reward/feet_air_time: -0.0043
 Episode_Reward/undesired_contacts: -0.0577
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2672
Metrics/base_velocity/error_vel_xy: 0.2610
Metrics/base_velocity/error_vel_yaw: 0.2541
      Episode_Termination/time_out: 0.1564
  Episode_Termination/base_contact: 0.0811
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 3.21s
                      Time elapsed: 00:00:27
                               ETA: 00:00:10

################################################################################
                       [1m Learning iteration 8/10 [0m                        

                       Computation: 15453 steps/s (collection: 2.910s, learning 0.270s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 15.7131
                       Mean reward: -3.76
               Mean episode length: 178.73
Episode_Reward/track_lin_vel_xy_exp: 0.0393
Episode_Reward/track_ang_vel_z_exp: 0.0284
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0370
     Episode_Reward/dof_torques_l2: -0.0260
         Episode_Reward/dof_acc_l2: -0.0652
     Episode_Reward/action_rate_l2: -0.0419
      Episode_Reward/feet_air_time: -0.0045
 Episode_Reward/undesired_contacts: -0.0551
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2315
Metrics/base_velocity/error_vel_xy: 0.2976
Metrics/base_velocity/error_vel_yaw: 0.2918
      Episode_Termination/time_out: 0.1774
  Episode_Termination/base_contact: 0.0904
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 3.18s
                      Time elapsed: 00:00:30
                               ETA: 00:00:06

################################################################################
                       [1m Learning iteration 9/10 [0m                        

                       Computation: 15662 steps/s (collection: 2.870s, learning 0.268s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 15.5414
                       Mean reward: -3.88
               Mean episode length: 206.67
Episode_Reward/track_lin_vel_xy_exp: 0.0572
Episode_Reward/track_ang_vel_z_exp: 0.0352
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0446
     Episode_Reward/dof_torques_l2: -0.0310
         Episode_Reward/dof_acc_l2: -0.0760
     Episode_Reward/action_rate_l2: -0.0499
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0560
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2034
Metrics/base_velocity/error_vel_xy: 0.3319
Metrics/base_velocity/error_vel_yaw: 0.3496
      Episode_Termination/time_out: 0.1949
  Episode_Termination/base_contact: 0.0991
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 3.14s
                      Time elapsed: 00:00:33
                               ETA: 00:00:03

Training time: 35.17 seconds
Exit code: 0

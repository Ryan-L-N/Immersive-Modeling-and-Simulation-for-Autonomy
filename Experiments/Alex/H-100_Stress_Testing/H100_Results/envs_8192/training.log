[3gH    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H   [INFO] Using python from: /home/t2user/miniconda3/envs/env_isaaclab/bin/python
[INFO][AppLauncher]: Using device: cuda:0
[INFO][AppLauncher]: Loading experience file: /home/t2user/IsaacLab/apps/isaaclab.python.headless.kit
Loading user config located at: '/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/isaacsim/kit/data/Kit/Isaac-Sim/5.1/user.config.json'
[Info] [carb] Logging to file: /home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/isaacsim/kit/logs/Kit/Isaac-Sim/5.1/kit_20260212_221711.log
2026-02-13T03:17:12Z [166ms] [Warning] [omni.usd_config.extension] Enable omni.materialx.libs extension to use MaterialX
2026-02-13T03:17:12Z [651ms] [Warning] [omni.platforminfo.plugin] failed to open the default display.  Can't verify X Server version.
2026-02-13T03:17:12Z [913ms] [Warning] [carb] Acquiring non optional plugin interface which is not listed as dependency: [omni::physx::IPhysxBenchmarks v1.0] (plugin: <default plugin>), by client: omni.physics.physx.plugin. Add it to CARB_PLUGIN_IMPL_DEPS() macro of a client.
2026-02-13T03:17:12Z [928ms] [Warning] [omni.isaac.dynamic_control] omni.isaac.dynamic_control is deprecated as of Isaac Sim 4.5. No action is needed from end-users.

|---------------------------------------------------------------------------------------------|
| Driver Version: 580.126.16    | Graphics API: Vulkan
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|     |                                  |        |     |            | Bus-ID    |            |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA H100 NVL                  | Yes: 0 |     | 95830   MB | 10de      | 0          |
|     |                                  |        |     |            | 2321      | 64de321d.. |
|     |                                  |        |     |            | 40        |            |
|=============================================================================================|
| OS: 22.04.5 LTS (Jammy Jellyfish) ubuntu, Version: 22.04.5, Kernel: 5.15.0-170-generic
| Processor: INTEL(R) XEON(R) PLATINUM 8581V
| Cores: 60 | Logical Cores: 120
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 1031732 | Free Memory: 1022621
| Total Page/Swap (MB): 2047 | Free Page/Swap: 2047
|---------------------------------------------------------------------------------------------|
2026-02-13T03:17:16Z [4,824ms] [Warning] [gpu.foundation.plugin] ECC is enabled on physical device 0
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.anymal_c.rough_env_cfg:AnymalCRoughEnvCfg
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.anymal_c.agents.rsl_rl_ppo_cfg:AnymalCRoughPPORunnerCfg
[INFO] Logging experiment in directory: /home/t2user/IsaacLab/logs/rsl_rl/anymal_c_rough
Exact experiment name requested from command line: 2026-02-12_22-17-18

[36m======================================================================================[0m
[36m[1m[INFO][IsaacLab]: Logging to file: /tmp/isaaclab/logs/isaaclab_2026-02-12_22-17-18.log[0m
[36m======================================================================================[0m

[33m22:17:18 [simulation_context.py] WARNING: The `enable_external_forces_every_iteration` parameter in the PhysxCfg is set to False. If you are experiencing noisy velocities, consider enabling this flag. You may need to slightly increase the number of velocity iterations (setting it to 1 or 2 rather than 0), together with this flag, to improve the accuracy of velocity updates.[0m
[INFO]: Base environment:
	Environment device    : cuda:0
	Environment seed      : 42
	Physics step-size     : 0.005
	Rendering step-size   : 0.02
	Environment step-size : 0.02
[INFO] Generating terrains based on curriculum took : 1.231309 seconds
[INFO]: Time taken for scene creation : 8.971172 seconds
[INFO]: Scene manager:  <class InteractiveScene>
	Number of environments: 8192
	Environment spacing   : 2.5
	Source prim name      : /World/envs/env_0
	Global prim paths     : ['/World/ground']
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
[INFO]: Time taken for simulation start : 11.349718 seconds
[INFO] Command Manager:  <CommandManager> contains 1 active terms.
+------------------------------------------------+
|              Active Command Terms              |
+-------+---------------+------------------------+
| Index | Name          |          Type          |
+-------+---------------+------------------------+
|   0   | base_velocity | UniformVelocityCommand |
+-------+---------------+------------------------+

[INFO] Event Manager:  <EventManager> contains 3 active terms.
+--------------------------------------+
| Active Event Terms in Mode: 'startup' |
+----------+---------------------------+
|  Index   | Name                      |
+----------+---------------------------+
|    0     | physics_material          |
|    1     | add_base_mass             |
|    2     | base_com                  |
+----------+---------------------------+
+---------------------------------------+
|  Active Event Terms in Mode: 'reset'  |
+--------+------------------------------+
| Index  | Name                         |
+--------+------------------------------+
|   0    | base_external_force_torque   |
|   1    | reset_base                   |
|   2    | reset_robot_joints           |
+--------+------------------------------+
+----------------------------------------------+
|    Active Event Terms in Mode: 'interval'    |
+-------+------------+-------------------------+
| Index | Name       | Interval time range (s) |
+-------+------------+-------------------------+
|   0   | push_robot |       (10.0, 15.0)      |
+-------+------------+-------------------------+

[INFO] Recorder Manager:  <RecorderManager> contains 0 active terms.
+---------------------+
| Active Recorder Terms |
+-----------+---------+
|   Index   | Name    |
+-----------+---------+
+-----------+---------+

[INFO] Action Manager:  <ActionManager> contains 1 active terms.
+------------------------------------+
|  Active Action Terms (shape: 12)   |
+--------+-------------+-------------+
| Index  | Name        |   Dimension |
+--------+-------------+-------------+
|   0    | joint_pos   |          12 |
+--------+-------------+-------------+

[INFO] Observation Manager: <ObservationManager> contains 1 groups.
+----------------------------------------------------------+
| Active Observation Terms in Group: 'policy' (shape: (235,)) |
+-----------+--------------------------------+-------------+
|   Index   | Name                           |    Shape    |
+-----------+--------------------------------+-------------+
|     0     | base_lin_vel                   |     (3,)    |
|     1     | base_ang_vel                   |     (3,)    |
|     2     | projected_gravity              |     (3,)    |
|     3     | velocity_commands              |     (3,)    |
|     4     | joint_pos                      |    (12,)    |
|     5     | joint_vel                      |    (12,)    |
|     6     | actions                        |    (12,)    |
|     7     | height_scan                    |    (187,)   |
+-----------+--------------------------------+-------------+

[INFO] Termination Manager:  <TerminationManager> contains 2 active terms.
+---------------------------------+
|     Active Termination Terms    |
+-------+--------------+----------+
| Index | Name         | Time Out |
+-------+--------------+----------+
|   0   | time_out     |   True   |
|   1   | base_contact |  False   |
+-------+--------------+----------+

[INFO] Reward Manager:  <RewardManager> contains 11 active terms.
+-----------------------------------------+
|           Active Reward Terms           |
+-------+----------------------+----------+
| Index | Name                 |   Weight |
+-------+----------------------+----------+
|   0   | track_lin_vel_xy_exp |      1.0 |
|   1   | track_ang_vel_z_exp  |      0.5 |
|   2   | lin_vel_z_l2         |     -2.0 |
|   3   | ang_vel_xy_l2        |    -0.05 |
|   4   | dof_torques_l2       |   -1e-05 |
|   5   | dof_acc_l2           | -2.5e-07 |
|   6   | action_rate_l2       |    -0.01 |
|   7   | feet_air_time        |    0.125 |
|   8   | undesired_contacts   |     -1.0 |
|   9   | flat_orientation_l2  |      0.0 |
|   10  | dof_pos_limits       |      0.0 |
+-------+----------------------+----------+

[INFO] Curriculum Manager:  <CurriculumManager> contains 1 active terms.
+---------------------------+
|  Active Curriculum Terms  |
+--------+------------------+
| Index  | Name             |
+--------+------------------+
|   0    | terrain_levels   |
+--------+------------------+

[INFO]: Completed setting up the environment...
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
2026-02-13T03:17:40Z [28,672ms] [Warning] [omni.physx.fabric.plugin] FabricManager::initializePointInstancer mismatched prototypes on point instancer: /Visuals/Command/velocity_goal.
2026-02-13T03:17:40Z [28,672ms] [Warning] [omni.physx.fabric.plugin] FabricManager::initializePointInstancer mismatched prototypes on point instancer: /Visuals/Command/velocity_current.
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/utils/utils.py:245: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'policy' key. As an observation group with the name 'policy' was found, this is assumed to be the observation set. Consider adding the 'policy' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/utils/utils.py:291: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'critic' key. As the configuration for 'critic' is missing, the observations from the 'policy' set are used. Consider adding the 'critic' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
--------------------------------------------------------------------------------
Resolved observation sets: 
	 policy :  ['policy']
	 critic :  ['policy']
--------------------------------------------------------------------------------
Actor MLP: MLP(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=12, bias=True)
)
Critic MLP: MLP(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
################################################################################
                       [1m Learning iteration 0/10 [0m                        

                       Computation: 27526 steps/s (collection: 6.679s, learning 0.463s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0521
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 16.9938
                       Mean reward: -0.82
               Mean episode length: 20.21
Episode_Reward/track_lin_vel_xy_exp: 0.0020
Episode_Reward/track_ang_vel_z_exp: 0.0021
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0030
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0107
     Episode_Reward/action_rate_l2: -0.0029
      Episode_Reward/feet_air_time: -0.0003
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4976
Metrics/base_velocity/error_vel_xy: 0.0206
Metrics/base_velocity/error_vel_yaw: 0.0171
      Episode_Termination/time_out: 0.0121
  Episode_Termination/base_contact: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 7.14s
                      Time elapsed: 00:00:07
                               ETA: 00:01:11

Could not find git repository in /home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/__init__.py. Skipping.
Storing git diff for 'IsaacLab' in: /home/t2user/IsaacLab/logs/rsl_rl/anymal_c_rough/2026-02-12_22-17-18/git/IsaacLab.diff
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
################################################################################
                       [1m Learning iteration 1/10 [0m                        

                       Computation: 35984 steps/s (collection: 5.191s, learning 0.273s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 16.9448
                       Mean reward: -1.38
               Mean episode length: 44.97
Episode_Reward/track_lin_vel_xy_exp: 0.0072
Episode_Reward/track_ang_vel_z_exp: 0.0053
       Episode_Reward/lin_vel_z_l2: -0.0211
      Episode_Reward/ang_vel_xy_l2: -0.0098
     Episode_Reward/dof_torques_l2: -0.0048
         Episode_Reward/dof_acc_l2: -0.0215
     Episode_Reward/action_rate_l2: -0.0087
      Episode_Reward/feet_air_time: -0.0009
 Episode_Reward/undesired_contacts: -0.0064
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4630
Metrics/base_velocity/error_vel_xy: 0.0623
Metrics/base_velocity/error_vel_yaw: 0.0595
      Episode_Termination/time_out: 0.0365
  Episode_Termination/base_contact: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 5.46s
                      Time elapsed: 00:00:12
                               ETA: 00:00:56

################################################################################
                       [1m Learning iteration 2/10 [0m                        

                       Computation: 34402 steps/s (collection: 5.441s, learning 0.274s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 16.7574
                       Mean reward: -1.83
               Mean episode length: 68.56
Episode_Reward/track_lin_vel_xy_exp: 0.0110
Episode_Reward/track_ang_vel_z_exp: 0.0091
       Episode_Reward/lin_vel_z_l2: -0.0219
      Episode_Reward/ang_vel_xy_l2: -0.0155
     Episode_Reward/dof_torques_l2: -0.0081
         Episode_Reward/dof_acc_l2: -0.0299
     Episode_Reward/action_rate_l2: -0.0143
      Episode_Reward/feet_air_time: -0.0016
 Episode_Reward/undesired_contacts: -0.0166
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4227
Metrics/base_velocity/error_vel_xy: 0.1018
Metrics/base_velocity/error_vel_yaw: 0.0968
      Episode_Termination/time_out: 0.0605
  Episode_Termination/base_contact: 0.0281
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 5.71s
                      Time elapsed: 00:00:18
                               ETA: 00:00:48

################################################################################
                       [1m Learning iteration 3/10 [0m                        

                       Computation: 33944 steps/s (collection: 5.519s, learning 0.274s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0195
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 16.5605
                       Mean reward: -2.31
               Mean episode length: 88.58
Episode_Reward/track_lin_vel_xy_exp: 0.0166
Episode_Reward/track_ang_vel_z_exp: 0.0128
       Episode_Reward/lin_vel_z_l2: -0.0242
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.0111
         Episode_Reward/dof_acc_l2: -0.0369
     Episode_Reward/action_rate_l2: -0.0194
      Episode_Reward/feet_air_time: -0.0021
 Episode_Reward/undesired_contacts: -0.0264
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3874
Metrics/base_velocity/error_vel_xy: 0.1339
Metrics/base_velocity/error_vel_yaw: 0.1297
      Episode_Termination/time_out: 0.0818
  Episode_Termination/base_contact: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 5.79s
                      Time elapsed: 00:00:24
                               ETA: 00:00:42

################################################################################
                       [1m Learning iteration 4/10 [0m                        

                       Computation: 34193 steps/s (collection: 5.476s, learning 0.274s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 16.3921
                       Mean reward: -2.84
               Mean episode length: 111.44
Episode_Reward/track_lin_vel_xy_exp: 0.0203
Episode_Reward/track_ang_vel_z_exp: 0.0170
       Episode_Reward/lin_vel_z_l2: -0.0247
      Episode_Reward/ang_vel_xy_l2: -0.0233
     Episode_Reward/dof_torques_l2: -0.0144
         Episode_Reward/dof_acc_l2: -0.0421
     Episode_Reward/action_rate_l2: -0.0245
      Episode_Reward/feet_air_time: -0.0027
 Episode_Reward/undesired_contacts: -0.0410
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3583
Metrics/base_velocity/error_vel_xy: 0.1702
Metrics/base_velocity/error_vel_yaw: 0.1595
      Episode_Termination/time_out: 0.1017
  Episode_Termination/base_contact: 0.0500
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 5.75s
                      Time elapsed: 00:00:29
                               ETA: 00:00:35

################################################################################
                       [1m Learning iteration 5/10 [0m                        

                       Computation: 34521 steps/s (collection: 5.423s, learning 0.273s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 16.2643
                       Mean reward: -3.07
               Mean episode length: 131.73
Episode_Reward/track_lin_vel_xy_exp: 0.0259
Episode_Reward/track_ang_vel_z_exp: 0.0199
       Episode_Reward/lin_vel_z_l2: -0.0257
      Episode_Reward/ang_vel_xy_l2: -0.0268
     Episode_Reward/dof_torques_l2: -0.0175
         Episode_Reward/dof_acc_l2: -0.0472
     Episode_Reward/action_rate_l2: -0.0292
      Episode_Reward/feet_air_time: -0.0032
 Episode_Reward/undesired_contacts: -0.0491
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3299
Metrics/base_velocity/error_vel_xy: 0.1976
Metrics/base_velocity/error_vel_yaw: 0.1934
      Episode_Termination/time_out: 0.1218
  Episode_Termination/base_contact: 0.0567
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 5.70s
                      Time elapsed: 00:00:35
                               ETA: 00:00:29

################################################################################
                       [1m Learning iteration 6/10 [0m                        

                       Computation: 34793 steps/s (collection: 5.377s, learning 0.274s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 16.1372
                       Mean reward: -3.68
               Mean episode length: 157.10
Episode_Reward/track_lin_vel_xy_exp: 0.0269
Episode_Reward/track_ang_vel_z_exp: 0.0230
       Episode_Reward/lin_vel_z_l2: -0.0272
      Episode_Reward/ang_vel_xy_l2: -0.0310
     Episode_Reward/dof_torques_l2: -0.0210
         Episode_Reward/dof_acc_l2: -0.0528
     Episode_Reward/action_rate_l2: -0.0347
      Episode_Reward/feet_air_time: -0.0038
 Episode_Reward/undesired_contacts: -0.0575
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3021
Metrics/base_velocity/error_vel_xy: 0.2476
Metrics/base_velocity/error_vel_yaw: 0.2377
      Episode_Termination/time_out: 0.1430
  Episode_Termination/base_contact: 0.0619
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 5.65s
                      Time elapsed: 00:00:41
                               ETA: 00:00:23

################################################################################
                       [1m Learning iteration 7/10 [0m                        

                       Computation: 35140 steps/s (collection: 5.321s, learning 0.274s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 16.0032
                       Mean reward: -3.59
               Mean episode length: 170.58
Episode_Reward/track_lin_vel_xy_exp: 0.0378
Episode_Reward/track_ang_vel_z_exp: 0.0259
       Episode_Reward/lin_vel_z_l2: -0.0280
      Episode_Reward/ang_vel_xy_l2: -0.0349
     Episode_Reward/dof_torques_l2: -0.0238
         Episode_Reward/dof_acc_l2: -0.0579
     Episode_Reward/action_rate_l2: -0.0387
      Episode_Reward/feet_air_time: -0.0041
 Episode_Reward/undesired_contacts: -0.0615
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2723
Metrics/base_velocity/error_vel_xy: 0.2661
Metrics/base_velocity/error_vel_yaw: 0.2656
      Episode_Termination/time_out: 0.1663
  Episode_Termination/base_contact: 0.0673
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 5.59s
                      Time elapsed: 00:00:46
                               ETA: 00:00:17

################################################################################
                       [1m Learning iteration 8/10 [0m                        

                       Computation: 35886 steps/s (collection: 5.210s, learning 0.268s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 15.8591
                       Mean reward: -4.03
               Mean episode length: 198.65
Episode_Reward/track_lin_vel_xy_exp: 0.0437
Episode_Reward/track_ang_vel_z_exp: 0.0304
       Episode_Reward/lin_vel_z_l2: -0.0290
      Episode_Reward/ang_vel_xy_l2: -0.0384
     Episode_Reward/dof_torques_l2: -0.0270
         Episode_Reward/dof_acc_l2: -0.0650
     Episode_Reward/action_rate_l2: -0.0440
      Episode_Reward/feet_air_time: -0.0045
 Episode_Reward/undesired_contacts: -0.0700
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2431
Metrics/base_velocity/error_vel_xy: 0.3019
Metrics/base_velocity/error_vel_yaw: 0.3009
      Episode_Termination/time_out: 0.1873
  Episode_Termination/base_contact: 0.0738
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 5.48s
                      Time elapsed: 00:00:52
                               ETA: 00:00:11

################################################################################
                       [1m Learning iteration 9/10 [0m                        

                       Computation: 36447 steps/s (collection: 5.122s, learning 0.273s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 15.7253
                       Mean reward: -4.39
               Mean episode length: 222.54
Episode_Reward/track_lin_vel_xy_exp: 0.0431
Episode_Reward/track_ang_vel_z_exp: 0.0335
       Episode_Reward/lin_vel_z_l2: -0.0295
      Episode_Reward/ang_vel_xy_l2: -0.0419
     Episode_Reward/dof_torques_l2: -0.0300
         Episode_Reward/dof_acc_l2: -0.0675
     Episode_Reward/action_rate_l2: -0.0478
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0783
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2130
Metrics/base_velocity/error_vel_xy: 0.3352
Metrics/base_velocity/error_vel_yaw: 0.3297
      Episode_Termination/time_out: 0.2068
  Episode_Termination/base_contact: 0.0820
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 5.39s
                      Time elapsed: 00:00:57
                               ETA: 00:00:05

Training time: 59.03 seconds
Exit code: 0

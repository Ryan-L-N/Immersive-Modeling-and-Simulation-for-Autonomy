[3gH    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H   [INFO] Using python from: /home/t2user/miniconda3/envs/env_isaaclab/bin/python
[INFO][AppLauncher]: Using device: cuda:0
[INFO][AppLauncher]: Loading experience file: /home/t2user/IsaacLab/apps/isaaclab.python.headless.kit
Loading user config located at: '/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/isaacsim/kit/data/Kit/Isaac-Sim/5.1/user.config.json'
[Info] [carb] Logging to file: /home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/isaacsim/kit/logs/Kit/Isaac-Sim/5.1/kit_20260212_205325.log
2026-02-13T01:53:25Z [159ms] [Warning] [omni.usd_config.extension] Enable omni.materialx.libs extension to use MaterialX
2026-02-13T01:53:26Z [646ms] [Warning] [omni.platforminfo.plugin] failed to open the default display.  Can't verify X Server version.
2026-02-13T01:53:26Z [898ms] [Warning] [carb] Acquiring non optional plugin interface which is not listed as dependency: [omni::physx::IPhysxBenchmarks v1.0] (plugin: <default plugin>), by client: omni.physics.physx.plugin. Add it to CARB_PLUGIN_IMPL_DEPS() macro of a client.
2026-02-13T01:53:26Z [913ms] [Warning] [omni.isaac.dynamic_control] omni.isaac.dynamic_control is deprecated as of Isaac Sim 4.5. No action is needed from end-users.

|---------------------------------------------------------------------------------------------|
| Driver Version: 580.126.16    | Graphics API: Vulkan
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|     |                                  |        |     |            | Bus-ID    |            |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA H100 NVL                  | Yes: 0 |     | 95830   MB | 10de      | 0          |
|     |                                  |        |     |            | 2321      | 64de321d.. |
|     |                                  |        |     |            | 40        |            |
|=============================================================================================|
| OS: 22.04.5 LTS (Jammy Jellyfish) ubuntu, Version: 22.04.5, Kernel: 5.15.0-170-generic
| Processor: INTEL(R) XEON(R) PLATINUM 8581V
| Cores: 60 | Logical Cores: 120
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 1031732 | Free Memory: 1022700
| Total Page/Swap (MB): 2047 | Free Page/Swap: 2047
|---------------------------------------------------------------------------------------------|
2026-02-13T01:53:30Z [5,376ms] [Warning] [gpu.foundation.plugin] ECC is enabled on physical device 0
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.anymal_c.rough_env_cfg:AnymalCRoughEnvCfg
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.anymal_c.agents.rsl_rl_ppo_cfg:AnymalCRoughPPORunnerCfg
[INFO] Logging experiment in directory: /home/t2user/IsaacLab/logs/rsl_rl/anymal_c_rough
Exact experiment name requested from command line: 2026-02-12_20-53-32

[36m======================================================================================[0m
[36m[1m[INFO][IsaacLab]: Logging to file: /tmp/isaaclab/logs/isaaclab_2026-02-12_20-53-32.log[0m
[36m======================================================================================[0m

[33m20:53:32 [simulation_context.py] WARNING: The `enable_external_forces_every_iteration` parameter in the PhysxCfg is set to False. If you are experiencing noisy velocities, consider enabling this flag. You may need to slightly increase the number of velocity iterations (setting it to 1 or 2 rather than 0), together with this flag, to improve the accuracy of velocity updates.[0m
[INFO]: Base environment:
	Environment device    : cuda:0
	Environment seed      : 42
	Physics step-size     : 0.005
	Rendering step-size   : 0.02
	Environment step-size : 0.02
[INFO] Generating terrains based on curriculum took : 1.205391 seconds
[INFO]: Time taken for scene creation : 3.045557 seconds
[INFO]: Scene manager:  <class InteractiveScene>
	Number of environments: 1024
	Environment spacing   : 2.5
	Source prim name      : /World/envs/env_0
	Global prim paths     : ['/World/ground']
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
[INFO]: Time taken for simulation start : 2.596668 seconds
[INFO] Command Manager:  <CommandManager> contains 1 active terms.
+------------------------------------------------+
|              Active Command Terms              |
+-------+---------------+------------------------+
| Index | Name          |          Type          |
+-------+---------------+------------------------+
|   0   | base_velocity | UniformVelocityCommand |
+-------+---------------+------------------------+

[INFO] Event Manager:  <EventManager> contains 3 active terms.
+--------------------------------------+
| Active Event Terms in Mode: 'startup' |
+----------+---------------------------+
|  Index   | Name                      |
+----------+---------------------------+
|    0     | physics_material          |
|    1     | add_base_mass             |
|    2     | base_com                  |
+----------+---------------------------+
+---------------------------------------+
|  Active Event Terms in Mode: 'reset'  |
+--------+------------------------------+
| Index  | Name                         |
+--------+------------------------------+
|   0    | base_external_force_torque   |
|   1    | reset_base                   |
|   2    | reset_robot_joints           |
+--------+------------------------------+
+----------------------------------------------+
|    Active Event Terms in Mode: 'interval'    |
+-------+------------+-------------------------+
| Index | Name       | Interval time range (s) |
+-------+------------+-------------------------+
|   0   | push_robot |       (10.0, 15.0)      |
+-------+------------+-------------------------+

[INFO] Recorder Manager:  <RecorderManager> contains 0 active terms.
+---------------------+
| Active Recorder Terms |
+-----------+---------+
|   Index   | Name    |
+-----------+---------+
+-----------+---------+

[INFO] Action Manager:  <ActionManager> contains 1 active terms.
+------------------------------------+
|  Active Action Terms (shape: 12)   |
+--------+-------------+-------------+
| Index  | Name        |   Dimension |
+--------+-------------+-------------+
|   0    | joint_pos   |          12 |
+--------+-------------+-------------+

[INFO] Observation Manager: <ObservationManager> contains 1 groups.
+----------------------------------------------------------+
| Active Observation Terms in Group: 'policy' (shape: (235,)) |
+-----------+--------------------------------+-------------+
|   Index   | Name                           |    Shape    |
+-----------+--------------------------------+-------------+
|     0     | base_lin_vel                   |     (3,)    |
|     1     | base_ang_vel                   |     (3,)    |
|     2     | projected_gravity              |     (3,)    |
|     3     | velocity_commands              |     (3,)    |
|     4     | joint_pos                      |    (12,)    |
|     5     | joint_vel                      |    (12,)    |
|     6     | actions                        |    (12,)    |
|     7     | height_scan                    |    (187,)   |
+-----------+--------------------------------+-------------+

[INFO] Termination Manager:  <TerminationManager> contains 2 active terms.
+---------------------------------+
|     Active Termination Terms    |
+-------+--------------+----------+
| Index | Name         | Time Out |
+-------+--------------+----------+
|   0   | time_out     |   True   |
|   1   | base_contact |  False   |
+-------+--------------+----------+

[INFO] Reward Manager:  <RewardManager> contains 11 active terms.
+-----------------------------------------+
|           Active Reward Terms           |
+-------+----------------------+----------+
| Index | Name                 |   Weight |
+-------+----------------------+----------+
|   0   | track_lin_vel_xy_exp |      1.0 |
|   1   | track_ang_vel_z_exp  |      0.5 |
|   2   | lin_vel_z_l2         |     -2.0 |
|   3   | ang_vel_xy_l2        |    -0.05 |
|   4   | dof_torques_l2       |   -1e-05 |
|   5   | dof_acc_l2           | -2.5e-07 |
|   6   | action_rate_l2       |    -0.01 |
|   7   | feet_air_time        |    0.125 |
|   8   | undesired_contacts   |     -1.0 |
|   9   | flat_orientation_l2  |      0.0 |
|   10  | dof_pos_limits       |      0.0 |
+-------+----------------------+----------+

[INFO] Curriculum Manager:  <CurriculumManager> contains 1 active terms.
+---------------------------+
|  Active Curriculum Terms  |
+--------+------------------+
| Index  | Name             |
+--------+------------------+
|   0    | terrain_levels   |
+--------+------------------+

[INFO]: Completed setting up the environment...
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
2026-02-13T01:53:39Z [13,893ms] [Warning] [omni.physx.fabric.plugin] FabricManager::initializePointInstancer mismatched prototypes on point instancer: /Visuals/Command/velocity_goal.
2026-02-13T01:53:39Z [13,893ms] [Warning] [omni.physx.fabric.plugin] FabricManager::initializePointInstancer mismatched prototypes on point instancer: /Visuals/Command/velocity_current.
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/utils/utils.py:245: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'policy' key. As an observation group with the name 'policy' was found, this is assumed to be the observation set. Consider adding the 'policy' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/utils/utils.py:291: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'critic' key. As the configuration for 'critic' is missing, the observations from the 'policy' set are used. Consider adding the 'critic' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.
  warnings.warn(
--------------------------------------------------------------------------------
Resolved observation sets: 
	 policy :  ['policy']
	 critic :  ['policy']
--------------------------------------------------------------------------------
Actor MLP: MLP(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=12, bias=True)
)
Critic MLP: MLP(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
################################################################################
                      [1m Learning iteration 0/1400 [0m                       

                       Computation: 5255 steps/s (collection: 4.208s, learning 0.468s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0536
               Mean surrogate loss: 0.0314
                 Mean entropy loss: 17.0065
                       Mean reward: -0.46
               Mean episode length: 10.95
Episode_Reward/track_lin_vel_xy_exp: 0.0030
Episode_Reward/track_ang_vel_z_exp: 0.0022
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0027
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0105
     Episode_Reward/action_rate_l2: -0.0026
      Episode_Reward/feet_air_time: -0.0003
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5452
Metrics/base_velocity/error_vel_xy: 0.0163
Metrics/base_velocity/error_vel_yaw: 0.0135
      Episode_Termination/time_out: 0.0107
  Episode_Termination/base_contact: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 4.68s
                      Time elapsed: 00:00:04
                               ETA: 01:49:06

Could not find git repository in /home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/__init__.py. Skipping.
Storing git diff for 'IsaacLab' in: /home/t2user/IsaacLab/logs/rsl_rl/anymal_c_rough/2026-02-12_20-53-32/git/IsaacLab.diff
/home/t2user/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1481.)
  return forward_call(*args, **kwargs)
################################################################################
                      [1m Learning iteration 1/1400 [0m                       

                       Computation: 9169 steps/s (collection: 2.422s, learning 0.258s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0297
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 16.9916
                       Mean reward: -0.93
               Mean episode length: 27.51
Episode_Reward/track_lin_vel_xy_exp: 0.0054
Episode_Reward/track_ang_vel_z_exp: 0.0057
       Episode_Reward/lin_vel_z_l2: -0.0197
      Episode_Reward/ang_vel_xy_l2: -0.0082
     Episode_Reward/dof_torques_l2: -0.0048
         Episode_Reward/dof_acc_l2: -0.0195
     Episode_Reward/action_rate_l2: -0.0084
      Episode_Reward/feet_air_time: -0.0009
 Episode_Reward/undesired_contacts: -0.0071
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5267
Metrics/base_velocity/error_vel_xy: 0.0628
Metrics/base_velocity/error_vel_yaw: 0.0546
      Episode_Termination/time_out: 0.0255
  Episode_Termination/base_contact: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 2.68s
                      Time elapsed: 00:00:07
                               ETA: 01:25:45

################################################################################
                      [1m Learning iteration 2/1400 [0m                       

                       Computation: 8826 steps/s (collection: 2.524s, learning 0.261s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 16.9116
                       Mean reward: -1.35
               Mean episode length: 42.90
Episode_Reward/track_lin_vel_xy_exp: 0.0121
Episode_Reward/track_ang_vel_z_exp: 0.0096
       Episode_Reward/lin_vel_z_l2: -0.0220
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.0079
         Episode_Reward/dof_acc_l2: -0.0313
     Episode_Reward/action_rate_l2: -0.0144
      Episode_Reward/feet_air_time: -0.0014
 Episode_Reward/undesired_contacts: -0.0173
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4862
Metrics/base_velocity/error_vel_xy: 0.1015
Metrics/base_velocity/error_vel_yaw: 0.0989
      Episode_Termination/time_out: 0.0449
  Episode_Termination/base_contact: 0.0267
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 2.78s
                      Time elapsed: 00:00:10
                               ETA: 01:18:45

################################################################################
                      [1m Learning iteration 3/1400 [0m                       

                       Computation: 8931 steps/s (collection: 2.490s, learning 0.261s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 16.7929
                       Mean reward: -1.96
               Mean episode length: 67.06
Episode_Reward/track_lin_vel_xy_exp: 0.0109
Episode_Reward/track_ang_vel_z_exp: 0.0122
       Episode_Reward/lin_vel_z_l2: -0.0256
      Episode_Reward/ang_vel_xy_l2: -0.0226
     Episode_Reward/dof_torques_l2: -0.0115
         Episode_Reward/dof_acc_l2: -0.0413
     Episode_Reward/action_rate_l2: -0.0204
      Episode_Reward/feet_air_time: -0.0022
 Episode_Reward/undesired_contacts: -0.0241
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4519
Metrics/base_velocity/error_vel_xy: 0.1590
Metrics/base_velocity/error_vel_yaw: 0.1420
      Episode_Termination/time_out: 0.0635
  Episode_Termination/base_contact: 0.0437
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 2.75s
                      Time elapsed: 00:00:12
                               ETA: 01:15:02

################################################################################
                      [1m Learning iteration 4/1400 [0m                       

                       Computation: 8878 steps/s (collection: 2.503s, learning 0.265s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 16.6799
                       Mean reward: -2.27
               Mean episode length: 83.95
Episode_Reward/track_lin_vel_xy_exp: 0.0191
Episode_Reward/track_ang_vel_z_exp: 0.0154
       Episode_Reward/lin_vel_z_l2: -0.0246
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.0134
         Episode_Reward/dof_acc_l2: -0.0413
     Episode_Reward/action_rate_l2: -0.0238
      Episode_Reward/feet_air_time: -0.0026
 Episode_Reward/undesired_contacts: -0.0341
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4163
Metrics/base_velocity/error_vel_xy: 0.1682
Metrics/base_velocity/error_vel_yaw: 0.1591
      Episode_Termination/time_out: 0.0841
  Episode_Termination/base_contact: 0.0581
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 2.77s
                      Time elapsed: 00:00:15
                               ETA: 01:12:52

################################################################################
                      [1m Learning iteration 5/1400 [0m                       

                       Computation: 8851 steps/s (collection: 2.515s, learning 0.261s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 16.5621
                       Mean reward: -2.70
               Mean episode length: 108.55
Episode_Reward/track_lin_vel_xy_exp: 0.0198
Episode_Reward/track_ang_vel_z_exp: 0.0185
       Episode_Reward/lin_vel_z_l2: -0.0226
      Episode_Reward/ang_vel_xy_l2: -0.0269
     Episode_Reward/dof_torques_l2: -0.0173
         Episode_Reward/dof_acc_l2: -0.0503
     Episode_Reward/action_rate_l2: -0.0305
      Episode_Reward/feet_air_time: -0.0032
 Episode_Reward/undesired_contacts: -0.0574
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3843
Metrics/base_velocity/error_vel_xy: 0.2236
Metrics/base_velocity/error_vel_yaw: 0.2058
      Episode_Termination/time_out: 0.0997
  Episode_Termination/base_contact: 0.0702
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 2.78s
                      Time elapsed: 00:00:18
                               ETA: 01:11:26

################################################################################
                      [1m Learning iteration 6/1400 [0m                       

                       Computation: 8858 steps/s (collection: 2.513s, learning 0.262s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 16.4184
                       Mean reward: -3.00
               Mean episode length: 128.69
Episode_Reward/track_lin_vel_xy_exp: 0.0321
Episode_Reward/track_ang_vel_z_exp: 0.0226
       Episode_Reward/lin_vel_z_l2: -0.0279
      Episode_Reward/ang_vel_xy_l2: -0.0311
     Episode_Reward/dof_torques_l2: -0.0192
         Episode_Reward/dof_acc_l2: -0.0506
     Episode_Reward/action_rate_l2: -0.0326
      Episode_Reward/feet_air_time: -0.0033
 Episode_Reward/undesired_contacts: -0.0491
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3435
Metrics/base_velocity/error_vel_xy: 0.2137
Metrics/base_velocity/error_vel_yaw: 0.2093
      Episode_Termination/time_out: 0.1242
  Episode_Termination/base_contact: 0.0803
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 2.77s
                      Time elapsed: 00:00:21
                               ETA: 01:10:24

################################################################################
                      [1m Learning iteration 7/1400 [0m                       

                       Computation: 8990 steps/s (collection: 2.472s, learning 0.262s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0083
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 16.2741
                       Mean reward: -3.36
               Mean episode length: 147.69
Episode_Reward/track_lin_vel_xy_exp: 0.0265
Episode_Reward/track_ang_vel_z_exp: 0.0279
       Episode_Reward/lin_vel_z_l2: -0.0298
      Episode_Reward/ang_vel_xy_l2: -0.0367
     Episode_Reward/dof_torques_l2: -0.0215
         Episode_Reward/dof_acc_l2: -0.0617
     Episode_Reward/action_rate_l2: -0.0369
      Episode_Reward/feet_air_time: -0.0040
 Episode_Reward/undesired_contacts: -0.0534
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3104
Metrics/base_velocity/error_vel_xy: 0.2672
Metrics/base_velocity/error_vel_yaw: 0.2280
      Episode_Termination/time_out: 0.1434
  Episode_Termination/base_contact: 0.0906
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.73s
                      Time elapsed: 00:00:23
                               ETA: 01:09:29

################################################################################
                      [1m Learning iteration 8/1400 [0m                       

                       Computation: 9004 steps/s (collection: 2.466s, learning 0.263s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0075
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 16.1028
                       Mean reward: -3.63
               Mean episode length: 164.14
Episode_Reward/track_lin_vel_xy_exp: 0.0414
Episode_Reward/track_ang_vel_z_exp: 0.0266
       Episode_Reward/lin_vel_z_l2: -0.0319
      Episode_Reward/ang_vel_xy_l2: -0.0353
     Episode_Reward/dof_torques_l2: -0.0237
         Episode_Reward/dof_acc_l2: -0.0588
     Episode_Reward/action_rate_l2: -0.0389
      Episode_Reward/feet_air_time: -0.0037
 Episode_Reward/undesired_contacts: -0.0596
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2778
Metrics/base_velocity/error_vel_xy: 0.2667
Metrics/base_velocity/error_vel_yaw: 0.2581
      Episode_Termination/time_out: 0.1670
  Episode_Termination/base_contact: 0.0961
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 2.73s
                      Time elapsed: 00:00:26
                               ETA: 01:08:45

################################################################################
                      [1m Learning iteration 9/1400 [0m                       

                       Computation: 9068 steps/s (collection: 2.442s, learning 0.268s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 15.9578
                       Mean reward: -3.82
               Mean episode length: 183.76
Episode_Reward/track_lin_vel_xy_exp: 0.0602
Episode_Reward/track_ang_vel_z_exp: 0.0343
       Episode_Reward/lin_vel_z_l2: -0.0302
      Episode_Reward/ang_vel_xy_l2: -0.0426
     Episode_Reward/dof_torques_l2: -0.0291
         Episode_Reward/dof_acc_l2: -0.0674
     Episode_Reward/action_rate_l2: -0.0479
      Episode_Reward/feet_air_time: -0.0046
 Episode_Reward/undesired_contacts: -0.0703
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2498
Metrics/base_velocity/error_vel_xy: 0.2799
Metrics/base_velocity/error_vel_yaw: 0.3141
      Episode_Termination/time_out: 0.1848
  Episode_Termination/base_contact: 0.1025
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 2.71s
                      Time elapsed: 00:00:29
                               ETA: 01:08:07

################################################################################
                      [1m Learning iteration 10/1400 [0m                      

                       Computation: 9107 steps/s (collection: 2.429s, learning 0.270s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0066
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 15.8465
                       Mean reward: -4.27
               Mean episode length: 209.79
Episode_Reward/track_lin_vel_xy_exp: 0.0439
Episode_Reward/track_ang_vel_z_exp: 0.0355
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0340
         Episode_Reward/dof_acc_l2: -0.0749
     Episode_Reward/action_rate_l2: -0.0550
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0784
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2177
Metrics/base_velocity/error_vel_xy: 0.3853
Metrics/base_velocity/error_vel_yaw: 0.3864
      Episode_Termination/time_out: 0.2035
  Episode_Termination/base_contact: 0.1142
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 2.70s
                      Time elapsed: 00:00:32
                               ETA: 01:07:34

################################################################################
                      [1m Learning iteration 11/1400 [0m                      

                       Computation: 9084 steps/s (collection: 2.445s, learning 0.260s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 15.7138
                       Mean reward: -4.67
               Mean episode length: 234.66
Episode_Reward/track_lin_vel_xy_exp: 0.0455
Episode_Reward/track_ang_vel_z_exp: 0.0316
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0343
         Episode_Reward/dof_acc_l2: -0.0787
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0701
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1817
Metrics/base_velocity/error_vel_xy: 0.4042
Metrics/base_velocity/error_vel_yaw: 0.4359
      Episode_Termination/time_out: 0.2204
  Episode_Termination/base_contact: 0.1287
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.71s
                      Time elapsed: 00:00:34
                               ETA: 01:07:06

################################################################################
                      [1m Learning iteration 12/1400 [0m                      

                       Computation: 9227 steps/s (collection: 2.405s, learning 0.258s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 15.5667
                       Mean reward: -4.99
               Mean episode length: 247.54
Episode_Reward/track_lin_vel_xy_exp: 0.0568
Episode_Reward/track_ang_vel_z_exp: 0.0402
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0556
     Episode_Reward/dof_torques_l2: -0.0357
         Episode_Reward/dof_acc_l2: -0.0851
     Episode_Reward/action_rate_l2: -0.0581
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0655
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1501
Metrics/base_velocity/error_vel_xy: 0.4118
Metrics/base_velocity/error_vel_yaw: 0.4241
      Episode_Termination/time_out: 0.2302
  Episode_Termination/base_contact: 0.1429
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 2.66s
                      Time elapsed: 00:00:37
                               ETA: 01:06:38

################################################################################
                      [1m Learning iteration 13/1400 [0m                      

                       Computation: 9105 steps/s (collection: 2.439s, learning 0.260s)
             Mean action noise std: 0.87
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 15.3734
                       Mean reward: -5.32
               Mean episode length: 278.26
Episode_Reward/track_lin_vel_xy_exp: 0.0431
Episode_Reward/track_ang_vel_z_exp: 0.0441
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0569
     Episode_Reward/dof_torques_l2: -0.0403
         Episode_Reward/dof_acc_l2: -0.0879
     Episode_Reward/action_rate_l2: -0.0637
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0686
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1112
Metrics/base_velocity/error_vel_xy: 0.5179
Metrics/base_velocity/error_vel_yaw: 0.4694
      Episode_Termination/time_out: 0.2486
  Episode_Termination/base_contact: 0.1578
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 2.70s
                      Time elapsed: 00:00:40
                               ETA: 01:06:17

################################################################################
                      [1m Learning iteration 14/1400 [0m                      

                       Computation: 9262 steps/s (collection: 2.387s, learning 0.267s)
             Mean action noise std: 0.86
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 15.2374
                       Mean reward: -5.18
               Mean episode length: 278.22
Episode_Reward/track_lin_vel_xy_exp: 0.0371
Episode_Reward/track_ang_vel_z_exp: 0.0444
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0559
     Episode_Reward/dof_torques_l2: -0.0369
         Episode_Reward/dof_acc_l2: -0.0883
     Episode_Reward/action_rate_l2: -0.0595
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0582
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0593
Metrics/base_velocity/error_vel_xy: 0.5039
Metrics/base_velocity/error_vel_yaw: 0.4250
      Episode_Termination/time_out: 0.2603
  Episode_Termination/base_contact: 0.1809
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 2.65s
                      Time elapsed: 00:00:42
                               ETA: 01:05:55

################################################################################
                      [1m Learning iteration 15/1400 [0m                      

                       Computation: 9136 steps/s (collection: 2.423s, learning 0.267s)
             Mean action noise std: 0.85
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 15.1042
                       Mean reward: -4.75
               Mean episode length: 286.20
Episode_Reward/track_lin_vel_xy_exp: 0.0831
Episode_Reward/track_ang_vel_z_exp: 0.0451
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0580
     Episode_Reward/dof_torques_l2: -0.0409
         Episode_Reward/dof_acc_l2: -0.0922
     Episode_Reward/action_rate_l2: -0.0646
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0686
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0112
Metrics/base_velocity/error_vel_xy: 0.4329
Metrics/base_velocity/error_vel_yaw: 0.4769
      Episode_Termination/time_out: 0.2616
  Episode_Termination/base_contact: 0.2109
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.69s
                      Time elapsed: 00:00:45
                               ETA: 01:05:37

################################################################################
                      [1m Learning iteration 16/1400 [0m                      

                       Computation: 9214 steps/s (collection: 2.406s, learning 0.261s)
             Mean action noise std: 0.84
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 15.0022
                       Mean reward: -4.95
               Mean episode length: 312.84
Episode_Reward/track_lin_vel_xy_exp: 0.0712
Episode_Reward/track_ang_vel_z_exp: 0.0536
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0569
     Episode_Reward/dof_torques_l2: -0.0408
         Episode_Reward/dof_acc_l2: -0.0940
     Episode_Reward/action_rate_l2: -0.0652
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0686
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9537
Metrics/base_velocity/error_vel_xy: 0.4689
Metrics/base_velocity/error_vel_yaw: 0.4434
      Episode_Termination/time_out: 0.2718
  Episode_Termination/base_contact: 0.2429
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 2.67s
                      Time elapsed: 00:00:48
                               ETA: 01:05:20

################################################################################
                      [1m Learning iteration 17/1400 [0m                      

                       Computation: 9083 steps/s (collection: 2.443s, learning 0.263s)
             Mean action noise std: 0.83
          Mean value_function loss: 0.0066
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 14.8926
                       Mean reward: -4.96
               Mean episode length: 318.44
Episode_Reward/track_lin_vel_xy_exp: 0.0769
Episode_Reward/track_ang_vel_z_exp: 0.0526
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0574
     Episode_Reward/dof_torques_l2: -0.0409
         Episode_Reward/dof_acc_l2: -0.0930
     Episode_Reward/action_rate_l2: -0.0648
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0521
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8897
Metrics/base_velocity/error_vel_xy: 0.4642
Metrics/base_velocity/error_vel_yaw: 0.4704
      Episode_Termination/time_out: 0.2749
  Episode_Termination/base_contact: 0.2801
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 2.71s
                      Time elapsed: 00:00:50
                               ETA: 01:05:08

################################################################################
                      [1m Learning iteration 18/1400 [0m                      

                       Computation: 9185 steps/s (collection: 2.416s, learning 0.259s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0068
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 14.7645
                       Mean reward: -4.33
               Mean episode length: 306.04
Episode_Reward/track_lin_vel_xy_exp: 0.0827
Episode_Reward/track_ang_vel_z_exp: 0.0515
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.0570
     Episode_Reward/dof_torques_l2: -0.0394
         Episode_Reward/dof_acc_l2: -0.0841
     Episode_Reward/action_rate_l2: -0.0617
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0513
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8116
Metrics/base_velocity/error_vel_xy: 0.4303
Metrics/base_velocity/error_vel_yaw: 0.4316
      Episode_Termination/time_out: 0.2755
  Episode_Termination/base_contact: 0.3256
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 2.68s
                      Time elapsed: 00:00:53
                               ETA: 01:04:54

################################################################################
                      [1m Learning iteration 19/1400 [0m                      

                       Computation: 9375 steps/s (collection: 2.364s, learning 0.257s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0061
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 14.6504
                       Mean reward: -4.36
               Mean episode length: 300.78
Episode_Reward/track_lin_vel_xy_exp: 0.0673
Episode_Reward/track_ang_vel_z_exp: 0.0544
       Episode_Reward/lin_vel_z_l2: -0.0342
      Episode_Reward/ang_vel_xy_l2: -0.0587
     Episode_Reward/dof_torques_l2: -0.0425
         Episode_Reward/dof_acc_l2: -0.0868
     Episode_Reward/action_rate_l2: -0.0655
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0523
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7290
Metrics/base_velocity/error_vel_xy: 0.4971
Metrics/base_velocity/error_vel_yaw: 0.4696
      Episode_Termination/time_out: 0.2795
  Episode_Termination/base_contact: 0.3682
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.62s
                      Time elapsed: 00:00:56
                               ETA: 01:04:38

################################################################################
                      [1m Learning iteration 20/1400 [0m                      

                       Computation: 9502 steps/s (collection: 2.330s, learning 0.256s)
             Mean action noise std: 0.81
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 14.5602
                       Mean reward: -4.18
               Mean episode length: 298.95
Episode_Reward/track_lin_vel_xy_exp: 0.0617
Episode_Reward/track_ang_vel_z_exp: 0.0509
       Episode_Reward/lin_vel_z_l2: -0.0328
      Episode_Reward/ang_vel_xy_l2: -0.0551
     Episode_Reward/dof_torques_l2: -0.0370
         Episode_Reward/dof_acc_l2: -0.0832
     Episode_Reward/action_rate_l2: -0.0590
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0423
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6511
Metrics/base_velocity/error_vel_xy: 0.4668
Metrics/base_velocity/error_vel_yaw: 0.4311
      Episode_Termination/time_out: 0.2697
  Episode_Termination/base_contact: 0.4140
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 2.59s
                      Time elapsed: 00:00:58
                               ETA: 01:04:20

################################################################################
                      [1m Learning iteration 21/1400 [0m                      

                       Computation: 9447 steps/s (collection: 2.347s, learning 0.254s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0071
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 14.4649
                       Mean reward: -4.14
               Mean episode length: 306.23
Episode_Reward/track_lin_vel_xy_exp: 0.0539
Episode_Reward/track_ang_vel_z_exp: 0.0511
       Episode_Reward/lin_vel_z_l2: -0.0315
      Episode_Reward/ang_vel_xy_l2: -0.0519
     Episode_Reward/dof_torques_l2: -0.0356
         Episode_Reward/dof_acc_l2: -0.0784
     Episode_Reward/action_rate_l2: -0.0557
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0335
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5585
Metrics/base_velocity/error_vel_xy: 0.4659
Metrics/base_velocity/error_vel_yaw: 0.3943
      Episode_Termination/time_out: 0.2527
  Episode_Termination/base_contact: 0.4702
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 2.60s
                      Time elapsed: 00:01:01
                               ETA: 01:04:05

################################################################################
                      [1m Learning iteration 22/1400 [0m                      

                       Computation: 9513 steps/s (collection: 2.327s, learning 0.256s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 14.3614
                       Mean reward: -3.77
               Mean episode length: 289.00
Episode_Reward/track_lin_vel_xy_exp: 0.0602
Episode_Reward/track_ang_vel_z_exp: 0.0485
       Episode_Reward/lin_vel_z_l2: -0.0299
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0343
         Episode_Reward/dof_acc_l2: -0.0747
     Episode_Reward/action_rate_l2: -0.0543
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0359
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4436
Metrics/base_velocity/error_vel_xy: 0.4455
Metrics/base_velocity/error_vel_yaw: 0.4011
      Episode_Termination/time_out: 0.2376
  Episode_Termination/base_contact: 0.5291
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 2.58s
                      Time elapsed: 00:01:03
                               ETA: 01:03:50

################################################################################
                      [1m Learning iteration 23/1400 [0m                      

                       Computation: 9499 steps/s (collection: 2.330s, learning 0.257s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 14.2617
                       Mean reward: -3.07
               Mean episode length: 260.40
Episode_Reward/track_lin_vel_xy_exp: 0.0499
Episode_Reward/track_ang_vel_z_exp: 0.0485
       Episode_Reward/lin_vel_z_l2: -0.0273
      Episode_Reward/ang_vel_xy_l2: -0.0460
     Episode_Reward/dof_torques_l2: -0.0310
         Episode_Reward/dof_acc_l2: -0.0704
     Episode_Reward/action_rate_l2: -0.0495
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0306
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3157
Metrics/base_velocity/error_vel_xy: 0.4213
Metrics/base_velocity/error_vel_yaw: 0.3481
      Episode_Termination/time_out: 0.2135
  Episode_Termination/base_contact: 0.5918
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 2.59s
                      Time elapsed: 00:01:06
                               ETA: 01:03:36

################################################################################
                      [1m Learning iteration 24/1400 [0m                      

                       Computation: 9441 steps/s (collection: 2.345s, learning 0.258s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0075
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 14.1561
                       Mean reward: -2.96
               Mean episode length: 265.69
Episode_Reward/track_lin_vel_xy_exp: 0.0605
Episode_Reward/track_ang_vel_z_exp: 0.0475
       Episode_Reward/lin_vel_z_l2: -0.0268
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0311
         Episode_Reward/dof_acc_l2: -0.0711
     Episode_Reward/action_rate_l2: -0.0498
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0274
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1852
Metrics/base_velocity/error_vel_xy: 0.4074
Metrics/base_velocity/error_vel_yaw: 0.3633
      Episode_Termination/time_out: 0.1821
  Episode_Termination/base_contact: 0.6605
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 2.60s
                      Time elapsed: 00:01:09
                               ETA: 01:03:24

################################################################################
                      [1m Learning iteration 25/1400 [0m                      

                       Computation: 9543 steps/s (collection: 2.318s, learning 0.257s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0070
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 14.0730
                       Mean reward: -2.49
               Mean episode length: 212.27
Episode_Reward/track_lin_vel_xy_exp: 0.0402
Episode_Reward/track_ang_vel_z_exp: 0.0429
       Episode_Reward/lin_vel_z_l2: -0.0240
      Episode_Reward/ang_vel_xy_l2: -0.0405
     Episode_Reward/dof_torques_l2: -0.0260
         Episode_Reward/dof_acc_l2: -0.0627
     Episode_Reward/action_rate_l2: -0.0416
      Episode_Reward/feet_air_time: -0.0051
 Episode_Reward/undesired_contacts: -0.0212
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0354
Metrics/base_velocity/error_vel_xy: 0.3684
Metrics/base_velocity/error_vel_yaw: 0.2971
      Episode_Termination/time_out: 0.1492
  Episode_Termination/base_contact: 0.7354
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 2.58s
                      Time elapsed: 00:01:11
                               ETA: 01:03:11

################################################################################
                      [1m Learning iteration 26/1400 [0m                      

                       Computation: 9452 steps/s (collection: 2.342s, learning 0.258s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0071
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.9873
                       Mean reward: -1.78
               Mean episode length: 181.16
Episode_Reward/track_lin_vel_xy_exp: 0.0473
Episode_Reward/track_ang_vel_z_exp: 0.0393
       Episode_Reward/lin_vel_z_l2: -0.0210
      Episode_Reward/ang_vel_xy_l2: -0.0350
     Episode_Reward/dof_torques_l2: -0.0230
         Episode_Reward/dof_acc_l2: -0.0548
     Episode_Reward/action_rate_l2: -0.0368
      Episode_Reward/feet_air_time: -0.0046
 Episode_Reward/undesired_contacts: -0.0181
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8719
Metrics/base_velocity/error_vel_xy: 0.3096
Metrics/base_velocity/error_vel_yaw: 0.2660
      Episode_Termination/time_out: 0.1158
  Episode_Termination/base_contact: 0.7957
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 2.60s
                      Time elapsed: 00:01:14
                               ETA: 01:03:00

################################################################################
                      [1m Learning iteration 27/1400 [0m                      

                       Computation: 9544 steps/s (collection: 2.319s, learning 0.256s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.8894
                       Mean reward: -1.31
               Mean episode length: 153.05
Episode_Reward/track_lin_vel_xy_exp: 0.0398
Episode_Reward/track_ang_vel_z_exp: 0.0330
       Episode_Reward/lin_vel_z_l2: -0.0171
      Episode_Reward/ang_vel_xy_l2: -0.0288
     Episode_Reward/dof_torques_l2: -0.0178
         Episode_Reward/dof_acc_l2: -0.0446
     Episode_Reward/action_rate_l2: -0.0288
      Episode_Reward/feet_air_time: -0.0037
 Episode_Reward/undesired_contacts: -0.0112
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6871
Metrics/base_velocity/error_vel_xy: 0.2479
Metrics/base_velocity/error_vel_yaw: 0.2071
      Episode_Termination/time_out: 0.0837
  Episode_Termination/base_contact: 0.8510
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.57s
                      Time elapsed: 00:01:16
                               ETA: 01:02:49

################################################################################
                      [1m Learning iteration 28/1400 [0m                      

                       Computation: 9495 steps/s (collection: 2.333s, learning 0.255s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.8051
                       Mean reward: -1.56
               Mean episode length: 152.03
Episode_Reward/track_lin_vel_xy_exp: 0.0405
Episode_Reward/track_ang_vel_z_exp: 0.0297
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0271
     Episode_Reward/dof_torques_l2: -0.0159
         Episode_Reward/dof_acc_l2: -0.0426
     Episode_Reward/action_rate_l2: -0.0261
      Episode_Reward/feet_air_time: -0.0036
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5068
Metrics/base_velocity/error_vel_xy: 0.2171
Metrics/base_velocity/error_vel_yaw: 0.1936
      Episode_Termination/time_out: 0.0601
  Episode_Termination/base_contact: 0.8924
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 2.59s
                      Time elapsed: 00:01:19
                               ETA: 01:02:39

################################################################################
                      [1m Learning iteration 29/1400 [0m                      

                       Computation: 9534 steps/s (collection: 2.321s, learning 0.256s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.7474
                       Mean reward: -1.40
               Mean episode length: 131.64
Episode_Reward/track_lin_vel_xy_exp: 0.0272
Episode_Reward/track_ang_vel_z_exp: 0.0248
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0223
     Episode_Reward/dof_torques_l2: -0.0120
         Episode_Reward/dof_acc_l2: -0.0343
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0030
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3281
Metrics/base_velocity/error_vel_xy: 0.1891
Metrics/base_velocity/error_vel_yaw: 0.1480
      Episode_Termination/time_out: 0.0479
  Episode_Termination/base_contact: 0.9161
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 2.58s
                      Time elapsed: 00:01:22
                               ETA: 01:02:29

################################################################################
                      [1m Learning iteration 30/1400 [0m                      

                       Computation: 9637 steps/s (collection: 2.293s, learning 0.257s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.6603
                       Mean reward: -0.89
               Mean episode length: 93.87
Episode_Reward/track_lin_vel_xy_exp: 0.0273
Episode_Reward/track_ang_vel_z_exp: 0.0219
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0195
     Episode_Reward/dof_torques_l2: -0.0105
         Episode_Reward/dof_acc_l2: -0.0299
     Episode_Reward/action_rate_l2: -0.0170
      Episode_Reward/feet_air_time: -0.0025
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1495
Metrics/base_velocity/error_vel_xy: 0.1560
Metrics/base_velocity/error_vel_yaw: 0.1271
      Episode_Termination/time_out: 0.0329
  Episode_Termination/base_contact: 0.9388
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 2.55s
                      Time elapsed: 00:01:24
                               ETA: 01:02:18

################################################################################
                      [1m Learning iteration 31/1400 [0m                      

                       Computation: 9588 steps/s (collection: 2.307s, learning 0.256s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.5760
                       Mean reward: -0.99
               Mean episode length: 97.14
Episode_Reward/track_lin_vel_xy_exp: 0.0204
Episode_Reward/track_ang_vel_z_exp: 0.0204
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.0088
         Episode_Reward/dof_acc_l2: -0.0268
     Episode_Reward/action_rate_l2: -0.0145
      Episode_Reward/feet_air_time: -0.0024
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9853
Metrics/base_velocity/error_vel_xy: 0.1440
Metrics/base_velocity/error_vel_yaw: 0.1048
      Episode_Termination/time_out: 0.0259
  Episode_Termination/base_contact: 0.9511
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 2.56s
                      Time elapsed: 00:01:27
                               ETA: 01:02:08

################################################################################
                      [1m Learning iteration 32/1400 [0m                      

                       Computation: 9643 steps/s (collection: 2.294s, learning 0.254s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0061
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.5107
                       Mean reward: -0.81
               Mean episode length: 91.66
Episode_Reward/track_lin_vel_xy_exp: 0.0247
Episode_Reward/track_ang_vel_z_exp: 0.0220
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.0096
         Episode_Reward/dof_acc_l2: -0.0286
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0025
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8292
Metrics/base_velocity/error_vel_xy: 0.1518
Metrics/base_velocity/error_vel_yaw: 0.1150
      Episode_Termination/time_out: 0.0165
  Episode_Termination/base_contact: 0.9655
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 2.55s
                      Time elapsed: 00:01:29
                               ETA: 01:01:58

################################################################################
                      [1m Learning iteration 33/1400 [0m                      

                       Computation: 8024 steps/s (collection: 2.807s, learning 0.256s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 13.4261
                       Mean reward: -0.83
               Mean episode length: 86.50
Episode_Reward/track_lin_vel_xy_exp: 0.0202
Episode_Reward/track_ang_vel_z_exp: 0.0186
       Episode_Reward/lin_vel_z_l2: -0.0103
      Episode_Reward/ang_vel_xy_l2: -0.0157
     Episode_Reward/dof_torques_l2: -0.0074
         Episode_Reward/dof_acc_l2: -0.0240
     Episode_Reward/action_rate_l2: -0.0124
      Episode_Reward/feet_air_time: -0.0021
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6810
Metrics/base_velocity/error_vel_xy: 0.1239
Metrics/base_velocity/error_vel_yaw: 0.0882
      Episode_Termination/time_out: 0.0112
  Episode_Termination/base_contact: 0.9742
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 3.06s
                      Time elapsed: 00:01:32
                               ETA: 01:02:09

################################################################################
                      [1m Learning iteration 34/1400 [0m                      

                       Computation: 9643 steps/s (collection: 2.293s, learning 0.255s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.3202
                       Mean reward: -0.59
               Mean episode length: 64.27
Episode_Reward/track_lin_vel_xy_exp: 0.0181
Episode_Reward/track_ang_vel_z_exp: 0.0175
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0149
     Episode_Reward/dof_torques_l2: -0.0067
         Episode_Reward/dof_acc_l2: -0.0234
     Episode_Reward/action_rate_l2: -0.0114
      Episode_Reward/feet_air_time: -0.0020
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5612
Metrics/base_velocity/error_vel_xy: 0.1195
Metrics/base_velocity/error_vel_yaw: 0.0848
      Episode_Termination/time_out: 0.0100
  Episode_Termination/base_contact: 0.9799
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 2.55s
                      Time elapsed: 00:01:35
                               ETA: 01:01:59

################################################################################
                      [1m Learning iteration 35/1400 [0m                      

                       Computation: 9528 steps/s (collection: 2.323s, learning 0.257s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 13.2183
                       Mean reward: -0.69
               Mean episode length: 76.84
Episode_Reward/track_lin_vel_xy_exp: 0.0172
Episode_Reward/track_ang_vel_z_exp: 0.0184
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0149
     Episode_Reward/dof_torques_l2: -0.0069
         Episode_Reward/dof_acc_l2: -0.0233
     Episode_Reward/action_rate_l2: -0.0115
      Episode_Reward/feet_air_time: -0.0021
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4551
Metrics/base_velocity/error_vel_xy: 0.1251
Metrics/base_velocity/error_vel_yaw: 0.0834
      Episode_Termination/time_out: 0.0098
  Episode_Termination/base_contact: 0.9830
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 2.58s
                      Time elapsed: 00:01:37
                               ETA: 01:01:51

################################################################################
                      [1m Learning iteration 36/1400 [0m                      

                       Computation: 9525 steps/s (collection: 2.325s, learning 0.255s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.1247
                       Mean reward: -0.75
               Mean episode length: 76.32
Episode_Reward/track_lin_vel_xy_exp: 0.0186
Episode_Reward/track_ang_vel_z_exp: 0.0177
       Episode_Reward/lin_vel_z_l2: -0.0097
      Episode_Reward/ang_vel_xy_l2: -0.0144
     Episode_Reward/dof_torques_l2: -0.0066
         Episode_Reward/dof_acc_l2: -0.0224
     Episode_Reward/action_rate_l2: -0.0108
      Episode_Reward/feet_air_time: -0.0019
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3469
Metrics/base_velocity/error_vel_xy: 0.1134
Metrics/base_velocity/error_vel_yaw: 0.0785
      Episode_Termination/time_out: 0.0086
  Episode_Termination/base_contact: 0.9854
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 2.58s
                      Time elapsed: 00:01:40
                               ETA: 01:01:43

################################################################################
                      [1m Learning iteration 37/1400 [0m                      

                       Computation: 9722 steps/s (collection: 2.271s, learning 0.257s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.0365
                       Mean reward: -0.51
               Mean episode length: 62.54
Episode_Reward/track_lin_vel_xy_exp: 0.0176
Episode_Reward/track_ang_vel_z_exp: 0.0166
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0136
     Episode_Reward/dof_torques_l2: -0.0062
         Episode_Reward/dof_acc_l2: -0.0209
     Episode_Reward/action_rate_l2: -0.0102
      Episode_Reward/feet_air_time: -0.0018
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2698
Metrics/base_velocity/error_vel_xy: 0.1048
Metrics/base_velocity/error_vel_yaw: 0.0741
      Episode_Termination/time_out: 0.0052
  Episode_Termination/base_contact: 0.9904
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 2.53s
                      Time elapsed: 00:01:42
                               ETA: 01:01:34

################################################################################
                      [1m Learning iteration 38/1400 [0m                      

                       Computation: 9707 steps/s (collection: 2.275s, learning 0.256s)
             Mean action noise std: 0.71
          Mean value_function loss: 0.0061
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 12.9649
                       Mean reward: -0.48
               Mean episode length: 65.83
Episode_Reward/track_lin_vel_xy_exp: 0.0167
Episode_Reward/track_ang_vel_z_exp: 0.0170
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0129
     Episode_Reward/dof_torques_l2: -0.0058
         Episode_Reward/dof_acc_l2: -0.0205
     Episode_Reward/action_rate_l2: -0.0094
      Episode_Reward/feet_air_time: -0.0018
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2054
Metrics/base_velocity/error_vel_xy: 0.1027
Metrics/base_velocity/error_vel_yaw: 0.0667
      Episode_Termination/time_out: 0.0044
  Episode_Termination/base_contact: 0.9921
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 2.53s
                      Time elapsed: 00:01:45
                               ETA: 01:01:25

################################################################################
                      [1m Learning iteration 39/1400 [0m                      

                       Computation: 9729 steps/s (collection: 2.270s, learning 0.255s)
             Mean action noise std: 0.71
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 12.8724
                       Mean reward: -0.41
               Mean episode length: 62.38
Episode_Reward/track_lin_vel_xy_exp: 0.0196
Episode_Reward/track_ang_vel_z_exp: 0.0173
       Episode_Reward/lin_vel_z_l2: -0.0094
      Episode_Reward/ang_vel_xy_l2: -0.0130
     Episode_Reward/dof_torques_l2: -0.0057
         Episode_Reward/dof_acc_l2: -0.0200
     Episode_Reward/action_rate_l2: -0.0093
      Episode_Reward/feet_air_time: -0.0019
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1534
Metrics/base_velocity/error_vel_xy: 0.0979
Metrics/base_velocity/error_vel_yaw: 0.0660
      Episode_Termination/time_out: 0.0035
  Episode_Termination/base_contact: 0.9936
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 2.53s
                      Time elapsed: 00:01:48
                               ETA: 01:01:16

################################################################################
                      [1m Learning iteration 40/1400 [0m                      

                       Computation: 9725 steps/s (collection: 2.272s, learning 0.255s)
             Mean action noise std: 0.70
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 12.7685
                       Mean reward: -0.45
               Mean episode length: 60.20
Episode_Reward/track_lin_vel_xy_exp: 0.0188
Episode_Reward/track_ang_vel_z_exp: 0.0170
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0122
     Episode_Reward/dof_torques_l2: -0.0057
         Episode_Reward/dof_acc_l2: -0.0193
     Episode_Reward/action_rate_l2: -0.0091
      Episode_Reward/feet_air_time: -0.0017
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1174
Metrics/base_velocity/error_vel_xy: 0.0941
Metrics/base_velocity/error_vel_yaw: 0.0631
      Episode_Termination/time_out: 0.0041
  Episode_Termination/base_contact: 0.9941
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 2.53s
                      Time elapsed: 00:01:50
                               ETA: 01:01:08

################################################################################
                      [1m Learning iteration 41/1400 [0m                      

                       Computation: 9607 steps/s (collection: 2.303s, learning 0.255s)
             Mean action noise std: 0.69
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 12.6624
                       Mean reward: -0.42
               Mean episode length: 62.04
Episode_Reward/track_lin_vel_xy_exp: 0.0173
Episode_Reward/track_ang_vel_z_exp: 0.0168
       Episode_Reward/lin_vel_z_l2: -0.0093
      Episode_Reward/ang_vel_xy_l2: -0.0120
     Episode_Reward/dof_torques_l2: -0.0054
         Episode_Reward/dof_acc_l2: -0.0187
     Episode_Reward/action_rate_l2: -0.0087
      Episode_Reward/feet_air_time: -0.0017
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0945
Metrics/base_velocity/error_vel_xy: 0.0948
Metrics/base_velocity/error_vel_yaw: 0.0615
      Episode_Termination/time_out: 0.0052
  Episode_Termination/base_contact: 0.9941
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 2.56s
                      Time elapsed: 00:01:53
                               ETA: 01:01:00

################################################################################
                      [1m Learning iteration 42/1400 [0m                      

                       Computation: 9722 steps/s (collection: 2.272s, learning 0.256s)
             Mean action noise std: 0.69
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 12.5724
                       Mean reward: -0.39
               Mean episode length: 64.19
Episode_Reward/track_lin_vel_xy_exp: 0.0193
Episode_Reward/track_ang_vel_z_exp: 0.0183
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0122
     Episode_Reward/dof_torques_l2: -0.0055
         Episode_Reward/dof_acc_l2: -0.0188
     Episode_Reward/action_rate_l2: -0.0090
      Episode_Reward/feet_air_time: -0.0018
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0672
Metrics/base_velocity/error_vel_xy: 0.0957
Metrics/base_velocity/error_vel_yaw: 0.0592
      Episode_Termination/time_out: 0.0042
  Episode_Termination/base_contact: 0.9958
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 2.53s
                      Time elapsed: 00:01:55
                               ETA: 01:00:52

################################################################################
                      [1m Learning iteration 43/1400 [0m                      

                       Computation: 9707 steps/s (collection: 2.277s, learning 0.255s)
             Mean action noise std: 0.68
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 12.4866
                       Mean reward: -0.34
               Mean episode length: 61.22
Episode_Reward/track_lin_vel_xy_exp: 0.0175
Episode_Reward/track_ang_vel_z_exp: 0.0168
       Episode_Reward/lin_vel_z_l2: -0.0091
      Episode_Reward/ang_vel_xy_l2: -0.0112
     Episode_Reward/dof_torques_l2: -0.0053
         Episode_Reward/dof_acc_l2: -0.0178
     Episode_Reward/action_rate_l2: -0.0082
      Episode_Reward/feet_air_time: -0.0016
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0517
Metrics/base_velocity/error_vel_xy: 0.0907
Metrics/base_velocity/error_vel_yaw: 0.0563
      Episode_Termination/time_out: 0.0038
  Episode_Termination/base_contact: 0.9962
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 2.53s
                      Time elapsed: 00:01:58
                               ETA: 01:00:45

################################################################################
                      [1m Learning iteration 44/1400 [0m                      

                       Computation: 9692 steps/s (collection: 2.280s, learning 0.256s)
             Mean action noise std: 0.68
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 12.4014
                       Mean reward: -0.36
               Mean episode length: 61.03
Episode_Reward/track_lin_vel_xy_exp: 0.0175
Episode_Reward/track_ang_vel_z_exp: 0.0168
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0115
     Episode_Reward/dof_torques_l2: -0.0051
         Episode_Reward/dof_acc_l2: -0.0170
     Episode_Reward/action_rate_l2: -0.0081
      Episode_Reward/feet_air_time: -0.0016
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0411
Metrics/base_velocity/error_vel_xy: 0.0906
Metrics/base_velocity/error_vel_yaw: 0.0571
      Episode_Termination/time_out: 0.0039
  Episode_Termination/base_contact: 0.9961
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 2.54s
                      Time elapsed: 00:02:00
                               ETA: 01:00:38

################################################################################
                      [1m Learning iteration 45/1400 [0m                      

                       Computation: 9599 steps/s (collection: 2.304s, learning 0.256s)
             Mean action noise std: 0.68
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 12.3311
                       Mean reward: -0.31
               Mean episode length: 76.71
Episode_Reward/track_lin_vel_xy_exp: 0.0203
Episode_Reward/track_ang_vel_z_exp: 0.0188
       Episode_Reward/lin_vel_z_l2: -0.0092
      Episode_Reward/ang_vel_xy_l2: -0.0115
     Episode_Reward/dof_torques_l2: -0.0056
         Episode_Reward/dof_acc_l2: -0.0177
     Episode_Reward/action_rate_l2: -0.0087
      Episode_Reward/feet_air_time: -0.0017
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0315
Metrics/base_velocity/error_vel_xy: 0.0929
Metrics/base_velocity/error_vel_yaw: 0.0583
      Episode_Termination/time_out: 0.0029
  Episode_Termination/base_contact: 0.9971
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 2.56s
                      Time elapsed: 00:02:03
                               ETA: 01:00:31

################################################################################
                      [1m Learning iteration 46/1400 [0m                      

                       Computation: 9767 steps/s (collection: 2.262s, learning 0.254s)
             Mean action noise std: 0.67
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 12.2508
                       Mean reward: -0.24
               Mean episode length: 66.97
Episode_Reward/track_lin_vel_xy_exp: 0.0212
Episode_Reward/track_ang_vel_z_exp: 0.0189
       Episode_Reward/lin_vel_z_l2: -0.0085
      Episode_Reward/ang_vel_xy_l2: -0.0112
     Episode_Reward/dof_torques_l2: -0.0053
         Episode_Reward/dof_acc_l2: -0.0174
     Episode_Reward/action_rate_l2: -0.0083
      Episode_Reward/feet_air_time: -0.0017
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0223
Metrics/base_velocity/error_vel_xy: 0.0863
Metrics/base_velocity/error_vel_yaw: 0.0534
      Episode_Termination/time_out: 0.0029
  Episode_Termination/base_contact: 0.9971
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 2.52s
                      Time elapsed: 00:02:05
                               ETA: 01:00:24

################################################################################
                      [1m Learning iteration 47/1400 [0m                      

                       Computation: 9799 steps/s (collection: 2.252s, learning 0.256s)
             Mean action noise std: 0.67
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 12.1636
                       Mean reward: -0.23
               Mean episode length: 66.72
Episode_Reward/track_lin_vel_xy_exp: 0.0218
Episode_Reward/track_ang_vel_z_exp: 0.0198
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0111
     Episode_Reward/dof_torques_l2: -0.0058
         Episode_Reward/dof_acc_l2: -0.0181
     Episode_Reward/action_rate_l2: -0.0090
      Episode_Reward/feet_air_time: -0.0018
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0168
Metrics/base_velocity/error_vel_xy: 0.0941
Metrics/base_velocity/error_vel_yaw: 0.0591
      Episode_Termination/time_out: 0.0029
  Episode_Termination/base_contact: 0.9971
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 2.51s
                      Time elapsed: 00:02:08
                               ETA: 01:00:17

################################################################################
                      [1m Learning iteration 48/1400 [0m                      

                       Computation: 9720 steps/s (collection: 2.272s, learning 0.256s)
             Mean action noise std: 0.66
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 12.0851
                       Mean reward: -0.33
               Mean episode length: 59.25
Episode_Reward/track_lin_vel_xy_exp: 0.0227
Episode_Reward/track_ang_vel_z_exp: 0.0187
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0114
     Episode_Reward/dof_torques_l2: -0.0056
         Episode_Reward/dof_acc_l2: -0.0175
     Episode_Reward/action_rate_l2: -0.0085
      Episode_Reward/feet_air_time: -0.0017
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0134
Metrics/base_velocity/error_vel_xy: 0.0874
Metrics/base_velocity/error_vel_yaw: 0.0586
      Episode_Termination/time_out: 0.0021
  Episode_Termination/base_contact: 0.9979
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 2.53s
                      Time elapsed: 00:02:10
                               ETA: 01:00:10

################################################################################
                      [1m Learning iteration 49/1400 [0m                      

                       Computation: 9687 steps/s (collection: 2.282s, learning 0.255s)
             Mean action noise std: 0.66
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 12.0203
                       Mean reward: -0.26
               Mean episode length: 65.59
Episode_Reward/track_lin_vel_xy_exp: 0.0224
Episode_Reward/track_ang_vel_z_exp: 0.0188
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0107
     Episode_Reward/dof_torques_l2: -0.0054
         Episode_Reward/dof_acc_l2: -0.0171
     Episode_Reward/action_rate_l2: -0.0081
      Episode_Reward/feet_air_time: -0.0017
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0127
Metrics/base_velocity/error_vel_xy: 0.0843
Metrics/base_velocity/error_vel_yaw: 0.0557
      Episode_Termination/time_out: 0.0020
  Episode_Termination/base_contact: 0.9980
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 2.54s
                      Time elapsed: 00:02:13
                               ETA: 01:00:04

################################################################################
                      [1m Learning iteration 50/1400 [0m                      

                       Computation: 9755 steps/s (collection: 2.263s, learning 0.256s)
             Mean action noise std: 0.66
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 11.9560
                       Mean reward: -0.23
               Mean episode length: 65.50
Episode_Reward/track_lin_vel_xy_exp: 0.0226
Episode_Reward/track_ang_vel_z_exp: 0.0188
       Episode_Reward/lin_vel_z_l2: -0.0086
      Episode_Reward/ang_vel_xy_l2: -0.0100
     Episode_Reward/dof_torques_l2: -0.0053
         Episode_Reward/dof_acc_l2: -0.0169
     Episode_Reward/action_rate_l2: -0.0080
      Episode_Reward/feet_air_time: -0.0017
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0118
Metrics/base_velocity/error_vel_xy: 0.0832
Metrics/base_velocity/error_vel_yaw: 0.0550
      Episode_Termination/time_out: 0.0016
  Episode_Termination/base_contact: 0.9984
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 2.52s
                      Time elapsed: 00:02:15
                               ETA: 00:59:57

################################################################################
                      [1m Learning iteration 51/1400 [0m                      

                       Computation: 9868 steps/s (collection: 2.236s, learning 0.255s)
             Mean action noise std: 0.65
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 11.8937
                       Mean reward: -0.21
               Mean episode length: 71.48
Episode_Reward/track_lin_vel_xy_exp: 0.0245
Episode_Reward/track_ang_vel_z_exp: 0.0219
       Episode_Reward/lin_vel_z_l2: -0.0091
      Episode_Reward/ang_vel_xy_l2: -0.0108
     Episode_Reward/dof_torques_l2: -0.0059
         Episode_Reward/dof_acc_l2: -0.0188
     Episode_Reward/action_rate_l2: -0.0090
      Episode_Reward/feet_air_time: -0.0019
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0101
Metrics/base_velocity/error_vel_xy: 0.0935
Metrics/base_velocity/error_vel_yaw: 0.0575
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 2.49s
                      Time elapsed: 00:02:18
                               ETA: 00:59:50

################################################################################
                      [1m Learning iteration 52/1400 [0m                      

                       Computation: 9704 steps/s (collection: 2.277s, learning 0.255s)
             Mean action noise std: 0.65
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 11.8144
                       Mean reward: -0.16
               Mean episode length: 68.44
Episode_Reward/track_lin_vel_xy_exp: 0.0249
Episode_Reward/track_ang_vel_z_exp: 0.0207
       Episode_Reward/lin_vel_z_l2: -0.0083
      Episode_Reward/ang_vel_xy_l2: -0.0103
     Episode_Reward/dof_torques_l2: -0.0057
         Episode_Reward/dof_acc_l2: -0.0181
     Episode_Reward/action_rate_l2: -0.0086
      Episode_Reward/feet_air_time: -0.0019
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0098
Metrics/base_velocity/error_vel_xy: 0.0889
Metrics/base_velocity/error_vel_yaw: 0.0589
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 2.53s
                      Time elapsed: 00:02:20
                               ETA: 00:59:44

################################################################################
                      [1m Learning iteration 53/1400 [0m                      

                       Computation: 9765 steps/s (collection: 2.261s, learning 0.256s)
             Mean action noise std: 0.64
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 11.7329
                       Mean reward: -0.13
               Mean episode length: 70.30
Episode_Reward/track_lin_vel_xy_exp: 0.0262
Episode_Reward/track_ang_vel_z_exp: 0.0213
       Episode_Reward/lin_vel_z_l2: -0.0088
      Episode_Reward/ang_vel_xy_l2: -0.0106
     Episode_Reward/dof_torques_l2: -0.0059
         Episode_Reward/dof_acc_l2: -0.0187
     Episode_Reward/action_rate_l2: -0.0089
      Episode_Reward/feet_air_time: -0.0019
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0096
Metrics/base_velocity/error_vel_xy: 0.0889
Metrics/base_velocity/error_vel_yaw: 0.0604
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 2.52s
                      Time elapsed: 00:02:23
                               ETA: 00:59:38

################################################################################
                      [1m Learning iteration 54/1400 [0m                      

                       Computation: 9813 steps/s (collection: 2.249s, learning 0.255s)
             Mean action noise std: 0.64
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 11.6562
                       Mean reward: -0.08
               Mean episode length: 77.84
Episode_Reward/track_lin_vel_xy_exp: 0.0307
Episode_Reward/track_ang_vel_z_exp: 0.0232
       Episode_Reward/lin_vel_z_l2: -0.0085
      Episode_Reward/ang_vel_xy_l2: -0.0106
     Episode_Reward/dof_torques_l2: -0.0062
         Episode_Reward/dof_acc_l2: -0.0196
     Episode_Reward/action_rate_l2: -0.0095
      Episode_Reward/feet_air_time: -0.0021
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0082
Metrics/base_velocity/error_vel_xy: 0.0908
Metrics/base_velocity/error_vel_yaw: 0.0625
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 2.50s
                      Time elapsed: 00:02:25
                               ETA: 00:59:31

################################################################################
                      [1m Learning iteration 55/1400 [0m                      

                       Computation: 9791 steps/s (collection: 2.255s, learning 0.255s)
             Mean action noise std: 0.63
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 11.5711
                       Mean reward: -0.09
               Mean episode length: 70.56
Episode_Reward/track_lin_vel_xy_exp: 0.0295
Episode_Reward/track_ang_vel_z_exp: 0.0226
       Episode_Reward/lin_vel_z_l2: -0.0082
      Episode_Reward/ang_vel_xy_l2: -0.0102
     Episode_Reward/dof_torques_l2: -0.0061
         Episode_Reward/dof_acc_l2: -0.0186
     Episode_Reward/action_rate_l2: -0.0093
      Episode_Reward/feet_air_time: -0.0021
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0061
Metrics/base_velocity/error_vel_xy: 0.0921
Metrics/base_velocity/error_vel_yaw: 0.0628
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 2.51s
                      Time elapsed: 00:02:28
                               ETA: 00:59:25

################################################################################
                      [1m Learning iteration 56/1400 [0m                      

                       Computation: 9814 steps/s (collection: 2.246s, learning 0.257s)
             Mean action noise std: 0.63
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 11.4913
                       Mean reward: -0.15
               Mean episode length: 74.70
Episode_Reward/track_lin_vel_xy_exp: 0.0284
Episode_Reward/track_ang_vel_z_exp: 0.0227
       Episode_Reward/lin_vel_z_l2: -0.0087
      Episode_Reward/ang_vel_xy_l2: -0.0105
     Episode_Reward/dof_torques_l2: -0.0061
         Episode_Reward/dof_acc_l2: -0.0194
     Episode_Reward/action_rate_l2: -0.0093
      Episode_Reward/feet_air_time: -0.0021
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0059
Metrics/base_velocity/error_vel_xy: 0.0948
Metrics/base_velocity/error_vel_yaw: 0.0657
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 2.50s
                      Time elapsed: 00:02:30
                               ETA: 00:59:19

################################################################################
                      [1m Learning iteration 57/1400 [0m                      

                       Computation: 9942 steps/s (collection: 2.217s, learning 0.255s)
             Mean action noise std: 0.63
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 11.4186
                       Mean reward: -0.06
               Mean episode length: 83.70
Episode_Reward/track_lin_vel_xy_exp: 0.0292
Episode_Reward/track_ang_vel_z_exp: 0.0228
       Episode_Reward/lin_vel_z_l2: -0.0086
      Episode_Reward/ang_vel_xy_l2: -0.0106
     Episode_Reward/dof_torques_l2: -0.0062
         Episode_Reward/dof_acc_l2: -0.0184
     Episode_Reward/action_rate_l2: -0.0092
      Episode_Reward/feet_air_time: -0.0020
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0059
Metrics/base_velocity/error_vel_xy: 0.0954
Metrics/base_velocity/error_vel_yaw: 0.0656
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 2.47s
                      Time elapsed: 00:02:33
                               ETA: 00:59:12

################################################################################
                      [1m Learning iteration 58/1400 [0m                      

                       Computation: 9832 steps/s (collection: 2.245s, learning 0.254s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 11.3636
                       Mean reward: -0.00
               Mean episode length: 90.92
Episode_Reward/track_lin_vel_xy_exp: 0.0341
Episode_Reward/track_ang_vel_z_exp: 0.0252
       Episode_Reward/lin_vel_z_l2: -0.0088
      Episode_Reward/ang_vel_xy_l2: -0.0109
     Episode_Reward/dof_torques_l2: -0.0067
         Episode_Reward/dof_acc_l2: -0.0199
     Episode_Reward/action_rate_l2: -0.0101
      Episode_Reward/feet_air_time: -0.0023
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0051
Metrics/base_velocity/error_vel_xy: 0.0978
Metrics/base_velocity/error_vel_yaw: 0.0711
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 2.50s
                      Time elapsed: 00:02:35
                               ETA: 00:59:06

################################################################################
                      [1m Learning iteration 59/1400 [0m                      

                       Computation: 9783 steps/s (collection: 2.256s, learning 0.256s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 11.2983
                       Mean reward: -0.06
               Mean episode length: 85.23
Episode_Reward/track_lin_vel_xy_exp: 0.0337
Episode_Reward/track_ang_vel_z_exp: 0.0243
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0109
     Episode_Reward/dof_torques_l2: -0.0068
         Episode_Reward/dof_acc_l2: -0.0199
     Episode_Reward/action_rate_l2: -0.0101
      Episode_Reward/feet_air_time: -0.0023
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0046
Metrics/base_velocity/error_vel_xy: 0.1002
Metrics/base_velocity/error_vel_yaw: 0.0746
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 2.51s
                      Time elapsed: 00:02:38
                               ETA: 00:59:01

################################################################################
                      [1m Learning iteration 60/1400 [0m                      

                       Computation: 9778 steps/s (collection: 2.260s, learning 0.253s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 11.2197
                       Mean reward: 0.03
               Mean episode length: 88.34
Episode_Reward/track_lin_vel_xy_exp: 0.0339
Episode_Reward/track_ang_vel_z_exp: 0.0251
       Episode_Reward/lin_vel_z_l2: -0.0091
      Episode_Reward/ang_vel_xy_l2: -0.0106
     Episode_Reward/dof_torques_l2: -0.0066
         Episode_Reward/dof_acc_l2: -0.0195
     Episode_Reward/action_rate_l2: -0.0097
      Episode_Reward/feet_air_time: -0.0023
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0037
Metrics/base_velocity/error_vel_xy: 0.0931
Metrics/base_velocity/error_vel_yaw: 0.0662
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 2.51s
                      Time elapsed: 00:02:40
                               ETA: 00:58:55

################################################################################
                      [1m Learning iteration 61/1400 [0m                      

                       Computation: 9777 steps/s (collection: 2.260s, learning 0.254s)
             Mean action noise std: 0.61
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 11.1644
                       Mean reward: 0.00
               Mean episode length: 85.33
Episode_Reward/track_lin_vel_xy_exp: 0.0323
Episode_Reward/track_ang_vel_z_exp: 0.0253
       Episode_Reward/lin_vel_z_l2: -0.0088
      Episode_Reward/ang_vel_xy_l2: -0.0105
     Episode_Reward/dof_torques_l2: -0.0069
         Episode_Reward/dof_acc_l2: -0.0200
     Episode_Reward/action_rate_l2: -0.0101
      Episode_Reward/feet_air_time: -0.0024
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0023
Metrics/base_velocity/error_vel_xy: 0.1035
Metrics/base_velocity/error_vel_yaw: 0.0732
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 2.51s
                      Time elapsed: 00:02:43
                               ETA: 00:58:50

################################################################################
                      [1m Learning iteration 62/1400 [0m                      

                       Computation: 9952 steps/s (collection: 2.216s, learning 0.253s)
             Mean action noise std: 0.61
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 11.1122
                       Mean reward: 0.07
               Mean episode length: 88.81
Episode_Reward/track_lin_vel_xy_exp: 0.0369
Episode_Reward/track_ang_vel_z_exp: 0.0278
       Episode_Reward/lin_vel_z_l2: -0.0087
      Episode_Reward/ang_vel_xy_l2: -0.0107
     Episode_Reward/dof_torques_l2: -0.0071
         Episode_Reward/dof_acc_l2: -0.0203
     Episode_Reward/action_rate_l2: -0.0105
      Episode_Reward/feet_air_time: -0.0024
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0015
Metrics/base_velocity/error_vel_xy: 0.1038
Metrics/base_velocity/error_vel_yaw: 0.0718
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 2.47s
                      Time elapsed: 00:02:45
                               ETA: 00:58:44

################################################################################
                      [1m Learning iteration 63/1400 [0m                      

                       Computation: 9782 steps/s (collection: 2.259s, learning 0.253s)
             Mean action noise std: 0.61
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 11.0236
                       Mean reward: -0.09
               Mean episode length: 99.12
Episode_Reward/track_lin_vel_xy_exp: 0.0294
Episode_Reward/track_ang_vel_z_exp: 0.0253
       Episode_Reward/lin_vel_z_l2: -0.0087
      Episode_Reward/ang_vel_xy_l2: -0.0108
     Episode_Reward/dof_torques_l2: -0.0073
         Episode_Reward/dof_acc_l2: -0.0214
     Episode_Reward/action_rate_l2: -0.0103
      Episode_Reward/feet_air_time: -0.0025
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0009
Metrics/base_velocity/error_vel_xy: 0.1168
Metrics/base_velocity/error_vel_yaw: 0.0810
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 2.51s
                      Time elapsed: 00:02:48
                               ETA: 00:58:39

################################################################################
                      [1m Learning iteration 64/1400 [0m                      

                       Computation: 9794 steps/s (collection: 2.254s, learning 0.255s)
             Mean action noise std: 0.60
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 10.9724
                       Mean reward: 0.21
               Mean episode length: 114.58
Episode_Reward/track_lin_vel_xy_exp: 0.0445
Episode_Reward/track_ang_vel_z_exp: 0.0328
       Episode_Reward/lin_vel_z_l2: -0.0088
      Episode_Reward/ang_vel_xy_l2: -0.0119
     Episode_Reward/dof_torques_l2: -0.0083
         Episode_Reward/dof_acc_l2: -0.0227
     Episode_Reward/action_rate_l2: -0.0123
      Episode_Reward/feet_air_time: -0.0027
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1182
Metrics/base_velocity/error_vel_yaw: 0.0830
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 2.51s
                      Time elapsed: 00:02:50
                               ETA: 00:58:33

################################################################################
                      [1m Learning iteration 65/1400 [0m                      

                       Computation: 9847 steps/s (collection: 2.240s, learning 0.255s)
             Mean action noise std: 0.60
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 10.8934
                       Mean reward: 0.08
               Mean episode length: 100.98
Episode_Reward/track_lin_vel_xy_exp: 0.0407
Episode_Reward/track_ang_vel_z_exp: 0.0299
       Episode_Reward/lin_vel_z_l2: -0.0092
      Episode_Reward/ang_vel_xy_l2: -0.0110
     Episode_Reward/dof_torques_l2: -0.0079
         Episode_Reward/dof_acc_l2: -0.0218
     Episode_Reward/action_rate_l2: -0.0115
      Episode_Reward/feet_air_time: -0.0027
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1137
Metrics/base_velocity/error_vel_yaw: 0.0838
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 2.50s
                      Time elapsed: 00:02:53
                               ETA: 00:58:28

################################################################################
                      [1m Learning iteration 66/1400 [0m                      

                       Computation: 10069 steps/s (collection: 2.187s, learning 0.254s)
             Mean action noise std: 0.60
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 10.8514
                       Mean reward: 0.12
               Mean episode length: 102.72
Episode_Reward/track_lin_vel_xy_exp: 0.0414
Episode_Reward/track_ang_vel_z_exp: 0.0316
       Episode_Reward/lin_vel_z_l2: -0.0091
      Episode_Reward/ang_vel_xy_l2: -0.0114
     Episode_Reward/dof_torques_l2: -0.0078
         Episode_Reward/dof_acc_l2: -0.0214
     Episode_Reward/action_rate_l2: -0.0115
      Episode_Reward/feet_air_time: -0.0026
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1118
Metrics/base_velocity/error_vel_yaw: 0.0778
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 2.44s
                      Time elapsed: 00:02:55
                               ETA: 00:58:22

################################################################################
                      [1m Learning iteration 67/1400 [0m                      

                       Computation: 9816 steps/s (collection: 2.249s, learning 0.255s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 10.7951
                       Mean reward: 0.26
               Mean episode length: 112.24
Episode_Reward/track_lin_vel_xy_exp: 0.0460
Episode_Reward/track_ang_vel_z_exp: 0.0339
       Episode_Reward/lin_vel_z_l2: -0.0087
      Episode_Reward/ang_vel_xy_l2: -0.0119
     Episode_Reward/dof_torques_l2: -0.0082
         Episode_Reward/dof_acc_l2: -0.0233
     Episode_Reward/action_rate_l2: -0.0120
      Episode_Reward/feet_air_time: -0.0028
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1145
Metrics/base_velocity/error_vel_yaw: 0.0782
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 2.50s
                      Time elapsed: 00:02:58
                               ETA: 00:58:17

################################################################################
                      [1m Learning iteration 68/1400 [0m                      

                       Computation: 9882 steps/s (collection: 2.233s, learning 0.254s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 10.7294
                       Mean reward: 0.39
               Mean episode length: 125.55
Episode_Reward/track_lin_vel_xy_exp: 0.0518
Episode_Reward/track_ang_vel_z_exp: 0.0345
       Episode_Reward/lin_vel_z_l2: -0.0095
      Episode_Reward/ang_vel_xy_l2: -0.0121
     Episode_Reward/dof_torques_l2: -0.0082
         Episode_Reward/dof_acc_l2: -0.0232
     Episode_Reward/action_rate_l2: -0.0123
      Episode_Reward/feet_air_time: -0.0028
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1124
Metrics/base_velocity/error_vel_yaw: 0.0832
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 2.49s
                      Time elapsed: 00:03:00
                               ETA: 00:58:11

################################################################################
                      [1m Learning iteration 69/1400 [0m                      

                       Computation: 9840 steps/s (collection: 2.243s, learning 0.254s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 10.6801
                       Mean reward: 0.60
               Mean episode length: 135.42
Episode_Reward/track_lin_vel_xy_exp: 0.0621
Episode_Reward/track_ang_vel_z_exp: 0.0432
       Episode_Reward/lin_vel_z_l2: -0.0094
      Episode_Reward/ang_vel_xy_l2: -0.0136
     Episode_Reward/dof_torques_l2: -0.0097
         Episode_Reward/dof_acc_l2: -0.0240
     Episode_Reward/action_rate_l2: -0.0144
      Episode_Reward/feet_air_time: -0.0031
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1265
Metrics/base_velocity/error_vel_yaw: 0.0870
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 2.50s
                      Time elapsed: 00:03:03
                               ETA: 00:58:06

################################################################################
                      [1m Learning iteration 70/1400 [0m                      

                       Computation: 9998 steps/s (collection: 2.206s, learning 0.252s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0086
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 10.6527
                       Mean reward: 0.28
               Mean episode length: 135.53
Episode_Reward/track_lin_vel_xy_exp: 0.0514
Episode_Reward/track_ang_vel_z_exp: 0.0401
       Episode_Reward/lin_vel_z_l2: -0.0093
      Episode_Reward/ang_vel_xy_l2: -0.0134
     Episode_Reward/dof_torques_l2: -0.0099
         Episode_Reward/dof_acc_l2: -0.0255
     Episode_Reward/action_rate_l2: -0.0143
      Episode_Reward/feet_air_time: -0.0032
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1438
Metrics/base_velocity/error_vel_yaw: 0.0987
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 2.46s
                      Time elapsed: 00:03:05
                               ETA: 00:58:01

################################################################################
                      [1m Learning iteration 71/1400 [0m                      

                       Computation: 9908 steps/s (collection: 2.231s, learning 0.250s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 10.6133
                       Mean reward: 0.39
               Mean episode length: 120.97
Episode_Reward/track_lin_vel_xy_exp: 0.0551
Episode_Reward/track_ang_vel_z_exp: 0.0419
       Episode_Reward/lin_vel_z_l2: -0.0101
      Episode_Reward/ang_vel_xy_l2: -0.0134
     Episode_Reward/dof_torques_l2: -0.0095
         Episode_Reward/dof_acc_l2: -0.0251
     Episode_Reward/action_rate_l2: -0.0140
      Episode_Reward/feet_air_time: -0.0029
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1373
Metrics/base_velocity/error_vel_yaw: 0.0897
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 2.48s
                      Time elapsed: 00:03:08
                               ETA: 00:57:56

################################################################################
                      [1m Learning iteration 72/1400 [0m                      

                       Computation: 10044 steps/s (collection: 2.195s, learning 0.252s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 10.5793
                       Mean reward: 0.89
               Mean episode length: 157.80
Episode_Reward/track_lin_vel_xy_exp: 0.0820
Episode_Reward/track_ang_vel_z_exp: 0.0528
       Episode_Reward/lin_vel_z_l2: -0.0102
      Episode_Reward/ang_vel_xy_l2: -0.0156
     Episode_Reward/dof_torques_l2: -0.0111
         Episode_Reward/dof_acc_l2: -0.0302
     Episode_Reward/action_rate_l2: -0.0173
      Episode_Reward/feet_air_time: -0.0037
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1419
Metrics/base_velocity/error_vel_yaw: 0.1027
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 2.45s
                      Time elapsed: 00:03:10
                               ETA: 00:57:50

################################################################################
                      [1m Learning iteration 73/1400 [0m                      

                       Computation: 9954 steps/s (collection: 2.217s, learning 0.252s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 10.5354
                       Mean reward: 0.77
               Mean episode length: 159.89
Episode_Reward/track_lin_vel_xy_exp: 0.0722
Episode_Reward/track_ang_vel_z_exp: 0.0483
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0144
     Episode_Reward/dof_torques_l2: -0.0110
         Episode_Reward/dof_acc_l2: -0.0282
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0037
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1406
Metrics/base_velocity/error_vel_yaw: 0.1033
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 2.47s
                      Time elapsed: 00:03:13
                               ETA: 00:57:45

################################################################################
                      [1m Learning iteration 74/1400 [0m                      

                       Computation: 9821 steps/s (collection: 2.248s, learning 0.254s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0076
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 10.4894
                       Mean reward: 0.67
               Mean episode length: 157.68
Episode_Reward/track_lin_vel_xy_exp: 0.0739
Episode_Reward/track_ang_vel_z_exp: 0.0517
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0148
     Episode_Reward/dof_torques_l2: -0.0117
         Episode_Reward/dof_acc_l2: -0.0299
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0038
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1587
Metrics/base_velocity/error_vel_yaw: 0.1151
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 2.50s
                      Time elapsed: 00:03:15
                               ETA: 00:57:40

################################################################################
                      [1m Learning iteration 75/1400 [0m                      

                       Computation: 9842 steps/s (collection: 2.240s, learning 0.257s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0068
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 10.4571
                       Mean reward: 1.14
               Mean episode length: 182.69
Episode_Reward/track_lin_vel_xy_exp: 0.0898
Episode_Reward/track_ang_vel_z_exp: 0.0600
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0164
     Episode_Reward/dof_torques_l2: -0.0128
         Episode_Reward/dof_acc_l2: -0.0319
     Episode_Reward/action_rate_l2: -0.0190
      Episode_Reward/feet_air_time: -0.0038
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1563
Metrics/base_velocity/error_vel_yaw: 0.1089
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 2.50s
                      Time elapsed: 00:03:18
                               ETA: 00:57:36

################################################################################
                      [1m Learning iteration 76/1400 [0m                      

                       Computation: 9854 steps/s (collection: 2.242s, learning 0.252s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 10.4060
                       Mean reward: 1.12
               Mean episode length: 187.80
Episode_Reward/track_lin_vel_xy_exp: 0.0869
Episode_Reward/track_ang_vel_z_exp: 0.0570
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.0127
         Episode_Reward/dof_acc_l2: -0.0314
     Episode_Reward/action_rate_l2: -0.0190
      Episode_Reward/feet_air_time: -0.0039
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1663
Metrics/base_velocity/error_vel_yaw: 0.1223
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 2.49s
                      Time elapsed: 00:03:20
                               ETA: 00:57:31

################################################################################
                      [1m Learning iteration 77/1400 [0m                      

                       Computation: 9878 steps/s (collection: 2.233s, learning 0.255s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0073
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 10.3522
                       Mean reward: 0.78
               Mean episode length: 195.27
Episode_Reward/track_lin_vel_xy_exp: 0.0752
Episode_Reward/track_ang_vel_z_exp: 0.0590
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0166
     Episode_Reward/dof_torques_l2: -0.0128
         Episode_Reward/dof_acc_l2: -0.0326
     Episode_Reward/action_rate_l2: -0.0190
      Episode_Reward/feet_air_time: -0.0042
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1901
Metrics/base_velocity/error_vel_yaw: 0.1175
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 2.49s
                      Time elapsed: 00:03:23
                               ETA: 00:57:26

################################################################################
                      [1m Learning iteration 78/1400 [0m                      

                       Computation: 9886 steps/s (collection: 2.233s, learning 0.253s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 10.3193
                       Mean reward: 0.84
               Mean episode length: 169.99
Episode_Reward/track_lin_vel_xy_exp: 0.0972
Episode_Reward/track_ang_vel_z_exp: 0.0656
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.0142
         Episode_Reward/dof_acc_l2: -0.0337
     Episode_Reward/action_rate_l2: -0.0210
      Episode_Reward/feet_air_time: -0.0044
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1783
Metrics/base_velocity/error_vel_yaw: 0.1271
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 2.49s
                      Time elapsed: 00:03:25
                               ETA: 00:57:22

################################################################################
                      [1m Learning iteration 79/1400 [0m                      

                       Computation: 9831 steps/s (collection: 2.245s, learning 0.255s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 10.2817
                       Mean reward: 0.57
               Mean episode length: 153.35
Episode_Reward/track_lin_vel_xy_exp: 0.0712
Episode_Reward/track_ang_vel_z_exp: 0.0504
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0152
     Episode_Reward/dof_torques_l2: -0.0121
         Episode_Reward/dof_acc_l2: -0.0295
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0037
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1694
Metrics/base_velocity/error_vel_yaw: 0.1250
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 2.50s
                      Time elapsed: 00:03:28
                               ETA: 00:57:18

################################################################################
                      [1m Learning iteration 80/1400 [0m                      

                       Computation: 10025 steps/s (collection: 2.205s, learning 0.246s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0085
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 10.2496
                       Mean reward: 1.20
               Mean episode length: 192.48
Episode_Reward/track_lin_vel_xy_exp: 0.1048
Episode_Reward/track_ang_vel_z_exp: 0.0711
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.0143
         Episode_Reward/dof_acc_l2: -0.0355
     Episode_Reward/action_rate_l2: -0.0220
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1884
Metrics/base_velocity/error_vel_yaw: 0.1301
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 2.45s
                      Time elapsed: 00:03:30
                               ETA: 00:57:12

################################################################################
                      [1m Learning iteration 81/1400 [0m                      

                       Computation: 9994 steps/s (collection: 2.206s, learning 0.253s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 10.2130
                       Mean reward: 0.67
               Mean episode length: 163.96
Episode_Reward/track_lin_vel_xy_exp: 0.0712
Episode_Reward/track_ang_vel_z_exp: 0.0532
       Episode_Reward/lin_vel_z_l2: -0.0102
      Episode_Reward/ang_vel_xy_l2: -0.0150
     Episode_Reward/dof_torques_l2: -0.0118
         Episode_Reward/dof_acc_l2: -0.0294
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0037
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1761
Metrics/base_velocity/error_vel_yaw: 0.1153
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 2.46s
                      Time elapsed: 00:03:33
                               ETA: 00:57:08

################################################################################
                      [1m Learning iteration 82/1400 [0m                      

                       Computation: 10039 steps/s (collection: 2.192s, learning 0.256s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0086
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 10.1872
                       Mean reward: 1.07
               Mean episode length: 192.72
Episode_Reward/track_lin_vel_xy_exp: 0.0898
Episode_Reward/track_ang_vel_z_exp: 0.0665
       Episode_Reward/lin_vel_z_l2: -0.0107
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.0137
         Episode_Reward/dof_acc_l2: -0.0323
     Episode_Reward/action_rate_l2: -0.0203
      Episode_Reward/feet_air_time: -0.0043
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1890
Metrics/base_velocity/error_vel_yaw: 0.1222
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 2.45s
                      Time elapsed: 00:03:35
                               ETA: 00:57:03

################################################################################
                      [1m Learning iteration 83/1400 [0m                      

                       Computation: 9878 steps/s (collection: 2.233s, learning 0.255s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 10.1685
                       Mean reward: 1.38
               Mean episode length: 208.71
Episode_Reward/track_lin_vel_xy_exp: 0.1022
Episode_Reward/track_ang_vel_z_exp: 0.0718
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.0144
         Episode_Reward/dof_acc_l2: -0.0355
     Episode_Reward/action_rate_l2: -0.0217
      Episode_Reward/feet_air_time: -0.0045
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1912
Metrics/base_velocity/error_vel_yaw: 0.1266
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 2.49s
                      Time elapsed: 00:03:38
                               ETA: 00:56:58

################################################################################
                      [1m Learning iteration 84/1400 [0m                      

                       Computation: 9857 steps/s (collection: 2.240s, learning 0.253s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 10.1314
                       Mean reward: 1.31
               Mean episode length: 215.39
Episode_Reward/track_lin_vel_xy_exp: 0.1026
Episode_Reward/track_ang_vel_z_exp: 0.0727
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0187
     Episode_Reward/dof_torques_l2: -0.0150
         Episode_Reward/dof_acc_l2: -0.0382
     Episode_Reward/action_rate_l2: -0.0223
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2054
Metrics/base_velocity/error_vel_yaw: 0.1406
      Episode_Termination/time_out: 0.0001
  Episode_Termination/base_contact: 0.9999
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 2.49s
                      Time elapsed: 00:03:40
                               ETA: 00:56:54

################################################################################
                      [1m Learning iteration 85/1400 [0m                      

                       Computation: 10101 steps/s (collection: 2.180s, learning 0.253s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 10.0891
                       Mean reward: 1.83
               Mean episode length: 265.84
Episode_Reward/track_lin_vel_xy_exp: 0.1102
Episode_Reward/track_ang_vel_z_exp: 0.0832
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.0166
         Episode_Reward/dof_acc_l2: -0.0385
     Episode_Reward/action_rate_l2: -0.0246
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2285
Metrics/base_velocity/error_vel_yaw: 0.1393
      Episode_Termination/time_out: 0.0010
  Episode_Termination/base_contact: 0.9990
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 2.43s
                      Time elapsed: 00:03:42
                               ETA: 00:56:49

################################################################################
                      [1m Learning iteration 86/1400 [0m                      

                       Computation: 9827 steps/s (collection: 2.246s, learning 0.255s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 10.0630
                       Mean reward: 1.40
               Mean episode length: 216.94
Episode_Reward/track_lin_vel_xy_exp: 0.1125
Episode_Reward/track_ang_vel_z_exp: 0.0840
       Episode_Reward/lin_vel_z_l2: -0.0131
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.0160
         Episode_Reward/dof_acc_l2: -0.0382
     Episode_Reward/action_rate_l2: -0.0241
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2138
Metrics/base_velocity/error_vel_yaw: 0.1278
      Episode_Termination/time_out: 0.0020
  Episode_Termination/base_contact: 0.9980
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 2.50s
                      Time elapsed: 00:03:45
                               ETA: 00:56:45

################################################################################
                      [1m Learning iteration 87/1400 [0m                      

                       Computation: 9999 steps/s (collection: 2.203s, learning 0.254s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0079
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 10.0532
                       Mean reward: 1.55
               Mean episode length: 238.85
Episode_Reward/track_lin_vel_xy_exp: 0.1033
Episode_Reward/track_ang_vel_z_exp: 0.0786
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.0159
         Episode_Reward/dof_acc_l2: -0.0397
     Episode_Reward/action_rate_l2: -0.0235
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2266
Metrics/base_velocity/error_vel_yaw: 0.1414
      Episode_Termination/time_out: 0.0025
  Episode_Termination/base_contact: 0.9975
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 2.46s
                      Time elapsed: 00:03:47
                               ETA: 00:56:40

################################################################################
                      [1m Learning iteration 88/1400 [0m                      

                       Computation: 9948 steps/s (collection: 2.218s, learning 0.253s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0094
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 10.0370
                       Mean reward: 1.49
               Mean episode length: 231.05
Episode_Reward/track_lin_vel_xy_exp: 0.1002
Episode_Reward/track_ang_vel_z_exp: 0.0725
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.0155
         Episode_Reward/dof_acc_l2: -0.0355
     Episode_Reward/action_rate_l2: -0.0222
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2071
Metrics/base_velocity/error_vel_yaw: 0.1385
      Episode_Termination/time_out: 0.0027
  Episode_Termination/base_contact: 0.9973
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 2.47s
                      Time elapsed: 00:03:50
                               ETA: 00:56:36

################################################################################
                      [1m Learning iteration 89/1400 [0m                      

                       Computation: 9953 steps/s (collection: 2.214s, learning 0.255s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 10.0182
                       Mean reward: 1.43
               Mean episode length: 234.36
Episode_Reward/track_lin_vel_xy_exp: 0.1110
Episode_Reward/track_ang_vel_z_exp: 0.0813
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0195
     Episode_Reward/dof_torques_l2: -0.0162
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0239
      Episode_Reward/feet_air_time: -0.0052
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2207
Metrics/base_velocity/error_vel_yaw: 0.1389
      Episode_Termination/time_out: 0.0027
  Episode_Termination/base_contact: 0.9973
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 2.47s
                      Time elapsed: 00:03:52
                               ETA: 00:56:32

################################################################################
                      [1m Learning iteration 90/1400 [0m                      

                       Computation: 9890 steps/s (collection: 2.231s, learning 0.254s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 9.9884
                       Mean reward: 1.47
               Mean episode length: 227.45
Episode_Reward/track_lin_vel_xy_exp: 0.1225
Episode_Reward/track_ang_vel_z_exp: 0.0874
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0209
     Episode_Reward/dof_torques_l2: -0.0186
         Episode_Reward/dof_acc_l2: -0.0391
     Episode_Reward/action_rate_l2: -0.0263
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2429
Metrics/base_velocity/error_vel_yaw: 0.1642
      Episode_Termination/time_out: 0.0049
  Episode_Termination/base_contact: 0.9951
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 2.48s
                      Time elapsed: 00:03:55
                               ETA: 00:56:28

################################################################################
                      [1m Learning iteration 91/1400 [0m                      

                       Computation: 9944 steps/s (collection: 2.219s, learning 0.252s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 9.9766
                       Mean reward: 1.31
               Mean episode length: 221.53
Episode_Reward/track_lin_vel_xy_exp: 0.1027
Episode_Reward/track_ang_vel_z_exp: 0.0785
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0187
     Episode_Reward/dof_torques_l2: -0.0158
         Episode_Reward/dof_acc_l2: -0.0359
     Episode_Reward/action_rate_l2: -0.0224
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2151
Metrics/base_velocity/error_vel_yaw: 0.1343
      Episode_Termination/time_out: 0.0073
  Episode_Termination/base_contact: 0.9927
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 2.47s
                      Time elapsed: 00:03:57
                               ETA: 00:56:23

################################################################################
                      [1m Learning iteration 92/1400 [0m                      

                       Computation: 9793 steps/s (collection: 2.255s, learning 0.254s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 9.9663
                       Mean reward: 2.27
               Mean episode length: 287.33
Episode_Reward/track_lin_vel_xy_exp: 0.1503
Episode_Reward/track_ang_vel_z_exp: 0.0978
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0229
     Episode_Reward/dof_torques_l2: -0.0193
         Episode_Reward/dof_acc_l2: -0.0451
     Episode_Reward/action_rate_l2: -0.0286
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2435
Metrics/base_velocity/error_vel_yaw: 0.1735
      Episode_Termination/time_out: 0.0110
  Episode_Termination/base_contact: 0.9890
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 2.51s
                      Time elapsed: 00:04:00
                               ETA: 00:56:20

################################################################################
                      [1m Learning iteration 93/1400 [0m                      

                       Computation: 9939 steps/s (collection: 2.222s, learning 0.251s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 9.9446
                       Mean reward: 1.52
               Mean episode length: 220.19
Episode_Reward/track_lin_vel_xy_exp: 0.1052
Episode_Reward/track_ang_vel_z_exp: 0.0812
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.0156
         Episode_Reward/dof_acc_l2: -0.0359
     Episode_Reward/action_rate_l2: -0.0225
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2138
Metrics/base_velocity/error_vel_yaw: 0.1234
      Episode_Termination/time_out: 0.0132
  Episode_Termination/base_contact: 0.9868
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 2.47s
                      Time elapsed: 00:04:02
                               ETA: 00:56:16

################################################################################
                      [1m Learning iteration 94/1400 [0m                      

                       Computation: 9919 steps/s (collection: 2.225s, learning 0.253s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 9.9108
                       Mean reward: 1.68
               Mean episode length: 234.15
Episode_Reward/track_lin_vel_xy_exp: 0.1254
Episode_Reward/track_ang_vel_z_exp: 0.0911
       Episode_Reward/lin_vel_z_l2: -0.0126
      Episode_Reward/ang_vel_xy_l2: -0.0201
     Episode_Reward/dof_torques_l2: -0.0174
         Episode_Reward/dof_acc_l2: -0.0403
     Episode_Reward/action_rate_l2: -0.0249
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2186
Metrics/base_velocity/error_vel_yaw: 0.1325
      Episode_Termination/time_out: 0.0120
  Episode_Termination/base_contact: 0.9880
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 2.48s
                      Time elapsed: 00:04:05
                               ETA: 00:56:12

################################################################################
                      [1m Learning iteration 95/1400 [0m                      

                       Computation: 9940 steps/s (collection: 2.218s, learning 0.255s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0069
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 9.8731
                       Mean reward: 2.22
               Mean episode length: 293.50
Episode_Reward/track_lin_vel_xy_exp: 0.1384
Episode_Reward/track_ang_vel_z_exp: 0.1082
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0229
     Episode_Reward/dof_torques_l2: -0.0208
         Episode_Reward/dof_acc_l2: -0.0451
     Episode_Reward/action_rate_l2: -0.0298
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0005
Metrics/base_velocity/error_vel_xy: 0.2803
Metrics/base_velocity/error_vel_yaw: 0.1572
      Episode_Termination/time_out: 0.0134
  Episode_Termination/base_contact: 0.9866
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 2.47s
                      Time elapsed: 00:04:07
                               ETA: 00:56:08

################################################################################
                      [1m Learning iteration 96/1400 [0m                      

                       Computation: 9840 steps/s (collection: 2.245s, learning 0.253s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 9.8499
                       Mean reward: 1.94
               Mean episode length: 271.18
Episode_Reward/track_lin_vel_xy_exp: 0.1391
Episode_Reward/track_ang_vel_z_exp: 0.1075
       Episode_Reward/lin_vel_z_l2: -0.0137
      Episode_Reward/ang_vel_xy_l2: -0.0229
     Episode_Reward/dof_torques_l2: -0.0209
         Episode_Reward/dof_acc_l2: -0.0437
     Episode_Reward/action_rate_l2: -0.0303
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0010
Metrics/base_velocity/error_vel_xy: 0.2834
Metrics/base_velocity/error_vel_yaw: 0.1639
      Episode_Termination/time_out: 0.0195
  Episode_Termination/base_contact: 0.9805
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 2.50s
                      Time elapsed: 00:04:10
                               ETA: 00:56:04

################################################################################
                      [1m Learning iteration 97/1400 [0m                      

                       Computation: 10057 steps/s (collection: 2.188s, learning 0.255s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0085
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 9.8226
                       Mean reward: 2.49
               Mean episode length: 296.70
Episode_Reward/track_lin_vel_xy_exp: 0.1543
Episode_Reward/track_ang_vel_z_exp: 0.1088
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0236
     Episode_Reward/dof_torques_l2: -0.0200
         Episode_Reward/dof_acc_l2: -0.0446
     Episode_Reward/action_rate_l2: -0.0295
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0010
Metrics/base_velocity/error_vel_xy: 0.2522
Metrics/base_velocity/error_vel_yaw: 0.1515
      Episode_Termination/time_out: 0.0254
  Episode_Termination/base_contact: 0.9746
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 2.44s
                      Time elapsed: 00:04:12
                               ETA: 00:55:59

################################################################################
                      [1m Learning iteration 98/1400 [0m                      

                       Computation: 9984 steps/s (collection: 2.212s, learning 0.249s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 9.8158
                       Mean reward: 2.20
               Mean episode length: 270.37
Episode_Reward/track_lin_vel_xy_exp: 0.1245
Episode_Reward/track_ang_vel_z_exp: 0.0960
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.0176
         Episode_Reward/dof_acc_l2: -0.0395
     Episode_Reward/action_rate_l2: -0.0257
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0010
Metrics/base_velocity/error_vel_xy: 0.2424
Metrics/base_velocity/error_vel_yaw: 0.1331
      Episode_Termination/time_out: 0.0307
  Episode_Termination/base_contact: 0.9693
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 2.46s
                      Time elapsed: 00:04:15
                               ETA: 00:55:55

################################################################################
                      [1m Learning iteration 99/1400 [0m                      

                       Computation: 9833 steps/s (collection: 2.245s, learning 0.254s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0086
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 9.8011
                       Mean reward: 1.95
               Mean episode length: 260.09
Episode_Reward/track_lin_vel_xy_exp: 0.1018
Episode_Reward/track_ang_vel_z_exp: 0.0825
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0183
     Episode_Reward/dof_torques_l2: -0.0161
         Episode_Reward/dof_acc_l2: -0.0371
     Episode_Reward/action_rate_l2: -0.0227
      Episode_Reward/feet_air_time: -0.0047
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0010
Metrics/base_velocity/error_vel_xy: 0.2282
Metrics/base_velocity/error_vel_yaw: 0.1248
      Episode_Termination/time_out: 0.0376
  Episode_Termination/base_contact: 0.9624
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 2.50s
                      Time elapsed: 00:04:17
                               ETA: 00:55:52

################################################################################
                     [1m Learning iteration 100/1400 [0m                      

                       Computation: 9961 steps/s (collection: 2.211s, learning 0.256s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0100
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 9.7796
                       Mean reward: 2.19
               Mean episode length: 282.47
Episode_Reward/track_lin_vel_xy_exp: 0.1674
Episode_Reward/track_ang_vel_z_exp: 0.1184
       Episode_Reward/lin_vel_z_l2: -0.0150
      Episode_Reward/ang_vel_xy_l2: -0.0251
     Episode_Reward/dof_torques_l2: -0.0228
         Episode_Reward/dof_acc_l2: -0.0506
     Episode_Reward/action_rate_l2: -0.0330
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0010
Metrics/base_velocity/error_vel_xy: 0.2889
Metrics/base_velocity/error_vel_yaw: 0.1834
      Episode_Termination/time_out: 0.0395
  Episode_Termination/base_contact: 0.9605
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 2.47s
                      Time elapsed: 00:04:20
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 101/1400 [0m                      

                       Computation: 10129 steps/s (collection: 2.172s, learning 0.254s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0061
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 9.7631
                       Mean reward: 2.60
               Mean episode length: 321.23
Episode_Reward/track_lin_vel_xy_exp: 0.1312
Episode_Reward/track_ang_vel_z_exp: 0.1074
       Episode_Reward/lin_vel_z_l2: -0.0131
      Episode_Reward/ang_vel_xy_l2: -0.0233
     Episode_Reward/dof_torques_l2: -0.0202
         Episode_Reward/dof_acc_l2: -0.0439
     Episode_Reward/action_rate_l2: -0.0288
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0010
Metrics/base_velocity/error_vel_xy: 0.2826
Metrics/base_velocity/error_vel_yaw: 0.1536
      Episode_Termination/time_out: 0.0440
  Episode_Termination/base_contact: 0.9560
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 2.43s
                      Time elapsed: 00:04:22
                               ETA: 00:55:43

################################################################################
                     [1m Learning iteration 102/1400 [0m                      

                       Computation: 9989 steps/s (collection: 2.206s, learning 0.254s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 9.7321
                       Mean reward: 2.24
               Mean episode length: 291.47
Episode_Reward/track_lin_vel_xy_exp: 0.1302
Episode_Reward/track_ang_vel_z_exp: 0.1014
       Episode_Reward/lin_vel_z_l2: -0.0138
      Episode_Reward/ang_vel_xy_l2: -0.0225
     Episode_Reward/dof_torques_l2: -0.0214
         Episode_Reward/dof_acc_l2: -0.0433
     Episode_Reward/action_rate_l2: -0.0292
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0010
Metrics/base_velocity/error_vel_xy: 0.2926
Metrics/base_velocity/error_vel_yaw: 0.1885
      Episode_Termination/time_out: 0.0458
  Episode_Termination/base_contact: 0.9542
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 2.46s
                      Time elapsed: 00:04:25
                               ETA: 00:55:39

################################################################################
                     [1m Learning iteration 103/1400 [0m                      

                       Computation: 9920 steps/s (collection: 2.222s, learning 0.255s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 9.7106
                       Mean reward: 3.02
               Mean episode length: 342.57
Episode_Reward/track_lin_vel_xy_exp: 0.1779
Episode_Reward/track_ang_vel_z_exp: 0.1272
       Episode_Reward/lin_vel_z_l2: -0.0150
      Episode_Reward/ang_vel_xy_l2: -0.0258
     Episode_Reward/dof_torques_l2: -0.0241
         Episode_Reward/dof_acc_l2: -0.0497
     Episode_Reward/action_rate_l2: -0.0340
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0015
Metrics/base_velocity/error_vel_xy: 0.2937
Metrics/base_velocity/error_vel_yaw: 0.1781
      Episode_Termination/time_out: 0.0480
  Episode_Termination/base_contact: 0.9520
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 2.48s
                      Time elapsed: 00:04:27
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 104/1400 [0m                      

                       Computation: 9888 steps/s (collection: 2.231s, learning 0.254s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 9.6869
                       Mean reward: 2.54
               Mean episode length: 295.95
Episode_Reward/track_lin_vel_xy_exp: 0.1007
Episode_Reward/track_ang_vel_z_exp: 0.0815
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0155
         Episode_Reward/dof_acc_l2: -0.0328
     Episode_Reward/action_rate_l2: -0.0212
      Episode_Reward/feet_air_time: -0.0045
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2098
Metrics/base_velocity/error_vel_yaw: 0.1120
      Episode_Termination/time_out: 0.0500
  Episode_Termination/base_contact: 0.9500
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 2.49s
                      Time elapsed: 00:04:29
                               ETA: 00:55:32

################################################################################
                     [1m Learning iteration 105/1400 [0m                      

                       Computation: 9981 steps/s (collection: 2.208s, learning 0.255s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0070
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 9.6586
                       Mean reward: 1.84
               Mean episode length: 263.83
Episode_Reward/track_lin_vel_xy_exp: 0.1227
Episode_Reward/track_ang_vel_z_exp: 0.0897
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0196
     Episode_Reward/dof_torques_l2: -0.0184
         Episode_Reward/dof_acc_l2: -0.0401
     Episode_Reward/action_rate_l2: -0.0249
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2392
Metrics/base_velocity/error_vel_yaw: 0.1497
      Episode_Termination/time_out: 0.0468
  Episode_Termination/base_contact: 0.9532
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 2.46s
                      Time elapsed: 00:04:32
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 106/1400 [0m                      

                       Computation: 9794 steps/s (collection: 2.259s, learning 0.251s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0086
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 9.6471
                       Mean reward: 2.22
               Mean episode length: 312.16
Episode_Reward/track_lin_vel_xy_exp: 0.1329
Episode_Reward/track_ang_vel_z_exp: 0.1120
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0231
     Episode_Reward/dof_torques_l2: -0.0210
         Episode_Reward/dof_acc_l2: -0.0448
     Episode_Reward/action_rate_l2: -0.0293
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2982
Metrics/base_velocity/error_vel_yaw: 0.1530
      Episode_Termination/time_out: 0.0510
  Episode_Termination/base_contact: 0.9490
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 2.51s
                      Time elapsed: 00:04:34
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 107/1400 [0m                      

                       Computation: 9733 steps/s (collection: 2.269s, learning 0.255s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0084
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 9.6214
                       Mean reward: 2.18
               Mean episode length: 304.24
Episode_Reward/track_lin_vel_xy_exp: 0.1391
Episode_Reward/track_ang_vel_z_exp: 0.1077
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0226
     Episode_Reward/dof_torques_l2: -0.0206
         Episode_Reward/dof_acc_l2: -0.0436
     Episode_Reward/action_rate_l2: -0.0289
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2814
Metrics/base_velocity/error_vel_yaw: 0.1609
      Episode_Termination/time_out: 0.0549
  Episode_Termination/base_contact: 0.9451
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.52s
                      Time elapsed: 00:04:37
                               ETA: 00:55:22

################################################################################
                     [1m Learning iteration 108/1400 [0m                      

                       Computation: 9856 steps/s (collection: 2.242s, learning 0.251s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 9.5893
                       Mean reward: 2.12
               Mean episode length: 285.22
Episode_Reward/track_lin_vel_xy_exp: 0.1117
Episode_Reward/track_ang_vel_z_exp: 0.0937
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.0174
         Episode_Reward/dof_acc_l2: -0.0390
     Episode_Reward/action_rate_l2: -0.0244
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2458
Metrics/base_velocity/error_vel_yaw: 0.1284
      Episode_Termination/time_out: 0.0558
  Episode_Termination/base_contact: 0.9442
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 2.49s
                      Time elapsed: 00:04:39
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 109/1400 [0m                      

                       Computation: 9941 steps/s (collection: 2.220s, learning 0.253s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 9.5664
                       Mean reward: 2.18
               Mean episode length: 291.26
Episode_Reward/track_lin_vel_xy_exp: 0.1064
Episode_Reward/track_ang_vel_z_exp: 0.0871
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.0179
         Episode_Reward/dof_acc_l2: -0.0400
     Episode_Reward/action_rate_l2: -0.0239
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2553
Metrics/base_velocity/error_vel_yaw: 0.1513
      Episode_Termination/time_out: 0.0562
  Episode_Termination/base_contact: 0.9438
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 2.47s
                      Time elapsed: 00:04:42
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 110/1400 [0m                      

                       Computation: 9898 steps/s (collection: 2.231s, learning 0.252s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0085
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 9.5431
                       Mean reward: 2.33
               Mean episode length: 294.68
Episode_Reward/track_lin_vel_xy_exp: 0.1334
Episode_Reward/track_ang_vel_z_exp: 0.1029
       Episode_Reward/lin_vel_z_l2: -0.0126
      Episode_Reward/ang_vel_xy_l2: -0.0213
     Episode_Reward/dof_torques_l2: -0.0194
         Episode_Reward/dof_acc_l2: -0.0410
     Episode_Reward/action_rate_l2: -0.0268
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2598
Metrics/base_velocity/error_vel_yaw: 0.1442
      Episode_Termination/time_out: 0.0544
  Episode_Termination/base_contact: 0.9456
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 2.48s
                      Time elapsed: 00:04:44
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 111/1400 [0m                      

                       Computation: 9884 steps/s (collection: 2.231s, learning 0.255s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0109
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 9.5186
                       Mean reward: 2.17
               Mean episode length: 267.63
Episode_Reward/track_lin_vel_xy_exp: 0.1216
Episode_Reward/track_ang_vel_z_exp: 0.0988
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0201
     Episode_Reward/dof_torques_l2: -0.0187
         Episode_Reward/dof_acc_l2: -0.0382
     Episode_Reward/action_rate_l2: -0.0252
      Episode_Reward/feet_air_time: -0.0047
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2469
Metrics/base_velocity/error_vel_yaw: 0.1297
      Episode_Termination/time_out: 0.0551
  Episode_Termination/base_contact: 0.9449
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 2.49s
                      Time elapsed: 00:04:47
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 112/1400 [0m                      

                       Computation: 9742 steps/s (collection: 2.267s, learning 0.256s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 9.4959
                       Mean reward: 2.90
               Mean episode length: 334.73
Episode_Reward/track_lin_vel_xy_exp: 0.1840
Episode_Reward/track_ang_vel_z_exp: 0.1362
       Episode_Reward/lin_vel_z_l2: -0.0154
      Episode_Reward/ang_vel_xy_l2: -0.0264
     Episode_Reward/dof_torques_l2: -0.0254
         Episode_Reward/dof_acc_l2: -0.0500
     Episode_Reward/action_rate_l2: -0.0344
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.3139
Metrics/base_velocity/error_vel_yaw: 0.1785
      Episode_Termination/time_out: 0.0577
  Episode_Termination/base_contact: 0.9423
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 2.52s
                      Time elapsed: 00:04:49
                               ETA: 00:55:04

################################################################################
                     [1m Learning iteration 113/1400 [0m                      

                       Computation: 9913 steps/s (collection: 2.228s, learning 0.251s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 9.4619
                       Mean reward: 2.52
               Mean episode length: 315.10
Episode_Reward/track_lin_vel_xy_exp: 0.1400
Episode_Reward/track_ang_vel_z_exp: 0.1087
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.0210
         Episode_Reward/dof_acc_l2: -0.0430
     Episode_Reward/action_rate_l2: -0.0277
      Episode_Reward/feet_air_time: -0.0052
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2656
Metrics/base_velocity/error_vel_yaw: 0.1452
      Episode_Termination/time_out: 0.0611
  Episode_Termination/base_contact: 0.9389
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 2.48s
                      Time elapsed: 00:04:52
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 114/1400 [0m                      

                       Computation: 9723 steps/s (collection: 2.274s, learning 0.254s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0099
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 9.4414
                       Mean reward: 2.23
               Mean episode length: 283.30
Episode_Reward/track_lin_vel_xy_exp: 0.1232
Episode_Reward/track_ang_vel_z_exp: 0.0990
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.0186
         Episode_Reward/dof_acc_l2: -0.0387
     Episode_Reward/action_rate_l2: -0.0250
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2510
Metrics/base_velocity/error_vel_yaw: 0.1304
      Episode_Termination/time_out: 0.0627
  Episode_Termination/base_contact: 0.9373
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 2.53s
                      Time elapsed: 00:04:54
                               ETA: 00:54:58

################################################################################
                     [1m Learning iteration 115/1400 [0m                      

                       Computation: 9618 steps/s (collection: 2.300s, learning 0.255s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 9.4213
                       Mean reward: 2.55
               Mean episode length: 319.41
Episode_Reward/track_lin_vel_xy_exp: 0.1434
Episode_Reward/track_ang_vel_z_exp: 0.1242
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0246
     Episode_Reward/dof_torques_l2: -0.0224
         Episode_Reward/dof_acc_l2: -0.0480
     Episode_Reward/action_rate_l2: -0.0309
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.3196
Metrics/base_velocity/error_vel_yaw: 0.1530
      Episode_Termination/time_out: 0.0653
  Episode_Termination/base_contact: 0.9347
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 2.55s
                      Time elapsed: 00:04:57
                               ETA: 00:54:55

################################################################################
                     [1m Learning iteration 116/1400 [0m                      

                       Computation: 9801 steps/s (collection: 2.255s, learning 0.252s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 9.3899
                       Mean reward: 2.29
               Mean episode length: 308.82
Episode_Reward/track_lin_vel_xy_exp: 0.1531
Episode_Reward/track_ang_vel_z_exp: 0.1224
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0244
     Episode_Reward/dof_torques_l2: -0.0228
         Episode_Reward/dof_acc_l2: -0.0475
     Episode_Reward/action_rate_l2: -0.0304
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2967
Metrics/base_velocity/error_vel_yaw: 0.1547
      Episode_Termination/time_out: 0.0665
  Episode_Termination/base_contact: 0.9335
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 2.51s
                      Time elapsed: 00:05:00
                               ETA: 00:54:52

################################################################################
                     [1m Learning iteration 117/1400 [0m                      

                       Computation: 9702 steps/s (collection: 2.278s, learning 0.255s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 9.3597
                       Mean reward: 2.78
               Mean episode length: 334.33
Episode_Reward/track_lin_vel_xy_exp: 0.1617
Episode_Reward/track_ang_vel_z_exp: 0.1224
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0240
     Episode_Reward/dof_torques_l2: -0.0224
         Episode_Reward/dof_acc_l2: -0.0466
     Episode_Reward/action_rate_l2: -0.0305
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2852
Metrics/base_velocity/error_vel_yaw: 0.1561
      Episode_Termination/time_out: 0.0706
  Episode_Termination/base_contact: 0.9294
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 2.53s
                      Time elapsed: 00:05:02
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 118/1400 [0m                      

                       Computation: 9695 steps/s (collection: 2.279s, learning 0.256s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 9.3490
                       Mean reward: 2.28
               Mean episode length: 293.18
Episode_Reward/track_lin_vel_xy_exp: 0.1504
Episode_Reward/track_ang_vel_z_exp: 0.1182
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0232
     Episode_Reward/dof_torques_l2: -0.0220
         Episode_Reward/dof_acc_l2: -0.0445
     Episode_Reward/action_rate_l2: -0.0295
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0032
Metrics/base_velocity/error_vel_xy: 0.2982
Metrics/base_velocity/error_vel_yaw: 0.1539
      Episode_Termination/time_out: 0.0722
  Episode_Termination/base_contact: 0.9278
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 2.53s
                      Time elapsed: 00:05:05
                               ETA: 00:54:46

################################################################################
                     [1m Learning iteration 119/1400 [0m                      

                       Computation: 9610 steps/s (collection: 2.302s, learning 0.255s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 9.3295
                       Mean reward: 2.68
               Mean episode length: 312.99
Episode_Reward/track_lin_vel_xy_exp: 0.1518
Episode_Reward/track_ang_vel_z_exp: 0.1190
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0237
     Episode_Reward/dof_torques_l2: -0.0223
         Episode_Reward/dof_acc_l2: -0.0441
     Episode_Reward/action_rate_l2: -0.0295
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0035
Metrics/base_velocity/error_vel_xy: 0.2845
Metrics/base_velocity/error_vel_yaw: 0.1522
      Episode_Termination/time_out: 0.0727
  Episode_Termination/base_contact: 0.9273
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 2.56s
                      Time elapsed: 00:05:07
                               ETA: 00:54:43

################################################################################
                     [1m Learning iteration 120/1400 [0m                      

                       Computation: 9653 steps/s (collection: 2.291s, learning 0.255s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0084
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 9.3125
                       Mean reward: 2.44
               Mean episode length: 306.73
Episode_Reward/track_lin_vel_xy_exp: 0.1178
Episode_Reward/track_ang_vel_z_exp: 0.1016
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0217
     Episode_Reward/dof_torques_l2: -0.0198
         Episode_Reward/dof_acc_l2: -0.0406
     Episode_Reward/action_rate_l2: -0.0256
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2805
Metrics/base_velocity/error_vel_yaw: 0.1448
      Episode_Termination/time_out: 0.0722
  Episode_Termination/base_contact: 0.9278
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 2.55s
                      Time elapsed: 00:05:10
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 121/1400 [0m                      

                       Computation: 9798 steps/s (collection: 2.259s, learning 0.249s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 9.2814
                       Mean reward: 2.69
               Mean episode length: 332.03
Episode_Reward/track_lin_vel_xy_exp: 0.1583
Episode_Reward/track_ang_vel_z_exp: 0.1297
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0254
     Episode_Reward/dof_torques_l2: -0.0240
         Episode_Reward/dof_acc_l2: -0.0478
     Episode_Reward/action_rate_l2: -0.0314
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.3171
Metrics/base_velocity/error_vel_yaw: 0.1593
      Episode_Termination/time_out: 0.0699
  Episode_Termination/base_contact: 0.9301
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 2.51s
                      Time elapsed: 00:05:12
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 122/1400 [0m                      

                       Computation: 9804 steps/s (collection: 2.251s, learning 0.255s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0085
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 9.2521
                       Mean reward: 2.14
               Mean episode length: 330.35
Episode_Reward/track_lin_vel_xy_exp: 0.1157
Episode_Reward/track_ang_vel_z_exp: 0.1098
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0229
     Episode_Reward/dof_torques_l2: -0.0221
         Episode_Reward/dof_acc_l2: -0.0436
     Episode_Reward/action_rate_l2: -0.0275
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.3238
Metrics/base_velocity/error_vel_yaw: 0.1569
      Episode_Termination/time_out: 0.0682
  Episode_Termination/base_contact: 0.9318
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 2.51s
                      Time elapsed: 00:05:15
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 123/1400 [0m                      

                       Computation: 9595 steps/s (collection: 2.307s, learning 0.254s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0094
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 9.2210
                       Mean reward: 2.61
               Mean episode length: 336.39
Episode_Reward/track_lin_vel_xy_exp: 0.1532
Episode_Reward/track_ang_vel_z_exp: 0.1202
       Episode_Reward/lin_vel_z_l2: -0.0163
      Episode_Reward/ang_vel_xy_l2: -0.0242
     Episode_Reward/dof_torques_l2: -0.0236
         Episode_Reward/dof_acc_l2: -0.0462
     Episode_Reward/action_rate_l2: -0.0298
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0029
Metrics/base_velocity/error_vel_xy: 0.2973
Metrics/base_velocity/error_vel_yaw: 0.1621
      Episode_Termination/time_out: 0.0722
  Episode_Termination/base_contact: 0.9278
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 2.56s
                      Time elapsed: 00:05:17
                               ETA: 00:54:32

################################################################################
                     [1m Learning iteration 124/1400 [0m                      

                       Computation: 9888 steps/s (collection: 2.233s, learning 0.252s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0094
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 9.1929
                       Mean reward: 2.16
               Mean episode length: 298.53
Episode_Reward/track_lin_vel_xy_exp: 0.1227
Episode_Reward/track_ang_vel_z_exp: 0.1025
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.0203
         Episode_Reward/dof_acc_l2: -0.0408
     Episode_Reward/action_rate_l2: -0.0252
      Episode_Reward/feet_air_time: -0.0052
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0033
Metrics/base_velocity/error_vel_xy: 0.2747
Metrics/base_velocity/error_vel_yaw: 0.1416
      Episode_Termination/time_out: 0.0721
  Episode_Termination/base_contact: 0.9279
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 2.49s
                      Time elapsed: 00:05:20
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 125/1400 [0m                      

                       Computation: 9828 steps/s (collection: 2.246s, learning 0.254s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0071
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 9.1726
                       Mean reward: 2.43
               Mean episode length: 311.27
Episode_Reward/track_lin_vel_xy_exp: 0.1124
Episode_Reward/track_ang_vel_z_exp: 0.0972
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0202
     Episode_Reward/dof_torques_l2: -0.0195
         Episode_Reward/dof_acc_l2: -0.0396
     Episode_Reward/action_rate_l2: -0.0241
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0049
Metrics/base_velocity/error_vel_xy: 0.2623
Metrics/base_velocity/error_vel_yaw: 0.1379
      Episode_Termination/time_out: 0.0729
  Episode_Termination/base_contact: 0.9271
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 2.50s
                      Time elapsed: 00:05:22
                               ETA: 00:54:25

################################################################################
                     [1m Learning iteration 126/1400 [0m                      

                       Computation: 9802 steps/s (collection: 2.252s, learning 0.255s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0100
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 9.1531
                       Mean reward: 2.78
               Mean episode length: 306.73
Episode_Reward/track_lin_vel_xy_exp: 0.1649
Episode_Reward/track_ang_vel_z_exp: 0.1199
       Episode_Reward/lin_vel_z_l2: -0.0149
      Episode_Reward/ang_vel_xy_l2: -0.0243
     Episode_Reward/dof_torques_l2: -0.0231
         Episode_Reward/dof_acc_l2: -0.0471
     Episode_Reward/action_rate_l2: -0.0293
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0049
Metrics/base_velocity/error_vel_xy: 0.2772
Metrics/base_velocity/error_vel_yaw: 0.1635
      Episode_Termination/time_out: 0.0745
  Episode_Termination/base_contact: 0.9255
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 2.51s
                      Time elapsed: 00:05:25
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 127/1400 [0m                      

                       Computation: 9690 steps/s (collection: 2.281s, learning 0.255s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 9.1342
                       Mean reward: 1.85
               Mean episode length: 266.87
Episode_Reward/track_lin_vel_xy_exp: 0.1044
Episode_Reward/track_ang_vel_z_exp: 0.0871
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0195
     Episode_Reward/dof_torques_l2: -0.0178
         Episode_Reward/dof_acc_l2: -0.0403
     Episode_Reward/action_rate_l2: -0.0216
      Episode_Reward/feet_air_time: -0.0045
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0049
Metrics/base_velocity/error_vel_xy: 0.2342
Metrics/base_velocity/error_vel_yaw: 0.1312
      Episode_Termination/time_out: 0.0795
  Episode_Termination/base_contact: 0.9205
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 2.54s
                      Time elapsed: 00:05:27
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 128/1400 [0m                      

                       Computation: 9710 steps/s (collection: 2.269s, learning 0.261s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 9.1148
                       Mean reward: 2.69
               Mean episode length: 312.81
Episode_Reward/track_lin_vel_xy_exp: 0.1660
Episode_Reward/track_ang_vel_z_exp: 0.1222
       Episode_Reward/lin_vel_z_l2: -0.0147
      Episode_Reward/ang_vel_xy_l2: -0.0237
     Episode_Reward/dof_torques_l2: -0.0238
         Episode_Reward/dof_acc_l2: -0.0454
     Episode_Reward/action_rate_l2: -0.0294
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0061
Metrics/base_velocity/error_vel_xy: 0.2754
Metrics/base_velocity/error_vel_yaw: 0.1558
      Episode_Termination/time_out: 0.0824
  Episode_Termination/base_contact: 0.9176
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 2.53s
                      Time elapsed: 00:05:30
                               ETA: 00:54:17

################################################################################
                     [1m Learning iteration 129/1400 [0m                      

                       Computation: 9884 steps/s (collection: 2.232s, learning 0.254s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 9.0901
                       Mean reward: 2.48
               Mean episode length: 316.93
Episode_Reward/track_lin_vel_xy_exp: 0.1603
Episode_Reward/track_ang_vel_z_exp: 0.1206
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0243
     Episode_Reward/dof_torques_l2: -0.0221
         Episode_Reward/dof_acc_l2: -0.0481
     Episode_Reward/action_rate_l2: -0.0286
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0076
Metrics/base_velocity/error_vel_xy: 0.2757
Metrics/base_velocity/error_vel_yaw: 0.1478
      Episode_Termination/time_out: 0.0889
  Episode_Termination/base_contact: 0.9111
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 2.49s
                      Time elapsed: 00:05:32
                               ETA: 00:54:13

################################################################################
                     [1m Learning iteration 130/1400 [0m                      

                       Computation: 9636 steps/s (collection: 2.297s, learning 0.253s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 9.0790
                       Mean reward: 1.60
               Mean episode length: 245.86
Episode_Reward/track_lin_vel_xy_exp: 0.0990
Episode_Reward/track_ang_vel_z_exp: 0.0908
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0195
     Episode_Reward/dof_torques_l2: -0.0176
         Episode_Reward/dof_acc_l2: -0.0362
     Episode_Reward/action_rate_l2: -0.0215
      Episode_Reward/feet_air_time: -0.0043
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0078
Metrics/base_velocity/error_vel_xy: 0.2481
Metrics/base_velocity/error_vel_yaw: 0.1179
      Episode_Termination/time_out: 0.0894
  Episode_Termination/base_contact: 0.9106
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 2.55s
                      Time elapsed: 00:05:35
                               ETA: 00:54:11

################################################################################
                     [1m Learning iteration 131/1400 [0m                      

                       Computation: 9682 steps/s (collection: 2.284s, learning 0.254s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0109
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 9.0521
                       Mean reward: 2.45
               Mean episode length: 306.07
Episode_Reward/track_lin_vel_xy_exp: 0.1598
Episode_Reward/track_ang_vel_z_exp: 0.1330
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0253
     Episode_Reward/dof_torques_l2: -0.0250
         Episode_Reward/dof_acc_l2: -0.0492
     Episode_Reward/action_rate_l2: -0.0311
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0085
Metrics/base_velocity/error_vel_xy: 0.3207
Metrics/base_velocity/error_vel_yaw: 0.1575
      Episode_Termination/time_out: 0.0903
  Episode_Termination/base_contact: 0.9097
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 2.54s
                      Time elapsed: 00:05:37
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 132/1400 [0m                      

                       Computation: 9662 steps/s (collection: 2.291s, learning 0.252s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0079
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 9.0276
                       Mean reward: 3.27
               Mean episode length: 357.33
Episode_Reward/track_lin_vel_xy_exp: 0.1860
Episode_Reward/track_ang_vel_z_exp: 0.1409
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0269
     Episode_Reward/dof_torques_l2: -0.0268
         Episode_Reward/dof_acc_l2: -0.0504
     Episode_Reward/action_rate_l2: -0.0329
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0096
Metrics/base_velocity/error_vel_xy: 0.3157
Metrics/base_velocity/error_vel_yaw: 0.1658
      Episode_Termination/time_out: 0.0929
  Episode_Termination/base_contact: 0.9071
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 2.54s
                      Time elapsed: 00:05:40
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 133/1400 [0m                      

                       Computation: 9598 steps/s (collection: 2.305s, learning 0.255s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 9.0009
                       Mean reward: 2.88
               Mean episode length: 320.75
Episode_Reward/track_lin_vel_xy_exp: 0.1480
Episode_Reward/track_ang_vel_z_exp: 0.1107
       Episode_Reward/lin_vel_z_l2: -0.0147
      Episode_Reward/ang_vel_xy_l2: -0.0226
     Episode_Reward/dof_torques_l2: -0.0214
         Episode_Reward/dof_acc_l2: -0.0437
     Episode_Reward/action_rate_l2: -0.0259
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0098
Metrics/base_velocity/error_vel_xy: 0.2565
Metrics/base_velocity/error_vel_yaw: 0.1412
      Episode_Termination/time_out: 0.0970
  Episode_Termination/base_contact: 0.9030
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 2.56s
                      Time elapsed: 00:05:42
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 134/1400 [0m                      

                       Computation: 9720 steps/s (collection: 2.274s, learning 0.254s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 8.9928
                       Mean reward: 1.85
               Mean episode length: 286.06
Episode_Reward/track_lin_vel_xy_exp: 0.1065
Episode_Reward/track_ang_vel_z_exp: 0.1064
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0226
     Episode_Reward/dof_torques_l2: -0.0211
         Episode_Reward/dof_acc_l2: -0.0421
     Episode_Reward/action_rate_l2: -0.0250
      Episode_Reward/feet_air_time: -0.0051
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0098
Metrics/base_velocity/error_vel_xy: 0.3102
Metrics/base_velocity/error_vel_yaw: 0.1403
      Episode_Termination/time_out: 0.0990
  Episode_Termination/base_contact: 0.9010
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 2.53s
                      Time elapsed: 00:05:45
                               ETA: 00:54:00

################################################################################
                     [1m Learning iteration 135/1400 [0m                      

                       Computation: 9674 steps/s (collection: 2.284s, learning 0.256s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 8.9908
                       Mean reward: 2.55
               Mean episode length: 339.30
Episode_Reward/track_lin_vel_xy_exp: 0.2150
Episode_Reward/track_ang_vel_z_exp: 0.1708
       Episode_Reward/lin_vel_z_l2: -0.0209
      Episode_Reward/ang_vel_xy_l2: -0.0330
     Episode_Reward/dof_torques_l2: -0.0332
         Episode_Reward/dof_acc_l2: -0.0634
     Episode_Reward/action_rate_l2: -0.0409
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0098
Metrics/base_velocity/error_vel_xy: 0.4160
Metrics/base_velocity/error_vel_yaw: 0.2181
      Episode_Termination/time_out: 0.1040
  Episode_Termination/base_contact: 0.8960
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 2.54s
                      Time elapsed: 00:05:48
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 136/1400 [0m                      

                       Computation: 9747 steps/s (collection: 2.267s, learning 0.255s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0108
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 8.9717
                       Mean reward: 3.23
               Mean episode length: 400.36
Episode_Reward/track_lin_vel_xy_exp: 0.1804
Episode_Reward/track_ang_vel_z_exp: 0.1424
       Episode_Reward/lin_vel_z_l2: -0.0178
      Episode_Reward/ang_vel_xy_l2: -0.0276
     Episode_Reward/dof_torques_l2: -0.0287
         Episode_Reward/dof_acc_l2: -0.0556
     Episode_Reward/action_rate_l2: -0.0335
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0109
Metrics/base_velocity/error_vel_xy: 0.3476
Metrics/base_velocity/error_vel_yaw: 0.1895
      Episode_Termination/time_out: 0.1086
  Episode_Termination/base_contact: 0.8914
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 2.52s
                      Time elapsed: 00:05:50
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 137/1400 [0m                      

                       Computation: 9812 steps/s (collection: 2.250s, learning 0.254s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0092
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 8.9570
                       Mean reward: 2.16
               Mean episode length: 338.72
Episode_Reward/track_lin_vel_xy_exp: 0.1335
Episode_Reward/track_ang_vel_z_exp: 0.1254
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0251
     Episode_Reward/dof_torques_l2: -0.0245
         Episode_Reward/dof_acc_l2: -0.0485
     Episode_Reward/action_rate_l2: -0.0293
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0108
Metrics/base_velocity/error_vel_xy: 0.3476
Metrics/base_velocity/error_vel_yaw: 0.1622
      Episode_Termination/time_out: 0.1115
  Episode_Termination/base_contact: 0.8885
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 2.50s
                      Time elapsed: 00:05:53
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 138/1400 [0m                      

                       Computation: 9593 steps/s (collection: 2.309s, learning 0.253s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 8.9391
                       Mean reward: 2.69
               Mean episode length: 358.84
Episode_Reward/track_lin_vel_xy_exp: 0.1974
Episode_Reward/track_ang_vel_z_exp: 0.1505
       Episode_Reward/lin_vel_z_l2: -0.0184
      Episode_Reward/ang_vel_xy_l2: -0.0292
     Episode_Reward/dof_torques_l2: -0.0297
         Episode_Reward/dof_acc_l2: -0.0550
     Episode_Reward/action_rate_l2: -0.0351
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0113
Metrics/base_velocity/error_vel_xy: 0.3507
Metrics/base_velocity/error_vel_yaw: 0.1866
      Episode_Termination/time_out: 0.1133
  Episode_Termination/base_contact: 0.8867
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 2.56s
                      Time elapsed: 00:05:55
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 139/1400 [0m                      

                       Computation: 9724 steps/s (collection: 2.273s, learning 0.254s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 8.9227
                       Mean reward: 3.26
               Mean episode length: 366.98
Episode_Reward/track_lin_vel_xy_exp: 0.1679
Episode_Reward/track_ang_vel_z_exp: 0.1289
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0258
     Episode_Reward/dof_torques_l2: -0.0244
         Episode_Reward/dof_acc_l2: -0.0492
     Episode_Reward/action_rate_l2: -0.0291
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0117
Metrics/base_velocity/error_vel_xy: 0.2927
Metrics/base_velocity/error_vel_yaw: 0.1469
      Episode_Termination/time_out: 0.1151
  Episode_Termination/base_contact: 0.8849
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 2.53s
                      Time elapsed: 00:05:58
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 140/1400 [0m                      

                       Computation: 9627 steps/s (collection: 2.298s, learning 0.255s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0115
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 8.9005
                       Mean reward: 3.81
               Mean episode length: 416.98
Episode_Reward/track_lin_vel_xy_exp: 0.2191
Episode_Reward/track_ang_vel_z_exp: 0.1660
       Episode_Reward/lin_vel_z_l2: -0.0194
      Episode_Reward/ang_vel_xy_l2: -0.0308
     Episode_Reward/dof_torques_l2: -0.0325
         Episode_Reward/dof_acc_l2: -0.0593
     Episode_Reward/action_rate_l2: -0.0384
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0134
Metrics/base_velocity/error_vel_xy: 0.3685
Metrics/base_velocity/error_vel_yaw: 0.1933
      Episode_Termination/time_out: 0.1188
  Episode_Termination/base_contact: 0.8822
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 2.55s
                      Time elapsed: 00:06:00
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 141/1400 [0m                      

                       Computation: 9557 steps/s (collection: 2.315s, learning 0.256s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0093
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 8.8724
                       Mean reward: 2.61
               Mean episode length: 335.25
Episode_Reward/track_lin_vel_xy_exp: 0.1507
Episode_Reward/track_ang_vel_z_exp: 0.1208
       Episode_Reward/lin_vel_z_l2: -0.0181
      Episode_Reward/ang_vel_xy_l2: -0.0261
     Episode_Reward/dof_torques_l2: -0.0239
         Episode_Reward/dof_acc_l2: -0.0499
     Episode_Reward/action_rate_l2: -0.0282
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0153
Metrics/base_velocity/error_vel_xy: 0.2959
Metrics/base_velocity/error_vel_yaw: 0.1561
      Episode_Termination/time_out: 0.1211
  Episode_Termination/base_contact: 0.8799
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 2.57s
                      Time elapsed: 00:06:03
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 142/1400 [0m                      

                       Computation: 9645 steps/s (collection: 2.291s, learning 0.257s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0104
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 8.8492
                       Mean reward: 1.79
               Mean episode length: 259.59
Episode_Reward/track_lin_vel_xy_exp: 0.1090
Episode_Reward/track_ang_vel_z_exp: 0.0991
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0206
     Episode_Reward/dof_torques_l2: -0.0198
         Episode_Reward/dof_acc_l2: -0.0398
     Episode_Reward/action_rate_l2: -0.0227
      Episode_Reward/feet_air_time: -0.0044
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0180
Metrics/base_velocity/error_vel_xy: 0.2595
Metrics/base_velocity/error_vel_yaw: 0.1214
      Episode_Termination/time_out: 0.1183
  Episode_Termination/base_contact: 0.8827
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 2.55s
                      Time elapsed: 00:06:05
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 143/1400 [0m                      

                       Computation: 9670 steps/s (collection: 2.287s, learning 0.254s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0113
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 8.8311
                       Mean reward: 2.76
               Mean episode length: 327.69
Episode_Reward/track_lin_vel_xy_exp: 0.1935
Episode_Reward/track_ang_vel_z_exp: 0.1577
       Episode_Reward/lin_vel_z_l2: -0.0179
      Episode_Reward/ang_vel_xy_l2: -0.0298
     Episode_Reward/dof_torques_l2: -0.0299
         Episode_Reward/dof_acc_l2: -0.0569
     Episode_Reward/action_rate_l2: -0.0352
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0209
Metrics/base_velocity/error_vel_xy: 0.3587
Metrics/base_velocity/error_vel_yaw: 0.1716
      Episode_Termination/time_out: 0.1182
  Episode_Termination/base_contact: 0.8828
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 2.54s
                      Time elapsed: 00:06:08
                               ETA: 00:53:35

################################################################################
                     [1m Learning iteration 144/1400 [0m                      

                       Computation: 9614 steps/s (collection: 2.294s, learning 0.262s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0097
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 8.8070
                       Mean reward: 3.76
               Mean episode length: 398.59
Episode_Reward/track_lin_vel_xy_exp: 0.1846
Episode_Reward/track_ang_vel_z_exp: 0.1424
       Episode_Reward/lin_vel_z_l2: -0.0178
      Episode_Reward/ang_vel_xy_l2: -0.0285
     Episode_Reward/dof_torques_l2: -0.0270
         Episode_Reward/dof_acc_l2: -0.0517
     Episode_Reward/action_rate_l2: -0.0321
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0222
Metrics/base_velocity/error_vel_xy: 0.3193
Metrics/base_velocity/error_vel_yaw: 0.1595
      Episode_Termination/time_out: 0.1231
  Episode_Termination/base_contact: 0.8779
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 2.56s
                      Time elapsed: 00:06:10
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 145/1400 [0m                      

                       Computation: 9581 steps/s (collection: 2.311s, learning 0.254s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 8.8024
                       Mean reward: 3.79
               Mean episode length: 383.08
Episode_Reward/track_lin_vel_xy_exp: 0.1996
Episode_Reward/track_ang_vel_z_exp: 0.1517
       Episode_Reward/lin_vel_z_l2: -0.0181
      Episode_Reward/ang_vel_xy_l2: -0.0281
     Episode_Reward/dof_torques_l2: -0.0284
         Episode_Reward/dof_acc_l2: -0.0525
     Episode_Reward/action_rate_l2: -0.0337
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0229
Metrics/base_velocity/error_vel_xy: 0.3181
Metrics/base_velocity/error_vel_yaw: 0.1595
      Episode_Termination/time_out: 0.1272
  Episode_Termination/base_contact: 0.8738
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 2.57s
                      Time elapsed: 00:06:13
                               ETA: 00:53:30

################################################################################
                     [1m Learning iteration 146/1400 [0m                      

                       Computation: 9528 steps/s (collection: 2.325s, learning 0.255s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 8.7984
                       Mean reward: 3.35
               Mean episode length: 356.44
Episode_Reward/track_lin_vel_xy_exp: 0.1751
Episode_Reward/track_ang_vel_z_exp: 0.1322
       Episode_Reward/lin_vel_z_l2: -0.0176
      Episode_Reward/ang_vel_xy_l2: -0.0266
     Episode_Reward/dof_torques_l2: -0.0260
         Episode_Reward/dof_acc_l2: -0.0518
     Episode_Reward/action_rate_l2: -0.0300
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0232
Metrics/base_velocity/error_vel_xy: 0.3033
Metrics/base_velocity/error_vel_yaw: 0.1585
      Episode_Termination/time_out: 0.1328
  Episode_Termination/base_contact: 0.8682
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 2.58s
                      Time elapsed: 00:06:16
                               ETA: 00:53:28

################################################################################
                     [1m Learning iteration 147/1400 [0m                      

                       Computation: 9581 steps/s (collection: 2.309s, learning 0.256s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0098
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 8.7814
                       Mean reward: 2.77
               Mean episode length: 321.70
Episode_Reward/track_lin_vel_xy_exp: 0.1687
Episode_Reward/track_ang_vel_z_exp: 0.1348
       Episode_Reward/lin_vel_z_l2: -0.0173
      Episode_Reward/ang_vel_xy_l2: -0.0267
     Episode_Reward/dof_torques_l2: -0.0278
         Episode_Reward/dof_acc_l2: -0.0498
     Episode_Reward/action_rate_l2: -0.0305
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0249
Metrics/base_velocity/error_vel_xy: 0.3211
Metrics/base_velocity/error_vel_yaw: 0.1634
      Episode_Termination/time_out: 0.1305
  Episode_Termination/base_contact: 0.8695
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 2.56s
                      Time elapsed: 00:06:18
                               ETA: 00:53:25

################################################################################
                     [1m Learning iteration 148/1400 [0m                      

                       Computation: 9631 steps/s (collection: 2.296s, learning 0.256s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0122
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 8.7574
                       Mean reward: 3.66
               Mean episode length: 392.30
Episode_Reward/track_lin_vel_xy_exp: 0.2081
Episode_Reward/track_ang_vel_z_exp: 0.1583
       Episode_Reward/lin_vel_z_l2: -0.0197
      Episode_Reward/ang_vel_xy_l2: -0.0296
     Episode_Reward/dof_torques_l2: -0.0310
         Episode_Reward/dof_acc_l2: -0.0570
     Episode_Reward/action_rate_l2: -0.0355
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0251
Metrics/base_velocity/error_vel_xy: 0.3483
Metrics/base_velocity/error_vel_yaw: 0.1789
      Episode_Termination/time_out: 0.1305
  Episode_Termination/base_contact: 0.8695
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 2.55s
                      Time elapsed: 00:06:21
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 149/1400 [0m                      

                       Computation: 9707 steps/s (collection: 2.279s, learning 0.252s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0105
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 8.7346
                       Mean reward: 3.48
               Mean episode length: 380.28
Episode_Reward/track_lin_vel_xy_exp: 0.1543
Episode_Reward/track_ang_vel_z_exp: 0.1233
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0244
     Episode_Reward/dof_torques_l2: -0.0242
         Episode_Reward/dof_acc_l2: -0.0432
     Episode_Reward/action_rate_l2: -0.0267
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0255
Metrics/base_velocity/error_vel_xy: 0.2785
Metrics/base_velocity/error_vel_yaw: 0.1335
      Episode_Termination/time_out: 0.1348
  Episode_Termination/base_contact: 0.8652
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 2.53s
                      Time elapsed: 00:06:23
                               ETA: 00:53:20

################################################################################
                     [1m Learning iteration 150/1400 [0m                      

                       Computation: 9696 steps/s (collection: 2.280s, learning 0.254s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0106
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 8.7094
                       Mean reward: 2.86
               Mean episode length: 310.84
Episode_Reward/track_lin_vel_xy_exp: 0.1991
Episode_Reward/track_ang_vel_z_exp: 0.1360
       Episode_Reward/lin_vel_z_l2: -0.0181
      Episode_Reward/ang_vel_xy_l2: -0.0258
     Episode_Reward/dof_torques_l2: -0.0267
         Episode_Reward/dof_acc_l2: -0.0554
     Episode_Reward/action_rate_l2: -0.0306
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0264
Metrics/base_velocity/error_vel_xy: 0.2726
Metrics/base_velocity/error_vel_yaw: 0.1560
      Episode_Termination/time_out: 0.1349
  Episode_Termination/base_contact: 0.8651
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 2.53s
                      Time elapsed: 00:06:26
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 151/1400 [0m                      

                       Computation: 9735 steps/s (collection: 2.269s, learning 0.255s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0108
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 8.6923
                       Mean reward: 2.58
               Mean episode length: 314.01
Episode_Reward/track_lin_vel_xy_exp: 0.1594
Episode_Reward/track_ang_vel_z_exp: 0.1268
       Episode_Reward/lin_vel_z_l2: -0.0172
      Episode_Reward/ang_vel_xy_l2: -0.0258
     Episode_Reward/dof_torques_l2: -0.0258
         Episode_Reward/dof_acc_l2: -0.0532
     Episode_Reward/action_rate_l2: -0.0284
      Episode_Reward/feet_air_time: -0.0052
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0256
Metrics/base_velocity/error_vel_xy: 0.3072
Metrics/base_velocity/error_vel_yaw: 0.1524
      Episode_Termination/time_out: 0.1368
  Episode_Termination/base_contact: 0.8632
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 2.52s
                      Time elapsed: 00:06:28
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 152/1400 [0m                      

                       Computation: 9630 steps/s (collection: 2.298s, learning 0.254s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0107
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 8.6783
                       Mean reward: 2.86
               Mean episode length: 333.96
Episode_Reward/track_lin_vel_xy_exp: 0.1777
Episode_Reward/track_ang_vel_z_exp: 0.1351
       Episode_Reward/lin_vel_z_l2: -0.0178
      Episode_Reward/ang_vel_xy_l2: -0.0267
     Episode_Reward/dof_torques_l2: -0.0279
         Episode_Reward/dof_acc_l2: -0.0525
     Episode_Reward/action_rate_l2: -0.0300
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0274
Metrics/base_velocity/error_vel_xy: 0.3041
Metrics/base_velocity/error_vel_yaw: 0.1576
      Episode_Termination/time_out: 0.1383
  Episode_Termination/base_contact: 0.8617
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 2.55s
                      Time elapsed: 00:06:31
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 153/1400 [0m                      

                       Computation: 9645 steps/s (collection: 2.289s, learning 0.259s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 8.6598
                       Mean reward: 3.63
               Mean episode length: 355.88
Episode_Reward/track_lin_vel_xy_exp: 0.1913
Episode_Reward/track_ang_vel_z_exp: 0.1290
       Episode_Reward/lin_vel_z_l2: -0.0178
      Episode_Reward/ang_vel_xy_l2: -0.0259
     Episode_Reward/dof_torques_l2: -0.0254
         Episode_Reward/dof_acc_l2: -0.0522
     Episode_Reward/action_rate_l2: -0.0292
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0300
Metrics/base_velocity/error_vel_xy: 0.2606
Metrics/base_velocity/error_vel_yaw: 0.1558
      Episode_Termination/time_out: 0.1423
  Episode_Termination/base_contact: 0.8577
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 2.55s
                      Time elapsed: 00:06:33
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 154/1400 [0m                      

                       Computation: 9500 steps/s (collection: 2.332s, learning 0.255s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 8.6436
                       Mean reward: 4.03
               Mean episode length: 406.84
Episode_Reward/track_lin_vel_xy_exp: 0.2185
Episode_Reward/track_ang_vel_z_exp: 0.1642
       Episode_Reward/lin_vel_z_l2: -0.0213
      Episode_Reward/ang_vel_xy_l2: -0.0317
     Episode_Reward/dof_torques_l2: -0.0314
         Episode_Reward/dof_acc_l2: -0.0627
     Episode_Reward/action_rate_l2: -0.0360
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0317
Metrics/base_velocity/error_vel_xy: 0.3527
Metrics/base_velocity/error_vel_yaw: 0.1801
      Episode_Termination/time_out: 0.1465
  Episode_Termination/base_contact: 0.8535
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 2.59s
                      Time elapsed: 00:06:36
                               ETA: 00:53:07

################################################################################
                     [1m Learning iteration 155/1400 [0m                      

                       Computation: 9519 steps/s (collection: 2.325s, learning 0.256s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 8.6266
                       Mean reward: 3.73
               Mean episode length: 408.52
Episode_Reward/track_lin_vel_xy_exp: 0.1649
Episode_Reward/track_ang_vel_z_exp: 0.1337
       Episode_Reward/lin_vel_z_l2: -0.0184
      Episode_Reward/ang_vel_xy_l2: -0.0271
     Episode_Reward/dof_torques_l2: -0.0268
         Episode_Reward/dof_acc_l2: -0.0512
     Episode_Reward/action_rate_l2: -0.0294
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0370
Metrics/base_velocity/error_vel_xy: 0.3150
Metrics/base_velocity/error_vel_yaw: 0.1554
      Episode_Termination/time_out: 0.1492
  Episode_Termination/base_contact: 0.8508
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 2.58s
                      Time elapsed: 00:06:39
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 156/1400 [0m                      

                       Computation: 9591 steps/s (collection: 2.308s, learning 0.254s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0117
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 8.6176
                       Mean reward: 2.89
               Mean episode length: 362.31
Episode_Reward/track_lin_vel_xy_exp: 0.1683
Episode_Reward/track_ang_vel_z_exp: 0.1388
       Episode_Reward/lin_vel_z_l2: -0.0196
      Episode_Reward/ang_vel_xy_l2: -0.0278
     Episode_Reward/dof_torques_l2: -0.0281
         Episode_Reward/dof_acc_l2: -0.0577
     Episode_Reward/action_rate_l2: -0.0309
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0430
Metrics/base_velocity/error_vel_xy: 0.3341
Metrics/base_velocity/error_vel_yaw: 0.1629
      Episode_Termination/time_out: 0.1497
  Episode_Termination/base_contact: 0.8503
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 2.56s
                      Time elapsed: 00:06:41
                               ETA: 00:53:02

################################################################################
                     [1m Learning iteration 157/1400 [0m                      

                       Computation: 9473 steps/s (collection: 2.336s, learning 0.258s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0098
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 8.5831
                       Mean reward: 3.29
               Mean episode length: 357.70
Episode_Reward/track_lin_vel_xy_exp: 0.1623
Episode_Reward/track_ang_vel_z_exp: 0.1221
       Episode_Reward/lin_vel_z_l2: -0.0162
      Episode_Reward/ang_vel_xy_l2: -0.0250
     Episode_Reward/dof_torques_l2: -0.0248
         Episode_Reward/dof_acc_l2: -0.0480
     Episode_Reward/action_rate_l2: -0.0272
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0478
Metrics/base_velocity/error_vel_xy: 0.2771
Metrics/base_velocity/error_vel_yaw: 0.1509
      Episode_Termination/time_out: 0.1535
  Episode_Termination/base_contact: 0.8465
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 2.59s
                      Time elapsed: 00:06:44
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 158/1400 [0m                      

                       Computation: 9377 steps/s (collection: 2.365s, learning 0.256s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0106
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 8.5611
                       Mean reward: 3.53
               Mean episode length: 367.88
Episode_Reward/track_lin_vel_xy_exp: 0.1813
Episode_Reward/track_ang_vel_z_exp: 0.1295
       Episode_Reward/lin_vel_z_l2: -0.0177
      Episode_Reward/ang_vel_xy_l2: -0.0259
     Episode_Reward/dof_torques_l2: -0.0258
         Episode_Reward/dof_acc_l2: -0.0501
     Episode_Reward/action_rate_l2: -0.0285
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0495
Metrics/base_velocity/error_vel_xy: 0.2674
Metrics/base_velocity/error_vel_yaw: 0.1464
      Episode_Termination/time_out: 0.1573
  Episode_Termination/base_contact: 0.8427
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 2.62s
                      Time elapsed: 00:06:46
                               ETA: 00:52:57

################################################################################
                     [1m Learning iteration 159/1400 [0m                      

                       Computation: 9416 steps/s (collection: 2.353s, learning 0.257s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0108
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 8.5540
                       Mean reward: 2.96
               Mean episode length: 354.91
Episode_Reward/track_lin_vel_xy_exp: 0.1942
Episode_Reward/track_ang_vel_z_exp: 0.1551
       Episode_Reward/lin_vel_z_l2: -0.0211
      Episode_Reward/ang_vel_xy_l2: -0.0305
     Episode_Reward/dof_torques_l2: -0.0317
         Episode_Reward/dof_acc_l2: -0.0617
     Episode_Reward/action_rate_l2: -0.0342
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0530
Metrics/base_velocity/error_vel_xy: 0.3558
Metrics/base_velocity/error_vel_yaw: 0.1757
      Episode_Termination/time_out: 0.1608
  Episode_Termination/base_contact: 0.8392
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 2.61s
                      Time elapsed: 00:06:49
                               ETA: 00:52:55

################################################################################
                     [1m Learning iteration 160/1400 [0m                      

                       Computation: 9362 steps/s (collection: 2.369s, learning 0.256s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0107
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 8.5454
                       Mean reward: 2.78
               Mean episode length: 345.27
Episode_Reward/track_lin_vel_xy_exp: 0.2168
Episode_Reward/track_ang_vel_z_exp: 0.1610
       Episode_Reward/lin_vel_z_l2: -0.0214
      Episode_Reward/ang_vel_xy_l2: -0.0324
     Episode_Reward/dof_torques_l2: -0.0330
         Episode_Reward/dof_acc_l2: -0.0617
     Episode_Reward/action_rate_l2: -0.0358
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0560
Metrics/base_velocity/error_vel_xy: 0.3566
Metrics/base_velocity/error_vel_yaw: 0.1891
      Episode_Termination/time_out: 0.1632
  Episode_Termination/base_contact: 0.8368
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 2.63s
                      Time elapsed: 00:06:52
                               ETA: 00:52:53

################################################################################
                     [1m Learning iteration 161/1400 [0m                      

                       Computation: 9561 steps/s (collection: 2.314s, learning 0.256s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0103
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 8.5240
                       Mean reward: 3.20
               Mean episode length: 374.32
Episode_Reward/track_lin_vel_xy_exp: 0.2108
Episode_Reward/track_ang_vel_z_exp: 0.1474
       Episode_Reward/lin_vel_z_l2: -0.0200
      Episode_Reward/ang_vel_xy_l2: -0.0304
     Episode_Reward/dof_torques_l2: -0.0312
         Episode_Reward/dof_acc_l2: -0.0609
     Episode_Reward/action_rate_l2: -0.0333
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0633
Metrics/base_velocity/error_vel_xy: 0.3258
Metrics/base_velocity/error_vel_yaw: 0.1975
      Episode_Termination/time_out: 0.1660
  Episode_Termination/base_contact: 0.8340
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 2.57s
                      Time elapsed: 00:06:54
                               ETA: 00:52:51

################################################################################
                     [1m Learning iteration 162/1400 [0m                      

                       Computation: 9593 steps/s (collection: 2.307s, learning 0.255s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 8.5131
                       Mean reward: 3.72
               Mean episode length: 389.10
Episode_Reward/track_lin_vel_xy_exp: 0.1971
Episode_Reward/track_ang_vel_z_exp: 0.1379
       Episode_Reward/lin_vel_z_l2: -0.0199
      Episode_Reward/ang_vel_xy_l2: -0.0275
     Episode_Reward/dof_torques_l2: -0.0284
         Episode_Reward/dof_acc_l2: -0.0569
     Episode_Reward/action_rate_l2: -0.0306
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0703
Metrics/base_velocity/error_vel_xy: 0.2886
Metrics/base_velocity/error_vel_yaw: 0.1645
      Episode_Termination/time_out: 0.1703
  Episode_Termination/base_contact: 0.8297
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 2.56s
                      Time elapsed: 00:06:57
                               ETA: 00:52:48

################################################################################
                     [1m Learning iteration 163/1400 [0m                      

                       Computation: 9540 steps/s (collection: 2.319s, learning 0.257s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 8.4919
                       Mean reward: 3.71
               Mean episode length: 388.94
Episode_Reward/track_lin_vel_xy_exp: 0.2171
Episode_Reward/track_ang_vel_z_exp: 0.1540
       Episode_Reward/lin_vel_z_l2: -0.0214
      Episode_Reward/ang_vel_xy_l2: -0.0310
     Episode_Reward/dof_torques_l2: -0.0310
         Episode_Reward/dof_acc_l2: -0.0616
     Episode_Reward/action_rate_l2: -0.0338
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0795
Metrics/base_velocity/error_vel_xy: 0.3243
Metrics/base_velocity/error_vel_yaw: 0.1806
      Episode_Termination/time_out: 0.1736
  Episode_Termination/base_contact: 0.8264
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 2.58s
                      Time elapsed: 00:06:59
                               ETA: 00:52:46

################################################################################
                     [1m Learning iteration 164/1400 [0m                      

                       Computation: 9702 steps/s (collection: 2.283s, learning 0.250s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0113
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 8.4618
                       Mean reward: 3.84
               Mean episode length: 413.20
Episode_Reward/track_lin_vel_xy_exp: 0.2194
Episode_Reward/track_ang_vel_z_exp: 0.1591
       Episode_Reward/lin_vel_z_l2: -0.0210
      Episode_Reward/ang_vel_xy_l2: -0.0321
     Episode_Reward/dof_torques_l2: -0.0321
         Episode_Reward/dof_acc_l2: -0.0613
     Episode_Reward/action_rate_l2: -0.0348
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0871
Metrics/base_velocity/error_vel_xy: 0.3399
Metrics/base_velocity/error_vel_yaw: 0.1800
      Episode_Termination/time_out: 0.1819
  Episode_Termination/base_contact: 0.8181
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 2.53s
                      Time elapsed: 00:07:02
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 165/1400 [0m                      

                       Computation: 9503 steps/s (collection: 2.329s, learning 0.257s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0109
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 8.4513
                       Mean reward: 3.80
               Mean episode length: 401.85
Episode_Reward/track_lin_vel_xy_exp: 0.2106
Episode_Reward/track_ang_vel_z_exp: 0.1472
       Episode_Reward/lin_vel_z_l2: -0.0196
      Episode_Reward/ang_vel_xy_l2: -0.0293
     Episode_Reward/dof_torques_l2: -0.0295
         Episode_Reward/dof_acc_l2: -0.0579
     Episode_Reward/action_rate_l2: -0.0322
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0917
Metrics/base_velocity/error_vel_xy: 0.3042
Metrics/base_velocity/error_vel_yaw: 0.1707
      Episode_Termination/time_out: 0.1909
  Episode_Termination/base_contact: 0.8091
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 2.59s
                      Time elapsed: 00:07:04
                               ETA: 00:52:41

################################################################################
                     [1m Learning iteration 166/1400 [0m                      

                       Computation: 9583 steps/s (collection: 2.309s, learning 0.256s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 8.4347
                       Mean reward: 3.27
               Mean episode length: 350.99
Episode_Reward/track_lin_vel_xy_exp: 0.1904
Episode_Reward/track_ang_vel_z_exp: 0.1356
       Episode_Reward/lin_vel_z_l2: -0.0190
      Episode_Reward/ang_vel_xy_l2: -0.0268
     Episode_Reward/dof_torques_l2: -0.0278
         Episode_Reward/dof_acc_l2: -0.0533
     Episode_Reward/action_rate_l2: -0.0294
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0957
Metrics/base_velocity/error_vel_xy: 0.2791
Metrics/base_velocity/error_vel_yaw: 0.1523
      Episode_Termination/time_out: 0.1961
  Episode_Termination/base_contact: 0.8039
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 2.56s
                      Time elapsed: 00:07:07
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 167/1400 [0m                      

                       Computation: 9579 steps/s (collection: 2.310s, learning 0.255s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 8.4240
                       Mean reward: 2.76
               Mean episode length: 315.52
Episode_Reward/track_lin_vel_xy_exp: 0.1707
Episode_Reward/track_ang_vel_z_exp: 0.1244
       Episode_Reward/lin_vel_z_l2: -0.0183
      Episode_Reward/ang_vel_xy_l2: -0.0258
     Episode_Reward/dof_torques_l2: -0.0264
         Episode_Reward/dof_acc_l2: -0.0517
     Episode_Reward/action_rate_l2: -0.0274
      Episode_Reward/feet_air_time: -0.0045
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0967
Metrics/base_velocity/error_vel_xy: 0.2883
Metrics/base_velocity/error_vel_yaw: 0.1527
      Episode_Termination/time_out: 0.1947
  Episode_Termination/base_contact: 0.8053
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 2.57s
                      Time elapsed: 00:07:10
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 168/1400 [0m                      

                       Computation: 9529 steps/s (collection: 2.321s, learning 0.258s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0101
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 8.4100
                       Mean reward: 2.71
               Mean episode length: 347.66
Episode_Reward/track_lin_vel_xy_exp: 0.2442
Episode_Reward/track_ang_vel_z_exp: 0.1739
       Episode_Reward/lin_vel_z_l2: -0.0235
      Episode_Reward/ang_vel_xy_l2: -0.0347
     Episode_Reward/dof_torques_l2: -0.0360
         Episode_Reward/dof_acc_l2: -0.0760
     Episode_Reward/action_rate_l2: -0.0397
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0997
Metrics/base_velocity/error_vel_xy: 0.3956
Metrics/base_velocity/error_vel_yaw: 0.2299
      Episode_Termination/time_out: 0.1962
  Episode_Termination/base_contact: 0.8038
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 2.58s
                      Time elapsed: 00:07:12
                               ETA: 00:52:33

################################################################################
                     [1m Learning iteration 169/1400 [0m                      

                       Computation: 9595 steps/s (collection: 2.297s, learning 0.264s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0115
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 8.3951
                       Mean reward: 3.19
               Mean episode length: 374.58
Episode_Reward/track_lin_vel_xy_exp: 0.2252
Episode_Reward/track_ang_vel_z_exp: 0.1546
       Episode_Reward/lin_vel_z_l2: -0.0207
      Episode_Reward/ang_vel_xy_l2: -0.0298
     Episode_Reward/dof_torques_l2: -0.0312
         Episode_Reward/dof_acc_l2: -0.0601
     Episode_Reward/action_rate_l2: -0.0336
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1021
Metrics/base_velocity/error_vel_xy: 0.3097
Metrics/base_velocity/error_vel_yaw: 0.1741
      Episode_Termination/time_out: 0.1999
  Episode_Termination/base_contact: 0.8001
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 2.56s
                      Time elapsed: 00:07:15
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 170/1400 [0m                      

                       Computation: 9596 steps/s (collection: 2.306s, learning 0.255s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 8.3779
                       Mean reward: 3.69
               Mean episode length: 394.08
Episode_Reward/track_lin_vel_xy_exp: 0.2478
Episode_Reward/track_ang_vel_z_exp: 0.1819
       Episode_Reward/lin_vel_z_l2: -0.0242
      Episode_Reward/ang_vel_xy_l2: -0.0360
     Episode_Reward/dof_torques_l2: -0.0377
         Episode_Reward/dof_acc_l2: -0.0709
     Episode_Reward/action_rate_l2: -0.0393
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1052
Metrics/base_velocity/error_vel_xy: 0.3758
Metrics/base_velocity/error_vel_yaw: 0.1999
      Episode_Termination/time_out: 0.2048
  Episode_Termination/base_contact: 0.7952
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 2.56s
                      Time elapsed: 00:07:17
                               ETA: 00:52:28

################################################################################
                     [1m Learning iteration 171/1400 [0m                      

                       Computation: 9550 steps/s (collection: 2.318s, learning 0.255s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 8.3676
                       Mean reward: 3.62
               Mean episode length: 373.20
Episode_Reward/track_lin_vel_xy_exp: 0.2421
Episode_Reward/track_ang_vel_z_exp: 0.1565
       Episode_Reward/lin_vel_z_l2: -0.0215
      Episode_Reward/ang_vel_xy_l2: -0.0309
     Episode_Reward/dof_torques_l2: -0.0322
         Episode_Reward/dof_acc_l2: -0.0647
     Episode_Reward/action_rate_l2: -0.0346
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1106
Metrics/base_velocity/error_vel_xy: 0.2945
Metrics/base_velocity/error_vel_yaw: 0.1820
      Episode_Termination/time_out: 0.2100
  Episode_Termination/base_contact: 0.7900
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 2.57s
                      Time elapsed: 00:07:20
                               ETA: 00:52:26

################################################################################
                     [1m Learning iteration 172/1400 [0m                      

                       Computation: 9666 steps/s (collection: 2.286s, learning 0.256s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0122
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 8.3722
                       Mean reward: 3.85
               Mean episode length: 366.35
Episode_Reward/track_lin_vel_xy_exp: 0.2541
Episode_Reward/track_ang_vel_z_exp: 0.1634
       Episode_Reward/lin_vel_z_l2: -0.0232
      Episode_Reward/ang_vel_xy_l2: -0.0340
     Episode_Reward/dof_torques_l2: -0.0332
         Episode_Reward/dof_acc_l2: -0.0646
     Episode_Reward/action_rate_l2: -0.0358
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1178
Metrics/base_velocity/error_vel_xy: 0.3025
Metrics/base_velocity/error_vel_yaw: 0.1862
      Episode_Termination/time_out: 0.2158
  Episode_Termination/base_contact: 0.7842
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 2.54s
                      Time elapsed: 00:07:22
                               ETA: 00:52:23

################################################################################
                     [1m Learning iteration 173/1400 [0m                      

                       Computation: 9651 steps/s (collection: 2.291s, learning 0.255s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0117
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 8.3686
                       Mean reward: 3.68
               Mean episode length: 394.89
Episode_Reward/track_lin_vel_xy_exp: 0.1920
Episode_Reward/track_ang_vel_z_exp: 0.1502
       Episode_Reward/lin_vel_z_l2: -0.0213
      Episode_Reward/ang_vel_xy_l2: -0.0305
     Episode_Reward/dof_torques_l2: -0.0333
         Episode_Reward/dof_acc_l2: -0.0642
     Episode_Reward/action_rate_l2: -0.0336
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1236
Metrics/base_velocity/error_vel_xy: 0.3715
Metrics/base_velocity/error_vel_yaw: 0.1959
      Episode_Termination/time_out: 0.2195
  Episode_Termination/base_contact: 0.7805
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 2.55s
                      Time elapsed: 00:07:25
                               ETA: 00:52:20

################################################################################
                     [1m Learning iteration 174/1400 [0m                      

                       Computation: 9682 steps/s (collection: 2.281s, learning 0.257s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 8.3537
                       Mean reward: 3.63
               Mean episode length: 416.93
Episode_Reward/track_lin_vel_xy_exp: 0.2221
Episode_Reward/track_ang_vel_z_exp: 0.1549
       Episode_Reward/lin_vel_z_l2: -0.0214
      Episode_Reward/ang_vel_xy_l2: -0.0304
     Episode_Reward/dof_torques_l2: -0.0317
         Episode_Reward/dof_acc_l2: -0.0605
     Episode_Reward/action_rate_l2: -0.0331
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1249
Metrics/base_velocity/error_vel_xy: 0.3076
Metrics/base_velocity/error_vel_yaw: 0.1706
      Episode_Termination/time_out: 0.2243
  Episode_Termination/base_contact: 0.7757
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 2.54s
                      Time elapsed: 00:07:27
                               ETA: 00:52:18

################################################################################
                     [1m Learning iteration 175/1400 [0m                      

                       Computation: 9599 steps/s (collection: 2.305s, learning 0.256s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 8.3378
                       Mean reward: 3.66
               Mean episode length: 408.87
Episode_Reward/track_lin_vel_xy_exp: 0.1971
Episode_Reward/track_ang_vel_z_exp: 0.1433
       Episode_Reward/lin_vel_z_l2: -0.0209
      Episode_Reward/ang_vel_xy_l2: -0.0293
     Episode_Reward/dof_torques_l2: -0.0300
         Episode_Reward/dof_acc_l2: -0.0616
     Episode_Reward/action_rate_l2: -0.0314
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1312
Metrics/base_velocity/error_vel_xy: 0.3078
Metrics/base_velocity/error_vel_yaw: 0.1703
      Episode_Termination/time_out: 0.2275
  Episode_Termination/base_contact: 0.7725
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 2.56s
                      Time elapsed: 00:07:30
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 176/1400 [0m                      

                       Computation: 9587 steps/s (collection: 2.306s, learning 0.257s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 8.3428
                       Mean reward: 3.56
               Mean episode length: 368.90
Episode_Reward/track_lin_vel_xy_exp: 0.2408
Episode_Reward/track_ang_vel_z_exp: 0.1611
       Episode_Reward/lin_vel_z_l2: -0.0215
      Episode_Reward/ang_vel_xy_l2: -0.0314
     Episode_Reward/dof_torques_l2: -0.0336
         Episode_Reward/dof_acc_l2: -0.0641
     Episode_Reward/action_rate_l2: -0.0351
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1393
Metrics/base_velocity/error_vel_xy: 0.3173
Metrics/base_velocity/error_vel_yaw: 0.1879
      Episode_Termination/time_out: 0.2277
  Episode_Termination/base_contact: 0.7723
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 2.56s
                      Time elapsed: 00:07:33
                               ETA: 00:52:13

################################################################################
                     [1m Learning iteration 177/1400 [0m                      

                       Computation: 9597 steps/s (collection: 2.305s, learning 0.255s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 8.3425
                       Mean reward: 3.36
               Mean episode length: 360.86
Episode_Reward/track_lin_vel_xy_exp: 0.2184
Episode_Reward/track_ang_vel_z_exp: 0.1521
       Episode_Reward/lin_vel_z_l2: -0.0208
      Episode_Reward/ang_vel_xy_l2: -0.0313
     Episode_Reward/dof_torques_l2: -0.0312
         Episode_Reward/dof_acc_l2: -0.0603
     Episode_Reward/action_rate_l2: -0.0331
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1430
Metrics/base_velocity/error_vel_xy: 0.3069
Metrics/base_velocity/error_vel_yaw: 0.1685
      Episode_Termination/time_out: 0.2244
  Episode_Termination/base_contact: 0.7756
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 2.56s
                      Time elapsed: 00:07:35
                               ETA: 00:52:10

################################################################################
                     [1m Learning iteration 178/1400 [0m                      

                       Computation: 9579 steps/s (collection: 2.310s, learning 0.256s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 8.3311
                       Mean reward: 3.37
               Mean episode length: 380.78
Episode_Reward/track_lin_vel_xy_exp: 0.2140
Episode_Reward/track_ang_vel_z_exp: 0.1483
       Episode_Reward/lin_vel_z_l2: -0.0204
      Episode_Reward/ang_vel_xy_l2: -0.0298
     Episode_Reward/dof_torques_l2: -0.0317
         Episode_Reward/dof_acc_l2: -0.0606
     Episode_Reward/action_rate_l2: -0.0323
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1470
Metrics/base_velocity/error_vel_xy: 0.3149
Metrics/base_velocity/error_vel_yaw: 0.1760
      Episode_Termination/time_out: 0.2229
  Episode_Termination/base_contact: 0.7771
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 2.57s
                      Time elapsed: 00:07:38
                               ETA: 00:52:07

################################################################################
                     [1m Learning iteration 179/1400 [0m                      

                       Computation: 9633 steps/s (collection: 2.293s, learning 0.258s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 8.3253
                       Mean reward: 3.76
               Mean episode length: 400.35
Episode_Reward/track_lin_vel_xy_exp: 0.2656
Episode_Reward/track_ang_vel_z_exp: 0.1729
       Episode_Reward/lin_vel_z_l2: -0.0232
      Episode_Reward/ang_vel_xy_l2: -0.0337
     Episode_Reward/dof_torques_l2: -0.0359
         Episode_Reward/dof_acc_l2: -0.0711
     Episode_Reward/action_rate_l2: -0.0375
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1528
Metrics/base_velocity/error_vel_xy: 0.3257
Metrics/base_velocity/error_vel_yaw: 0.1939
      Episode_Termination/time_out: 0.2211
  Episode_Termination/base_contact: 0.7797
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.55s
                      Time elapsed: 00:07:40
                               ETA: 00:52:05

################################################################################
                     [1m Learning iteration 180/1400 [0m                      

                       Computation: 9739 steps/s (collection: 2.271s, learning 0.252s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 8.3108
                       Mean reward: 3.99
               Mean episode length: 397.52
Episode_Reward/track_lin_vel_xy_exp: 0.2199
Episode_Reward/track_ang_vel_z_exp: 0.1434
       Episode_Reward/lin_vel_z_l2: -0.0208
      Episode_Reward/ang_vel_xy_l2: -0.0298
     Episode_Reward/dof_torques_l2: -0.0315
         Episode_Reward/dof_acc_l2: -0.0617
     Episode_Reward/action_rate_l2: -0.0318
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1574
Metrics/base_velocity/error_vel_xy: 0.2965
Metrics/base_velocity/error_vel_yaw: 0.1818
      Episode_Termination/time_out: 0.2194
  Episode_Termination/base_contact: 0.7816
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 2.52s
                      Time elapsed: 00:07:43
                               ETA: 00:52:02

################################################################################
                     [1m Learning iteration 181/1400 [0m                      

                       Computation: 9589 steps/s (collection: 2.307s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 8.2835
                       Mean reward: 3.56
               Mean episode length: 358.99
Episode_Reward/track_lin_vel_xy_exp: 0.2338
Episode_Reward/track_ang_vel_z_exp: 0.1606
       Episode_Reward/lin_vel_z_l2: -0.0230
      Episode_Reward/ang_vel_xy_l2: -0.0318
     Episode_Reward/dof_torques_l2: -0.0347
         Episode_Reward/dof_acc_l2: -0.0673
     Episode_Reward/action_rate_l2: -0.0347
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1580
Metrics/base_velocity/error_vel_xy: 0.3122
Metrics/base_velocity/error_vel_yaw: 0.1785
      Episode_Termination/time_out: 0.2203
  Episode_Termination/base_contact: 0.7806
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 2.56s
                      Time elapsed: 00:07:45
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 182/1400 [0m                      

                       Computation: 9543 steps/s (collection: 2.318s, learning 0.258s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 8.2672
                       Mean reward: 3.71
               Mean episode length: 378.58
Episode_Reward/track_lin_vel_xy_exp: 0.2471
Episode_Reward/track_ang_vel_z_exp: 0.1575
       Episode_Reward/lin_vel_z_l2: -0.0226
      Episode_Reward/ang_vel_xy_l2: -0.0315
     Episode_Reward/dof_torques_l2: -0.0333
         Episode_Reward/dof_acc_l2: -0.0602
     Episode_Reward/action_rate_l2: -0.0344
      Episode_Reward/feet_air_time: -0.0051
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1581
Metrics/base_velocity/error_vel_xy: 0.2958
Metrics/base_velocity/error_vel_yaw: 0.1846
      Episode_Termination/time_out: 0.2210
  Episode_Termination/base_contact: 0.7799
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 2.58s
                      Time elapsed: 00:07:48
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 183/1400 [0m                      

                       Computation: 9541 steps/s (collection: 2.324s, learning 0.252s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 8.2523
                       Mean reward: 4.17
               Mean episode length: 428.59
Episode_Reward/track_lin_vel_xy_exp: 0.2223
Episode_Reward/track_ang_vel_z_exp: 0.1500
       Episode_Reward/lin_vel_z_l2: -0.0226
      Episode_Reward/ang_vel_xy_l2: -0.0311
     Episode_Reward/dof_torques_l2: -0.0325
         Episode_Reward/dof_acc_l2: -0.0629
     Episode_Reward/action_rate_l2: -0.0327
      Episode_Reward/feet_air_time: -0.0052
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1707
Metrics/base_velocity/error_vel_xy: 0.3037
Metrics/base_velocity/error_vel_yaw: 0.1816
      Episode_Termination/time_out: 0.2240
  Episode_Termination/base_contact: 0.7769
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 2.58s
                      Time elapsed: 00:07:50
                               ETA: 00:51:55

################################################################################
                     [1m Learning iteration 184/1400 [0m                      

                       Computation: 9607 steps/s (collection: 2.301s, learning 0.257s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 8.2431
                       Mean reward: 3.64
               Mean episode length: 397.99
Episode_Reward/track_lin_vel_xy_exp: 0.2295
Episode_Reward/track_ang_vel_z_exp: 0.1597
       Episode_Reward/lin_vel_z_l2: -0.0232
      Episode_Reward/ang_vel_xy_l2: -0.0328
     Episode_Reward/dof_torques_l2: -0.0350
         Episode_Reward/dof_acc_l2: -0.0667
     Episode_Reward/action_rate_l2: -0.0348
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1781
Metrics/base_velocity/error_vel_xy: 0.3357
Metrics/base_velocity/error_vel_yaw: 0.1885
      Episode_Termination/time_out: 0.2262
  Episode_Termination/base_contact: 0.7748
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 2.56s
                      Time elapsed: 00:07:53
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 185/1400 [0m                      

                       Computation: 9592 steps/s (collection: 2.306s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 8.2210
                       Mean reward: 3.65
               Mean episode length: 381.32
Episode_Reward/track_lin_vel_xy_exp: 0.2019
Episode_Reward/track_ang_vel_z_exp: 0.1478
       Episode_Reward/lin_vel_z_l2: -0.0206
      Episode_Reward/ang_vel_xy_l2: -0.0294
     Episode_Reward/dof_torques_l2: -0.0328
         Episode_Reward/dof_acc_l2: -0.0573
     Episode_Reward/action_rate_l2: -0.0311
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1828
Metrics/base_velocity/error_vel_xy: 0.3164
Metrics/base_velocity/error_vel_yaw: 0.1623
      Episode_Termination/time_out: 0.2245
  Episode_Termination/base_contact: 0.7765
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 2.56s
                      Time elapsed: 00:07:56
                               ETA: 00:51:49

################################################################################
                     [1m Learning iteration 186/1400 [0m                      

                       Computation: 9504 steps/s (collection: 2.329s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 8.2351
                       Mean reward: 3.76
               Mean episode length: 403.00
Episode_Reward/track_lin_vel_xy_exp: 0.2901
Episode_Reward/track_ang_vel_z_exp: 0.1930
       Episode_Reward/lin_vel_z_l2: -0.0295
      Episode_Reward/ang_vel_xy_l2: -0.0390
     Episode_Reward/dof_torques_l2: -0.0392
         Episode_Reward/dof_acc_l2: -0.0829
     Episode_Reward/action_rate_l2: -0.0420
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.1896
Metrics/base_velocity/error_vel_xy: 0.3777
Metrics/base_velocity/error_vel_yaw: 0.2262
      Episode_Termination/time_out: 0.2276
  Episode_Termination/base_contact: 0.7734
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 2.59s
                      Time elapsed: 00:07:58
                               ETA: 00:51:47

################################################################################
                     [1m Learning iteration 187/1400 [0m                      

                       Computation: 9601 steps/s (collection: 2.307s, learning 0.253s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 8.2413
                       Mean reward: 4.60
               Mean episode length: 456.86
Episode_Reward/track_lin_vel_xy_exp: 0.2791
Episode_Reward/track_ang_vel_z_exp: 0.1778
       Episode_Reward/lin_vel_z_l2: -0.0251
      Episode_Reward/ang_vel_xy_l2: -0.0372
     Episode_Reward/dof_torques_l2: -0.0377
         Episode_Reward/dof_acc_l2: -0.0705
     Episode_Reward/action_rate_l2: -0.0389
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2016
Metrics/base_velocity/error_vel_xy: 0.3322
Metrics/base_velocity/error_vel_yaw: 0.2089
      Episode_Termination/time_out: 0.2308
  Episode_Termination/base_contact: 0.7702
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.56s
                      Time elapsed: 00:08:01
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 188/1400 [0m                      

                       Computation: 9666 steps/s (collection: 2.286s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 8.2203
                       Mean reward: 4.37
               Mean episode length: 425.82
Episode_Reward/track_lin_vel_xy_exp: 0.2415
Episode_Reward/track_ang_vel_z_exp: 0.1563
       Episode_Reward/lin_vel_z_l2: -0.0253
      Episode_Reward/ang_vel_xy_l2: -0.0349
     Episode_Reward/dof_torques_l2: -0.0337
         Episode_Reward/dof_acc_l2: -0.0726
     Episode_Reward/action_rate_l2: -0.0348
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2027
Metrics/base_velocity/error_vel_xy: 0.3055
Metrics/base_velocity/error_vel_yaw: 0.1942
      Episode_Termination/time_out: 0.2333
  Episode_Termination/base_contact: 0.7677
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 2.54s
                      Time elapsed: 00:08:03
                               ETA: 00:51:42

################################################################################
                     [1m Learning iteration 189/1400 [0m                      

                       Computation: 9566 steps/s (collection: 2.306s, learning 0.263s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0127
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 8.2120
                       Mean reward: 4.64
               Mean episode length: 461.87
Episode_Reward/track_lin_vel_xy_exp: 0.3105
Episode_Reward/track_ang_vel_z_exp: 0.2004
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0394
     Episode_Reward/dof_torques_l2: -0.0428
         Episode_Reward/dof_acc_l2: -0.0888
     Episode_Reward/action_rate_l2: -0.0441
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2080
Metrics/base_velocity/error_vel_xy: 0.3800
Metrics/base_velocity/error_vel_yaw: 0.2348
      Episode_Termination/time_out: 0.2375
  Episode_Termination/base_contact: 0.7635
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 2.57s
                      Time elapsed: 00:08:06
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 190/1400 [0m                      

                       Computation: 9464 steps/s (collection: 2.341s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 8.2027
                       Mean reward: 5.00
               Mean episode length: 488.64
Episode_Reward/track_lin_vel_xy_exp: 0.3230
Episode_Reward/track_ang_vel_z_exp: 0.2107
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0406
     Episode_Reward/dof_torques_l2: -0.0441
         Episode_Reward/dof_acc_l2: -0.0881
     Episode_Reward/action_rate_l2: -0.0456
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2156
Metrics/base_velocity/error_vel_xy: 0.3904
Metrics/base_velocity/error_vel_yaw: 0.2378
      Episode_Termination/time_out: 0.2427
  Episode_Termination/base_contact: 0.7583
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 2.60s
                      Time elapsed: 00:08:08
                               ETA: 00:51:37

################################################################################
                     [1m Learning iteration 191/1400 [0m                      

                       Computation: 9556 steps/s (collection: 2.315s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 8.2007
                       Mean reward: 4.66
               Mean episode length: 455.57
Episode_Reward/track_lin_vel_xy_exp: 0.2440
Episode_Reward/track_ang_vel_z_exp: 0.1565
       Episode_Reward/lin_vel_z_l2: -0.0266
      Episode_Reward/ang_vel_xy_l2: -0.0345
     Episode_Reward/dof_torques_l2: -0.0331
         Episode_Reward/dof_acc_l2: -0.0712
     Episode_Reward/action_rate_l2: -0.0345
      Episode_Reward/feet_air_time: -0.0052
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2217
Metrics/base_velocity/error_vel_xy: 0.3073
Metrics/base_velocity/error_vel_yaw: 0.1910
      Episode_Termination/time_out: 0.2456
  Episode_Termination/base_contact: 0.7553
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 2.57s
                      Time elapsed: 00:08:11
                               ETA: 00:51:35

################################################################################
                     [1m Learning iteration 192/1400 [0m                      

                       Computation: 9572 steps/s (collection: 2.314s, learning 0.254s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 8.1951
                       Mean reward: 4.45
               Mean episode length: 441.95
Episode_Reward/track_lin_vel_xy_exp: 0.2329
Episode_Reward/track_ang_vel_z_exp: 0.1559
       Episode_Reward/lin_vel_z_l2: -0.0239
      Episode_Reward/ang_vel_xy_l2: -0.0322
     Episode_Reward/dof_torques_l2: -0.0349
         Episode_Reward/dof_acc_l2: -0.0668
     Episode_Reward/action_rate_l2: -0.0343
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2284
Metrics/base_velocity/error_vel_xy: 0.3212
Metrics/base_velocity/error_vel_yaw: 0.1874
      Episode_Termination/time_out: 0.2473
  Episode_Termination/base_contact: 0.7533
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 2.57s
                      Time elapsed: 00:08:14
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 193/1400 [0m                      

                       Computation: 9766 steps/s (collection: 2.262s, learning 0.254s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 8.1769
                       Mean reward: 4.07
               Mean episode length: 424.48
Episode_Reward/track_lin_vel_xy_exp: 0.2329
Episode_Reward/track_ang_vel_z_exp: 0.1498
       Episode_Reward/lin_vel_z_l2: -0.0264
      Episode_Reward/ang_vel_xy_l2: -0.0334
     Episode_Reward/dof_torques_l2: -0.0336
         Episode_Reward/dof_acc_l2: -0.0703
     Episode_Reward/action_rate_l2: -0.0341
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2323
Metrics/base_velocity/error_vel_xy: 0.3178
Metrics/base_velocity/error_vel_yaw: 0.2038
      Episode_Termination/time_out: 0.2520
  Episode_Termination/base_contact: 0.7480
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 2.52s
                      Time elapsed: 00:08:16
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 194/1400 [0m                      

                       Computation: 9760 steps/s (collection: 2.270s, learning 0.248s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 8.1520
                       Mean reward: 4.55
               Mean episode length: 432.51
Episode_Reward/track_lin_vel_xy_exp: 0.2678
Episode_Reward/track_ang_vel_z_exp: 0.1673
       Episode_Reward/lin_vel_z_l2: -0.0257
      Episode_Reward/ang_vel_xy_l2: -0.0339
     Episode_Reward/dof_torques_l2: -0.0369
         Episode_Reward/dof_acc_l2: -0.0736
     Episode_Reward/action_rate_l2: -0.0368
      Episode_Reward/feet_air_time: -0.0051
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2373
Metrics/base_velocity/error_vel_xy: 0.3112
Metrics/base_velocity/error_vel_yaw: 0.2020
      Episode_Termination/time_out: 0.2518
  Episode_Termination/base_contact: 0.7482
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 2.52s
                      Time elapsed: 00:08:19
                               ETA: 00:51:26

################################################################################
                     [1m Learning iteration 195/1400 [0m                      

                       Computation: 9768 steps/s (collection: 2.259s, learning 0.257s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 8.1433
                       Mean reward: 5.25
               Mean episode length: 464.80
Episode_Reward/track_lin_vel_xy_exp: 0.3425
Episode_Reward/track_ang_vel_z_exp: 0.2049
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0396
     Episode_Reward/dof_torques_l2: -0.0431
         Episode_Reward/dof_acc_l2: -0.0829
     Episode_Reward/action_rate_l2: -0.0441
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2463
Metrics/base_velocity/error_vel_xy: 0.3432
Metrics/base_velocity/error_vel_yaw: 0.2337
      Episode_Termination/time_out: 0.2497
  Episode_Termination/base_contact: 0.7503
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.52s
                      Time elapsed: 00:08:21
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 196/1400 [0m                      

                       Computation: 9827 steps/s (collection: 2.237s, learning 0.264s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 8.1143
                       Mean reward: 5.49
               Mean episode length: 530.42
Episode_Reward/track_lin_vel_xy_exp: 0.3254
Episode_Reward/track_ang_vel_z_exp: 0.2104
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0426
     Episode_Reward/dof_torques_l2: -0.0437
         Episode_Reward/dof_acc_l2: -0.0926
     Episode_Reward/action_rate_l2: -0.0457
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2575
Metrics/base_velocity/error_vel_xy: 0.3971
Metrics/base_velocity/error_vel_yaw: 0.2471
      Episode_Termination/time_out: 0.2512
  Episode_Termination/base_contact: 0.7488
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 2.50s
                      Time elapsed: 00:08:24
                               ETA: 00:51:21

################################################################################
                     [1m Learning iteration 197/1400 [0m                      

                       Computation: 9890 steps/s (collection: 2.226s, learning 0.259s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 8.1030
                       Mean reward: 5.65
               Mean episode length: 557.07
Episode_Reward/track_lin_vel_xy_exp: 0.3635
Episode_Reward/track_ang_vel_z_exp: 0.2288
       Episode_Reward/lin_vel_z_l2: -0.0319
      Episode_Reward/ang_vel_xy_l2: -0.0447
     Episode_Reward/dof_torques_l2: -0.0479
         Episode_Reward/dof_acc_l2: -0.0973
     Episode_Reward/action_rate_l2: -0.0490
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2670
Metrics/base_velocity/error_vel_xy: 0.4074
Metrics/base_velocity/error_vel_yaw: 0.2472
      Episode_Termination/time_out: 0.2586
  Episode_Termination/base_contact: 0.7414
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 2.48s
                      Time elapsed: 00:08:26
                               ETA: 00:51:18

################################################################################
                     [1m Learning iteration 198/1400 [0m                      

                       Computation: 9834 steps/s (collection: 2.243s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 8.0911
                       Mean reward: 5.37
               Mean episode length: 525.35
Episode_Reward/track_lin_vel_xy_exp: 0.3215
Episode_Reward/track_ang_vel_z_exp: 0.2048
       Episode_Reward/lin_vel_z_l2: -0.0316
      Episode_Reward/ang_vel_xy_l2: -0.0431
     Episode_Reward/dof_torques_l2: -0.0451
         Episode_Reward/dof_acc_l2: -0.0993
     Episode_Reward/action_rate_l2: -0.0456
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2720
Metrics/base_velocity/error_vel_xy: 0.4093
Metrics/base_velocity/error_vel_yaw: 0.2606
      Episode_Termination/time_out: 0.2643
  Episode_Termination/base_contact: 0.7357
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 2.50s
                      Time elapsed: 00:08:29
                               ETA: 00:51:15

################################################################################
                     [1m Learning iteration 199/1400 [0m                      

                       Computation: 9849 steps/s (collection: 2.239s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 8.0791
                       Mean reward: 4.83
               Mean episode length: 482.14
Episode_Reward/track_lin_vel_xy_exp: 0.2569
Episode_Reward/track_ang_vel_z_exp: 0.1607
       Episode_Reward/lin_vel_z_l2: -0.0276
      Episode_Reward/ang_vel_xy_l2: -0.0349
     Episode_Reward/dof_torques_l2: -0.0351
         Episode_Reward/dof_acc_l2: -0.0743
     Episode_Reward/action_rate_l2: -0.0353
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2793
Metrics/base_velocity/error_vel_xy: 0.3076
Metrics/base_velocity/error_vel_yaw: 0.2011
      Episode_Termination/time_out: 0.2667
  Episode_Termination/base_contact: 0.7333
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 2.50s
                      Time elapsed: 00:08:31
                               ETA: 00:51:12

################################################################################
                     [1m Learning iteration 200/1400 [0m                      

                       Computation: 9720 steps/s (collection: 2.275s, learning 0.254s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 8.0668
                       Mean reward: 4.26
               Mean episode length: 448.18
Episode_Reward/track_lin_vel_xy_exp: 0.2979
Episode_Reward/track_ang_vel_z_exp: 0.1875
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0399
     Episode_Reward/dof_torques_l2: -0.0418
         Episode_Reward/dof_acc_l2: -0.0866
     Episode_Reward/action_rate_l2: -0.0411
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2864
Metrics/base_velocity/error_vel_xy: 0.3683
Metrics/base_velocity/error_vel_yaw: 0.2377
      Episode_Termination/time_out: 0.2658
  Episode_Termination/base_contact: 0.7342
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 2.53s
                      Time elapsed: 00:08:34
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 201/1400 [0m                      

                       Computation: 9761 steps/s (collection: 2.264s, learning 0.253s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 8.0784
                       Mean reward: 4.37
               Mean episode length: 450.79
Episode_Reward/track_lin_vel_xy_exp: 0.3346
Episode_Reward/track_ang_vel_z_exp: 0.2013
       Episode_Reward/lin_vel_z_l2: -0.0297
      Episode_Reward/ang_vel_xy_l2: -0.0411
     Episode_Reward/dof_torques_l2: -0.0418
         Episode_Reward/dof_acc_l2: -0.0850
     Episode_Reward/action_rate_l2: -0.0433
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2902
Metrics/base_velocity/error_vel_xy: 0.3424
Metrics/base_velocity/error_vel_yaw: 0.2306
      Episode_Termination/time_out: 0.2673
  Episode_Termination/base_contact: 0.7327
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 2.52s
                      Time elapsed: 00:08:36
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 202/1400 [0m                      

                       Computation: 9635 steps/s (collection: 2.294s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 8.0724
                       Mean reward: 4.97
               Mean episode length: 463.99
Episode_Reward/track_lin_vel_xy_exp: 0.2923
Episode_Reward/track_ang_vel_z_exp: 0.1771
       Episode_Reward/lin_vel_z_l2: -0.0283
      Episode_Reward/ang_vel_xy_l2: -0.0359
     Episode_Reward/dof_torques_l2: -0.0388
         Episode_Reward/dof_acc_l2: -0.0811
     Episode_Reward/action_rate_l2: -0.0380
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2987
Metrics/base_velocity/error_vel_xy: 0.3106
Metrics/base_velocity/error_vel_yaw: 0.2031
      Episode_Termination/time_out: 0.2719
  Episode_Termination/base_contact: 0.7281
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 2.55s
                      Time elapsed: 00:08:39
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 203/1400 [0m                      

                       Computation: 9636 steps/s (collection: 2.294s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 8.0718
                       Mean reward: 4.13
               Mean episode length: 418.45
Episode_Reward/track_lin_vel_xy_exp: 0.2699
Episode_Reward/track_ang_vel_z_exp: 0.1668
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0377
     Episode_Reward/dof_torques_l2: -0.0365
         Episode_Reward/dof_acc_l2: -0.0823
     Episode_Reward/action_rate_l2: -0.0368
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2980
Metrics/base_velocity/error_vel_xy: 0.3147
Metrics/base_velocity/error_vel_yaw: 0.2041
      Episode_Termination/time_out: 0.2725
  Episode_Termination/base_contact: 0.7275
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 2.55s
                      Time elapsed: 00:08:41
                               ETA: 00:51:01

################################################################################
                     [1m Learning iteration 204/1400 [0m                      

                       Computation: 9646 steps/s (collection: 2.292s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 8.0770
                       Mean reward: 3.63
               Mean episode length: 408.34
Episode_Reward/track_lin_vel_xy_exp: 0.2473
Episode_Reward/track_ang_vel_z_exp: 0.1649
       Episode_Reward/lin_vel_z_l2: -0.0294
      Episode_Reward/ang_vel_xy_l2: -0.0363
     Episode_Reward/dof_torques_l2: -0.0377
         Episode_Reward/dof_acc_l2: -0.0760
     Episode_Reward/action_rate_l2: -0.0366
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3046
Metrics/base_velocity/error_vel_xy: 0.3537
Metrics/base_velocity/error_vel_yaw: 0.2118
      Episode_Termination/time_out: 0.2712
  Episode_Termination/base_contact: 0.7288
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 2.55s
                      Time elapsed: 00:08:44
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 205/1400 [0m                      

                       Computation: 9674 steps/s (collection: 2.279s, learning 0.262s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 8.0668
                       Mean reward: 4.33
               Mean episode length: 448.42
Episode_Reward/track_lin_vel_xy_exp: 0.2920
Episode_Reward/track_ang_vel_z_exp: 0.1821
       Episode_Reward/lin_vel_z_l2: -0.0294
      Episode_Reward/ang_vel_xy_l2: -0.0397
     Episode_Reward/dof_torques_l2: -0.0394
         Episode_Reward/dof_acc_l2: -0.0875
     Episode_Reward/action_rate_l2: -0.0397
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3132
Metrics/base_velocity/error_vel_xy: 0.3405
Metrics/base_velocity/error_vel_yaw: 0.2175
      Episode_Termination/time_out: 0.2699
  Episode_Termination/base_contact: 0.7301
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 2.54s
                      Time elapsed: 00:08:46
                               ETA: 00:50:56

################################################################################
                     [1m Learning iteration 206/1400 [0m                      

                       Computation: 9709 steps/s (collection: 2.275s, learning 0.256s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 8.0512
                       Mean reward: 4.32
               Mean episode length: 451.63
Episode_Reward/track_lin_vel_xy_exp: 0.2150
Episode_Reward/track_ang_vel_z_exp: 0.1448
       Episode_Reward/lin_vel_z_l2: -0.0238
      Episode_Reward/ang_vel_xy_l2: -0.0308
     Episode_Reward/dof_torques_l2: -0.0325
         Episode_Reward/dof_acc_l2: -0.0699
     Episode_Reward/action_rate_l2: -0.0319
      Episode_Reward/feet_air_time: -0.0051
 Episode_Reward/undesired_contacts: -0.0069
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3155
Metrics/base_velocity/error_vel_xy: 0.3041
Metrics/base_velocity/error_vel_yaw: 0.1805
      Episode_Termination/time_out: 0.2678
  Episode_Termination/base_contact: 0.7322
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 2.53s
                      Time elapsed: 00:08:49
                               ETA: 00:50:53

################################################################################
                     [1m Learning iteration 207/1400 [0m                      

                       Computation: 9639 steps/s (collection: 2.293s, learning 0.256s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 8.0558
                       Mean reward: 2.93
               Mean episode length: 355.58
Episode_Reward/track_lin_vel_xy_exp: 0.1992
Episode_Reward/track_ang_vel_z_exp: 0.1280
       Episode_Reward/lin_vel_z_l2: -0.0234
      Episode_Reward/ang_vel_xy_l2: -0.0288
     Episode_Reward/dof_torques_l2: -0.0298
         Episode_Reward/dof_acc_l2: -0.0688
     Episode_Reward/action_rate_l2: -0.0292
      Episode_Reward/feet_air_time: -0.0051
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3164
Metrics/base_velocity/error_vel_xy: 0.2709
Metrics/base_velocity/error_vel_yaw: 0.1741
      Episode_Termination/time_out: 0.2615
  Episode_Termination/base_contact: 0.7385
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 2.55s
                      Time elapsed: 00:08:51
                               ETA: 00:50:50

################################################################################
                     [1m Learning iteration 208/1400 [0m                      

                       Computation: 9681 steps/s (collection: 2.285s, learning 0.253s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 8.0593
                       Mean reward: 3.55
               Mean episode length: 383.43
Episode_Reward/track_lin_vel_xy_exp: 0.2316
Episode_Reward/track_ang_vel_z_exp: 0.1475
       Episode_Reward/lin_vel_z_l2: -0.0241
      Episode_Reward/ang_vel_xy_l2: -0.0332
     Episode_Reward/dof_torques_l2: -0.0322
         Episode_Reward/dof_acc_l2: -0.0671
     Episode_Reward/action_rate_l2: -0.0324
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3216
Metrics/base_velocity/error_vel_xy: 0.2884
Metrics/base_velocity/error_vel_yaw: 0.1808
      Episode_Termination/time_out: 0.2593
  Episode_Termination/base_contact: 0.7407
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 2.54s
                      Time elapsed: 00:08:54
                               ETA: 00:50:48

################################################################################
                     [1m Learning iteration 209/1400 [0m                      

                       Computation: 9733 steps/s (collection: 2.260s, learning 0.265s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 8.0474
                       Mean reward: 3.95
               Mean episode length: 400.87
Episode_Reward/track_lin_vel_xy_exp: 0.2759
Episode_Reward/track_ang_vel_z_exp: 0.1673
       Episode_Reward/lin_vel_z_l2: -0.0274
      Episode_Reward/ang_vel_xy_l2: -0.0359
     Episode_Reward/dof_torques_l2: -0.0372
         Episode_Reward/dof_acc_l2: -0.0758
     Episode_Reward/action_rate_l2: -0.0360
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3262
Metrics/base_velocity/error_vel_xy: 0.2910
Metrics/base_velocity/error_vel_yaw: 0.1887
      Episode_Termination/time_out: 0.2585
  Episode_Termination/base_contact: 0.7415
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 2.52s
                      Time elapsed: 00:08:56
                               ETA: 00:50:45

################################################################################
                     [1m Learning iteration 210/1400 [0m                      

                       Computation: 9834 steps/s (collection: 2.244s, learning 0.255s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 8.0342
                       Mean reward: 4.66
               Mean episode length: 454.14
Episode_Reward/track_lin_vel_xy_exp: 0.3285
Episode_Reward/track_ang_vel_z_exp: 0.2096
       Episode_Reward/lin_vel_z_l2: -0.0321
      Episode_Reward/ang_vel_xy_l2: -0.0420
     Episode_Reward/dof_torques_l2: -0.0444
         Episode_Reward/dof_acc_l2: -0.0881
     Episode_Reward/action_rate_l2: -0.0448
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3327
Metrics/base_velocity/error_vel_xy: 0.3787
Metrics/base_velocity/error_vel_yaw: 0.2290
      Episode_Termination/time_out: 0.2625
  Episode_Termination/base_contact: 0.7375
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 2.50s
                      Time elapsed: 00:08:59
                               ETA: 00:50:42

################################################################################
                     [1m Learning iteration 211/1400 [0m                      

                       Computation: 9664 steps/s (collection: 2.288s, learning 0.255s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 8.0169
                       Mean reward: 5.00
               Mean episode length: 477.61
Episode_Reward/track_lin_vel_xy_exp: 0.3562
Episode_Reward/track_ang_vel_z_exp: 0.2121
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0434
     Episode_Reward/dof_torques_l2: -0.0438
         Episode_Reward/dof_acc_l2: -0.0981
     Episode_Reward/action_rate_l2: -0.0454
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3334
Metrics/base_velocity/error_vel_xy: 0.3466
Metrics/base_velocity/error_vel_yaw: 0.2367
      Episode_Termination/time_out: 0.2601
  Episode_Termination/base_contact: 0.7399
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.54s
                      Time elapsed: 00:09:02
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 212/1400 [0m                      

                       Computation: 9646 steps/s (collection: 2.291s, learning 0.256s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 8.0086
                       Mean reward: 4.32
               Mean episode length: 408.94
Episode_Reward/track_lin_vel_xy_exp: 0.2647
Episode_Reward/track_ang_vel_z_exp: 0.1542
       Episode_Reward/lin_vel_z_l2: -0.0255
      Episode_Reward/ang_vel_xy_l2: -0.0321
     Episode_Reward/dof_torques_l2: -0.0329
         Episode_Reward/dof_acc_l2: -0.0689
     Episode_Reward/action_rate_l2: -0.0329
      Episode_Reward/feet_air_time: -0.0047
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3311
Metrics/base_velocity/error_vel_xy: 0.2509
Metrics/base_velocity/error_vel_yaw: 0.1746
      Episode_Termination/time_out: 0.2593
  Episode_Termination/base_contact: 0.7407
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 2.55s
                      Time elapsed: 00:09:04
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 213/1400 [0m                      

                       Computation: 9675 steps/s (collection: 2.285s, learning 0.255s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 8.0123
                       Mean reward: 4.78
               Mean episode length: 443.24
Episode_Reward/track_lin_vel_xy_exp: 0.3277
Episode_Reward/track_ang_vel_z_exp: 0.1922
       Episode_Reward/lin_vel_z_l2: -0.0302
      Episode_Reward/ang_vel_xy_l2: -0.0390
     Episode_Reward/dof_torques_l2: -0.0421
         Episode_Reward/dof_acc_l2: -0.0883
     Episode_Reward/action_rate_l2: -0.0410
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3403
Metrics/base_velocity/error_vel_xy: 0.3178
Metrics/base_velocity/error_vel_yaw: 0.2262
      Episode_Termination/time_out: 0.2615
  Episode_Termination/base_contact: 0.7385
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 2.54s
                      Time elapsed: 00:09:07
                               ETA: 00:50:34

################################################################################
                     [1m Learning iteration 214/1400 [0m                      

                       Computation: 9638 steps/s (collection: 2.296s, learning 0.254s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 8.0077
                       Mean reward: 4.49
               Mean episode length: 430.01
Episode_Reward/track_lin_vel_xy_exp: 0.2230
Episode_Reward/track_ang_vel_z_exp: 0.1383
       Episode_Reward/lin_vel_z_l2: -0.0258
      Episode_Reward/ang_vel_xy_l2: -0.0313
     Episode_Reward/dof_torques_l2: -0.0309
         Episode_Reward/dof_acc_l2: -0.0703
     Episode_Reward/action_rate_l2: -0.0307
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3470
Metrics/base_velocity/error_vel_xy: 0.2680
Metrics/base_velocity/error_vel_yaw: 0.1763
      Episode_Termination/time_out: 0.2627
  Episode_Termination/base_contact: 0.7373
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 2.55s
                      Time elapsed: 00:09:09
                               ETA: 00:50:32

################################################################################
                     [1m Learning iteration 215/1400 [0m                      

                       Computation: 9626 steps/s (collection: 2.297s, learning 0.256s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 8.0153
                       Mean reward: 3.99
               Mean episode length: 399.84
Episode_Reward/track_lin_vel_xy_exp: 0.2630
Episode_Reward/track_ang_vel_z_exp: 0.1563
       Episode_Reward/lin_vel_z_l2: -0.0243
      Episode_Reward/ang_vel_xy_l2: -0.0315
     Episode_Reward/dof_torques_l2: -0.0364
         Episode_Reward/dof_acc_l2: -0.0736
     Episode_Reward/action_rate_l2: -0.0345
      Episode_Reward/feet_air_time: -0.0048
 Episode_Reward/undesired_contacts: -0.0069
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3556
Metrics/base_velocity/error_vel_xy: 0.2955
Metrics/base_velocity/error_vel_yaw: 0.2121
      Episode_Termination/time_out: 0.2598
  Episode_Termination/base_contact: 0.7402
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.55s
                      Time elapsed: 00:09:12
                               ETA: 00:50:29

################################################################################
                     [1m Learning iteration 216/1400 [0m                      

                       Computation: 9710 steps/s (collection: 2.279s, learning 0.252s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 8.0021
                       Mean reward: 4.81
               Mean episode length: 450.49
Episode_Reward/track_lin_vel_xy_exp: 0.3459
Episode_Reward/track_ang_vel_z_exp: 0.1972
       Episode_Reward/lin_vel_z_l2: -0.0296
      Episode_Reward/ang_vel_xy_l2: -0.0403
     Episode_Reward/dof_torques_l2: -0.0419
         Episode_Reward/dof_acc_l2: -0.0854
     Episode_Reward/action_rate_l2: -0.0421
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3638
Metrics/base_velocity/error_vel_xy: 0.3062
Metrics/base_velocity/error_vel_yaw: 0.2242
      Episode_Termination/time_out: 0.2618
  Episode_Termination/base_contact: 0.7382
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 2.53s
                      Time elapsed: 00:09:14
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 217/1400 [0m                      

                       Computation: 9753 steps/s (collection: 2.257s, learning 0.263s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 7.9862
                       Mean reward: 5.16
               Mean episode length: 465.49
Episode_Reward/track_lin_vel_xy_exp: 0.2705
Episode_Reward/track_ang_vel_z_exp: 0.1676
       Episode_Reward/lin_vel_z_l2: -0.0271
      Episode_Reward/ang_vel_xy_l2: -0.0342
     Episode_Reward/dof_torques_l2: -0.0353
         Episode_Reward/dof_acc_l2: -0.0735
     Episode_Reward/action_rate_l2: -0.0357
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3687
Metrics/base_velocity/error_vel_xy: 0.2998
Metrics/base_velocity/error_vel_yaw: 0.1926
      Episode_Termination/time_out: 0.2624
  Episode_Termination/base_contact: 0.7376
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 2.52s
                      Time elapsed: 00:09:17
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 218/1400 [0m                      

                       Computation: 9757 steps/s (collection: 2.263s, learning 0.256s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 7.9768
                       Mean reward: 5.40
               Mean episode length: 511.36
Episode_Reward/track_lin_vel_xy_exp: 0.3538
Episode_Reward/track_ang_vel_z_exp: 0.2231
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0445
     Episode_Reward/dof_torques_l2: -0.0474
         Episode_Reward/dof_acc_l2: -0.0977
     Episode_Reward/action_rate_l2: -0.0469
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3739
Metrics/base_velocity/error_vel_xy: 0.4140
Metrics/base_velocity/error_vel_yaw: 0.2488
      Episode_Termination/time_out: 0.2673
  Episode_Termination/base_contact: 0.7327
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 2.52s
                      Time elapsed: 00:09:19
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 219/1400 [0m                      

                       Computation: 9753 steps/s (collection: 2.265s, learning 0.255s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 7.9576
                       Mean reward: 5.36
               Mean episode length: 513.39
Episode_Reward/track_lin_vel_xy_exp: 0.3438
Episode_Reward/track_ang_vel_z_exp: 0.1985
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0398
     Episode_Reward/dof_torques_l2: -0.0436
         Episode_Reward/dof_acc_l2: -0.0889
     Episode_Reward/action_rate_l2: -0.0429
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3838
Metrics/base_velocity/error_vel_xy: 0.3253
Metrics/base_velocity/error_vel_yaw: 0.2319
      Episode_Termination/time_out: 0.2726
  Episode_Termination/base_contact: 0.7274
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.52s
                      Time elapsed: 00:09:22
                               ETA: 00:50:18

################################################################################
                     [1m Learning iteration 220/1400 [0m                      

                       Computation: 9786 steps/s (collection: 2.255s, learning 0.256s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 7.9522
                       Mean reward: 4.88
               Mean episode length: 454.81
Episode_Reward/track_lin_vel_xy_exp: 0.2468
Episode_Reward/track_ang_vel_z_exp: 0.1462
       Episode_Reward/lin_vel_z_l2: -0.0262
      Episode_Reward/ang_vel_xy_l2: -0.0311
     Episode_Reward/dof_torques_l2: -0.0334
         Episode_Reward/dof_acc_l2: -0.0702
     Episode_Reward/action_rate_l2: -0.0322
      Episode_Reward/feet_air_time: -0.0045
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3936
Metrics/base_velocity/error_vel_xy: 0.2633
Metrics/base_velocity/error_vel_yaw: 0.1884
      Episode_Termination/time_out: 0.2736
  Episode_Termination/base_contact: 0.7264
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 2.51s
                      Time elapsed: 00:09:24
                               ETA: 00:50:15

################################################################################
                     [1m Learning iteration 221/1400 [0m                      

                       Computation: 9622 steps/s (collection: 2.297s, learning 0.257s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 7.9491
                       Mean reward: 4.87
               Mean episode length: 448.00
Episode_Reward/track_lin_vel_xy_exp: 0.2907
Episode_Reward/track_ang_vel_z_exp: 0.1708
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0360
     Episode_Reward/dof_torques_l2: -0.0369
         Episode_Reward/dof_acc_l2: -0.0798
     Episode_Reward/action_rate_l2: -0.0367
      Episode_Reward/feet_air_time: -0.0052
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4062
Metrics/base_velocity/error_vel_xy: 0.2883
Metrics/base_velocity/error_vel_yaw: 0.2026
      Episode_Termination/time_out: 0.2766
  Episode_Termination/base_contact: 0.7234
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 2.55s
                      Time elapsed: 00:09:27
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 222/1400 [0m                      

                       Computation: 9836 steps/s (collection: 2.242s, learning 0.257s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 7.9378
                       Mean reward: 5.06
               Mean episode length: 459.78
Episode_Reward/track_lin_vel_xy_exp: 0.3001
Episode_Reward/track_ang_vel_z_exp: 0.1789
       Episode_Reward/lin_vel_z_l2: -0.0289
      Episode_Reward/ang_vel_xy_l2: -0.0365
     Episode_Reward/dof_torques_l2: -0.0389
         Episode_Reward/dof_acc_l2: -0.0812
     Episode_Reward/action_rate_l2: -0.0383
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4114
Metrics/base_velocity/error_vel_xy: 0.3105
Metrics/base_velocity/error_vel_yaw: 0.2112
      Episode_Termination/time_out: 0.2773
  Episode_Termination/base_contact: 0.7227
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 2.50s
                      Time elapsed: 00:09:29
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 223/1400 [0m                      

                       Computation: 9710 steps/s (collection: 2.280s, learning 0.251s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.9327
                       Mean reward: 4.76
               Mean episode length: 453.54
Episode_Reward/track_lin_vel_xy_exp: 0.2622
Episode_Reward/track_ang_vel_z_exp: 0.1665
       Episode_Reward/lin_vel_z_l2: -0.0268
      Episode_Reward/ang_vel_xy_l2: -0.0358
     Episode_Reward/dof_torques_l2: -0.0385
         Episode_Reward/dof_acc_l2: -0.0788
     Episode_Reward/action_rate_l2: -0.0363
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4130
Metrics/base_velocity/error_vel_xy: 0.3339
Metrics/base_velocity/error_vel_yaw: 0.2105
      Episode_Termination/time_out: 0.2767
  Episode_Termination/base_contact: 0.7233
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.53s
                      Time elapsed: 00:09:32
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 224/1400 [0m                      

                       Computation: 9669 steps/s (collection: 2.280s, learning 0.261s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 7.9310
                       Mean reward: 4.60
               Mean episode length: 450.84
Episode_Reward/track_lin_vel_xy_exp: 0.2496
Episode_Reward/track_ang_vel_z_exp: 0.1557
       Episode_Reward/lin_vel_z_l2: -0.0281
      Episode_Reward/ang_vel_xy_l2: -0.0334
     Episode_Reward/dof_torques_l2: -0.0346
         Episode_Reward/dof_acc_l2: -0.0793
     Episode_Reward/action_rate_l2: -0.0337
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4172
Metrics/base_velocity/error_vel_xy: 0.2911
Metrics/base_velocity/error_vel_yaw: 0.1855
      Episode_Termination/time_out: 0.2743
  Episode_Termination/base_contact: 0.7257
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 2.54s
                      Time elapsed: 00:09:34
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 225/1400 [0m                      

                       Computation: 9705 steps/s (collection: 2.277s, learning 0.255s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 7.9284
                       Mean reward: 5.44
               Mean episode length: 490.43
Episode_Reward/track_lin_vel_xy_exp: 0.3527
Episode_Reward/track_ang_vel_z_exp: 0.2006
       Episode_Reward/lin_vel_z_l2: -0.0314
      Episode_Reward/ang_vel_xy_l2: -0.0396
     Episode_Reward/dof_torques_l2: -0.0424
         Episode_Reward/dof_acc_l2: -0.0892
     Episode_Reward/action_rate_l2: -0.0423
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4279
Metrics/base_velocity/error_vel_xy: 0.3026
Metrics/base_velocity/error_vel_yaw: 0.2192
      Episode_Termination/time_out: 0.2796
  Episode_Termination/base_contact: 0.7204
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 2.53s
                      Time elapsed: 00:09:37
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 226/1400 [0m                      

                       Computation: 9712 steps/s (collection: 2.280s, learning 0.251s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 7.9080
                       Mean reward: 5.58
               Mean episode length: 505.32
Episode_Reward/track_lin_vel_xy_exp: 0.3428
Episode_Reward/track_ang_vel_z_exp: 0.2009
       Episode_Reward/lin_vel_z_l2: -0.0334
      Episode_Reward/ang_vel_xy_l2: -0.0401
     Episode_Reward/dof_torques_l2: -0.0424
         Episode_Reward/dof_acc_l2: -0.0904
     Episode_Reward/action_rate_l2: -0.0425
      Episode_Reward/feet_air_time: -0.0050
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4390
Metrics/base_velocity/error_vel_xy: 0.3229
Metrics/base_velocity/error_vel_yaw: 0.2193
      Episode_Termination/time_out: 0.2878
  Episode_Termination/base_contact: 0.7122
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 2.53s
                      Time elapsed: 00:09:40
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 227/1400 [0m                      

                       Computation: 9791 steps/s (collection: 2.257s, learning 0.253s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 7.8988
                       Mean reward: 5.38
               Mean episode length: 481.57
Episode_Reward/track_lin_vel_xy_exp: 0.3437
Episode_Reward/track_ang_vel_z_exp: 0.2000
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.0427
         Episode_Reward/dof_acc_l2: -0.0884
     Episode_Reward/action_rate_l2: -0.0423
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4464
Metrics/base_velocity/error_vel_xy: 0.3116
Metrics/base_velocity/error_vel_yaw: 0.2172
      Episode_Termination/time_out: 0.2902
  Episode_Termination/base_contact: 0.7098
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.51s
                      Time elapsed: 00:09:42
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 228/1400 [0m                      

                       Computation: 9670 steps/s (collection: 2.286s, learning 0.255s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 7.9025
                       Mean reward: 5.58
               Mean episode length: 506.84
Episode_Reward/track_lin_vel_xy_exp: 0.3813
Episode_Reward/track_ang_vel_z_exp: 0.2256
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.1007
     Episode_Reward/action_rate_l2: -0.0477
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4593
Metrics/base_velocity/error_vel_xy: 0.3667
Metrics/base_velocity/error_vel_yaw: 0.2458
      Episode_Termination/time_out: 0.2902
  Episode_Termination/base_contact: 0.7098
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 2.54s
                      Time elapsed: 00:09:45
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 229/1400 [0m                      

                       Computation: 9772 steps/s (collection: 2.250s, learning 0.264s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 7.8974
                       Mean reward: 5.09
               Mean episode length: 473.77
Episode_Reward/track_lin_vel_xy_exp: 0.3203
Episode_Reward/track_ang_vel_z_exp: 0.1929
       Episode_Reward/lin_vel_z_l2: -0.0318
      Episode_Reward/ang_vel_xy_l2: -0.0386
     Episode_Reward/dof_torques_l2: -0.0403
         Episode_Reward/dof_acc_l2: -0.0872
     Episode_Reward/action_rate_l2: -0.0406
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4670
Metrics/base_velocity/error_vel_xy: 0.3255
Metrics/base_velocity/error_vel_yaw: 0.2152
      Episode_Termination/time_out: 0.2849
  Episode_Termination/base_contact: 0.7151
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 2.51s
                      Time elapsed: 00:09:47
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 230/1400 [0m                      

                       Computation: 9743 steps/s (collection: 2.268s, learning 0.254s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 7.8819
                       Mean reward: 4.21
               Mean episode length: 418.30
Episode_Reward/track_lin_vel_xy_exp: 0.3205
Episode_Reward/track_ang_vel_z_exp: 0.1927
       Episode_Reward/lin_vel_z_l2: -0.0319
      Episode_Reward/ang_vel_xy_l2: -0.0389
     Episode_Reward/dof_torques_l2: -0.0414
         Episode_Reward/dof_acc_l2: -0.0889
     Episode_Reward/action_rate_l2: -0.0412
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4786
Metrics/base_velocity/error_vel_xy: 0.3258
Metrics/base_velocity/error_vel_yaw: 0.2183
      Episode_Termination/time_out: 0.2796
  Episode_Termination/base_contact: 0.7204
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 2.52s
                      Time elapsed: 00:09:50
                               ETA: 00:49:48

################################################################################
                     [1m Learning iteration 231/1400 [0m                      

                       Computation: 9813 steps/s (collection: 2.242s, learning 0.263s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.8564
                       Mean reward: 4.68
               Mean episode length: 453.04
Episode_Reward/track_lin_vel_xy_exp: 0.3176
Episode_Reward/track_ang_vel_z_exp: 0.1972
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0421
     Episode_Reward/dof_torques_l2: -0.0424
         Episode_Reward/dof_acc_l2: -0.0847
     Episode_Reward/action_rate_l2: -0.0421
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4808
Metrics/base_velocity/error_vel_xy: 0.3489
Metrics/base_velocity/error_vel_yaw: 0.2236
      Episode_Termination/time_out: 0.2771
  Episode_Termination/base_contact: 0.7229
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 2.50s
                      Time elapsed: 00:09:52
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 232/1400 [0m                      

                       Computation: 9625 steps/s (collection: 2.298s, learning 0.255s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 7.8570
                       Mean reward: 5.14
               Mean episode length: 479.40
Episode_Reward/track_lin_vel_xy_exp: 0.3120
Episode_Reward/track_ang_vel_z_exp: 0.1849
       Episode_Reward/lin_vel_z_l2: -0.0297
      Episode_Reward/ang_vel_xy_l2: -0.0371
     Episode_Reward/dof_torques_l2: -0.0393
         Episode_Reward/dof_acc_l2: -0.0820
     Episode_Reward/action_rate_l2: -0.0391
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4904
Metrics/base_velocity/error_vel_xy: 0.3053
Metrics/base_velocity/error_vel_yaw: 0.2098
      Episode_Termination/time_out: 0.2775
  Episode_Termination/base_contact: 0.7225
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 2.55s
                      Time elapsed: 00:09:55
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 233/1400 [0m                      

                       Computation: 9724 steps/s (collection: 2.271s, learning 0.257s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 7.8657
                       Mean reward: 5.07
               Mean episode length: 468.69
Episode_Reward/track_lin_vel_xy_exp: 0.2988
Episode_Reward/track_ang_vel_z_exp: 0.1823
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0368
     Episode_Reward/dof_torques_l2: -0.0389
         Episode_Reward/dof_acc_l2: -0.0837
     Episode_Reward/action_rate_l2: -0.0387
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4992
Metrics/base_velocity/error_vel_xy: 0.3149
Metrics/base_velocity/error_vel_yaw: 0.2058
      Episode_Termination/time_out: 0.2786
  Episode_Termination/base_contact: 0.7214
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 2.53s
                      Time elapsed: 00:09:57
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 234/1400 [0m                      

                       Computation: 9699 steps/s (collection: 2.272s, learning 0.262s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.8518
                       Mean reward: 5.38
               Mean episode length: 479.04
Episode_Reward/track_lin_vel_xy_exp: 0.3199
Episode_Reward/track_ang_vel_z_exp: 0.1946
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0365
     Episode_Reward/dof_torques_l2: -0.0409
         Episode_Reward/dof_acc_l2: -0.0836
     Episode_Reward/action_rate_l2: -0.0409
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5085
Metrics/base_velocity/error_vel_xy: 0.3307
Metrics/base_velocity/error_vel_yaw: 0.2083
      Episode_Termination/time_out: 0.2767
  Episode_Termination/base_contact: 0.7233
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 2.53s
                      Time elapsed: 00:10:00
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 235/1400 [0m                      

                       Computation: 9775 steps/s (collection: 2.252s, learning 0.262s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 7.8423
                       Mean reward: 5.79
               Mean episode length: 488.40
Episode_Reward/track_lin_vel_xy_exp: 0.3694
Episode_Reward/track_ang_vel_z_exp: 0.2131
       Episode_Reward/lin_vel_z_l2: -0.0315
      Episode_Reward/ang_vel_xy_l2: -0.0408
     Episode_Reward/dof_torques_l2: -0.0453
         Episode_Reward/dof_acc_l2: -0.0920
     Episode_Reward/action_rate_l2: -0.0446
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5129
Metrics/base_velocity/error_vel_xy: 0.3254
Metrics/base_velocity/error_vel_yaw: 0.2276
      Episode_Termination/time_out: 0.2788
  Episode_Termination/base_contact: 0.7212
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 2.51s
                      Time elapsed: 00:10:02
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 236/1400 [0m                      

                       Computation: 9567 steps/s (collection: 2.313s, learning 0.256s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 7.8340
                       Mean reward: 5.89
               Mean episode length: 502.05
Episode_Reward/track_lin_vel_xy_exp: 0.4150
Episode_Reward/track_ang_vel_z_exp: 0.2382
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0453
     Episode_Reward/dof_torques_l2: -0.0481
         Episode_Reward/dof_acc_l2: -0.0985
     Episode_Reward/action_rate_l2: -0.0493
      Episode_Reward/feet_air_time: -0.0065
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5171
Metrics/base_velocity/error_vel_xy: 0.3488
Metrics/base_velocity/error_vel_yaw: 0.2407
      Episode_Termination/time_out: 0.2828
  Episode_Termination/base_contact: 0.7172
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 2.57s
                      Time elapsed: 00:10:05
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 237/1400 [0m                      

                       Computation: 9630 steps/s (collection: 2.295s, learning 0.257s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 7.8336
                       Mean reward: 5.67
               Mean episode length: 493.43
Episode_Reward/track_lin_vel_xy_exp: 0.3140
Episode_Reward/track_ang_vel_z_exp: 0.1818
       Episode_Reward/lin_vel_z_l2: -0.0290
      Episode_Reward/ang_vel_xy_l2: -0.0368
     Episode_Reward/dof_torques_l2: -0.0367
         Episode_Reward/dof_acc_l2: -0.0787
     Episode_Reward/action_rate_l2: -0.0380
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5244
Metrics/base_velocity/error_vel_xy: 0.2831
Metrics/base_velocity/error_vel_yaw: 0.1983
      Episode_Termination/time_out: 0.2832
  Episode_Termination/base_contact: 0.7168
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 2.55s
                      Time elapsed: 00:10:07
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 238/1400 [0m                      

                       Computation: 9664 steps/s (collection: 2.287s, learning 0.256s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 7.8235
                       Mean reward: 5.50
               Mean episode length: 479.96
Episode_Reward/track_lin_vel_xy_exp: 0.3215
Episode_Reward/track_ang_vel_z_exp: 0.1907
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0371
     Episode_Reward/dof_torques_l2: -0.0398
         Episode_Reward/dof_acc_l2: -0.0829
     Episode_Reward/action_rate_l2: -0.0402
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5349
Metrics/base_velocity/error_vel_xy: 0.3131
Metrics/base_velocity/error_vel_yaw: 0.2119
      Episode_Termination/time_out: 0.2833
  Episode_Termination/base_contact: 0.7167
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 2.54s
                      Time elapsed: 00:10:10
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 239/1400 [0m                      

                       Computation: 9746 steps/s (collection: 2.256s, learning 0.266s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 7.8140
                       Mean reward: 5.35
               Mean episode length: 473.86
Episode_Reward/track_lin_vel_xy_exp: 0.2865
Episode_Reward/track_ang_vel_z_exp: 0.1685
       Episode_Reward/lin_vel_z_l2: -0.0283
      Episode_Reward/ang_vel_xy_l2: -0.0357
     Episode_Reward/dof_torques_l2: -0.0365
         Episode_Reward/dof_acc_l2: -0.0787
     Episode_Reward/action_rate_l2: -0.0361
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5377
Metrics/base_velocity/error_vel_xy: 0.2886
Metrics/base_velocity/error_vel_yaw: 0.1957
      Episode_Termination/time_out: 0.2806
  Episode_Termination/base_contact: 0.7194
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 2.52s
                      Time elapsed: 00:10:12
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 240/1400 [0m                      

                       Computation: 9436 steps/s (collection: 2.348s, learning 0.256s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 7.8027
                       Mean reward: 4.57
               Mean episode length: 426.09
Episode_Reward/track_lin_vel_xy_exp: 0.3204
Episode_Reward/track_ang_vel_z_exp: 0.1846
       Episode_Reward/lin_vel_z_l2: -0.0313
      Episode_Reward/ang_vel_xy_l2: -0.0365
     Episode_Reward/dof_torques_l2: -0.0386
         Episode_Reward/dof_acc_l2: -0.0846
     Episode_Reward/action_rate_l2: -0.0393
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5328
Metrics/base_velocity/error_vel_xy: 0.2909
Metrics/base_velocity/error_vel_yaw: 0.2063
      Episode_Termination/time_out: 0.2760
  Episode_Termination/base_contact: 0.7240
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 2.60s
                      Time elapsed: 00:10:15
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 241/1400 [0m                      

                       Computation: 9426 steps/s (collection: 2.350s, learning 0.257s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.7790
                       Mean reward: 4.91
               Mean episode length: 449.50
Episode_Reward/track_lin_vel_xy_exp: 0.2876
Episode_Reward/track_ang_vel_z_exp: 0.1700
       Episode_Reward/lin_vel_z_l2: -0.0284
      Episode_Reward/ang_vel_xy_l2: -0.0354
     Episode_Reward/dof_torques_l2: -0.0355
         Episode_Reward/dof_acc_l2: -0.0792
     Episode_Reward/action_rate_l2: -0.0368
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5354
Metrics/base_velocity/error_vel_xy: 0.2949
Metrics/base_velocity/error_vel_yaw: 0.2072
      Episode_Termination/time_out: 0.2767
  Episode_Termination/base_contact: 0.7233
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 2.61s
                      Time elapsed: 00:10:18
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 242/1400 [0m                      

                       Computation: 9381 steps/s (collection: 2.363s, learning 0.257s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 7.7694
                       Mean reward: 5.06
               Mean episode length: 482.81
Episode_Reward/track_lin_vel_xy_exp: 0.2976
Episode_Reward/track_ang_vel_z_exp: 0.1768
       Episode_Reward/lin_vel_z_l2: -0.0311
      Episode_Reward/ang_vel_xy_l2: -0.0368
     Episode_Reward/dof_torques_l2: -0.0376
         Episode_Reward/dof_acc_l2: -0.0808
     Episode_Reward/action_rate_l2: -0.0379
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0110
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5442
Metrics/base_velocity/error_vel_xy: 0.3041
Metrics/base_velocity/error_vel_yaw: 0.2050
      Episode_Termination/time_out: 0.2804
  Episode_Termination/base_contact: 0.7196
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 2.62s
                      Time elapsed: 00:10:20
                               ETA: 00:49:18

################################################################################
                     [1m Learning iteration 243/1400 [0m                      

                       Computation: 9364 steps/s (collection: 2.367s, learning 0.257s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 7.7643
                       Mean reward: 5.69
               Mean episode length: 507.24
Episode_Reward/track_lin_vel_xy_exp: 0.3085
Episode_Reward/track_ang_vel_z_exp: 0.1761
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0352
     Episode_Reward/dof_torques_l2: -0.0363
         Episode_Reward/dof_acc_l2: -0.0741
     Episode_Reward/action_rate_l2: -0.0372
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5484
Metrics/base_velocity/error_vel_xy: 0.2672
Metrics/base_velocity/error_vel_yaw: 0.1954
      Episode_Termination/time_out: 0.2826
  Episode_Termination/base_contact: 0.7174
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 2.62s
                      Time elapsed: 00:10:23
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 244/1400 [0m                      

                       Computation: 9428 steps/s (collection: 2.354s, learning 0.253s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 7.7605
                       Mean reward: 5.36
               Mean episode length: 473.99
Episode_Reward/track_lin_vel_xy_exp: 0.3026
Episode_Reward/track_ang_vel_z_exp: 0.1890
       Episode_Reward/lin_vel_z_l2: -0.0303
      Episode_Reward/ang_vel_xy_l2: -0.0375
     Episode_Reward/dof_torques_l2: -0.0401
         Episode_Reward/dof_acc_l2: -0.0848
     Episode_Reward/action_rate_l2: -0.0397
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5586
Metrics/base_velocity/error_vel_xy: 0.3317
Metrics/base_velocity/error_vel_yaw: 0.2046
      Episode_Termination/time_out: 0.2830
  Episode_Termination/base_contact: 0.7170
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 2.61s
                      Time elapsed: 00:10:25
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 245/1400 [0m                      

                       Computation: 9603 steps/s (collection: 2.303s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 7.7494
                       Mean reward: 5.74
               Mean episode length: 517.55
Episode_Reward/track_lin_vel_xy_exp: 0.3878
Episode_Reward/track_ang_vel_z_exp: 0.2248
       Episode_Reward/lin_vel_z_l2: -0.0378
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.1023
     Episode_Reward/action_rate_l2: -0.0476
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5660
Metrics/base_velocity/error_vel_xy: 0.3588
Metrics/base_velocity/error_vel_yaw: 0.2539
      Episode_Termination/time_out: 0.2846
  Episode_Termination/base_contact: 0.7154
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 2.56s
                      Time elapsed: 00:10:28
                               ETA: 00:49:11

################################################################################
                     [1m Learning iteration 246/1400 [0m                      

                       Computation: 9505 steps/s (collection: 2.329s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0176
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 7.7591
                       Mean reward: 6.22
               Mean episode length: 540.57
Episode_Reward/track_lin_vel_xy_exp: 0.3860
Episode_Reward/track_ang_vel_z_exp: 0.2235
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0446
     Episode_Reward/dof_torques_l2: -0.0473
         Episode_Reward/dof_acc_l2: -0.0956
     Episode_Reward/action_rate_l2: -0.0460
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5824
Metrics/base_velocity/error_vel_xy: 0.3287
Metrics/base_velocity/error_vel_yaw: 0.2241
      Episode_Termination/time_out: 0.2876
  Episode_Termination/base_contact: 0.7124
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 2.59s
                      Time elapsed: 00:10:31
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 247/1400 [0m                      

                       Computation: 9674 steps/s (collection: 2.276s, learning 0.265s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 7.7753
                       Mean reward: 5.45
               Mean episode length: 493.97
Episode_Reward/track_lin_vel_xy_exp: 0.3301
Episode_Reward/track_ang_vel_z_exp: 0.2014
       Episode_Reward/lin_vel_z_l2: -0.0329
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.0427
         Episode_Reward/dof_acc_l2: -0.0888
     Episode_Reward/action_rate_l2: -0.0422
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5931
Metrics/base_velocity/error_vel_xy: 0.3469
Metrics/base_velocity/error_vel_yaw: 0.2187
      Episode_Termination/time_out: 0.2875
  Episode_Termination/base_contact: 0.7125
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 2.54s
                      Time elapsed: 00:10:33
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 248/1400 [0m                      

                       Computation: 9739 steps/s (collection: 2.269s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 7.7757
                       Mean reward: 5.14
               Mean episode length: 476.02
Episode_Reward/track_lin_vel_xy_exp: 0.3010
Episode_Reward/track_ang_vel_z_exp: 0.1794
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.0376
     Episode_Reward/dof_torques_l2: -0.0388
         Episode_Reward/dof_acc_l2: -0.0829
     Episode_Reward/action_rate_l2: -0.0379
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6034
Metrics/base_velocity/error_vel_xy: 0.3002
Metrics/base_velocity/error_vel_yaw: 0.2033
      Episode_Termination/time_out: 0.2861
  Episode_Termination/base_contact: 0.7139
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 2.52s
                      Time elapsed: 00:10:36
                               ETA: 00:49:03

################################################################################
                     [1m Learning iteration 249/1400 [0m                      

                       Computation: 9515 steps/s (collection: 2.328s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 7.7679
                       Mean reward: 4.64
               Mean episode length: 421.81
Episode_Reward/track_lin_vel_xy_exp: 0.2746
Episode_Reward/track_ang_vel_z_exp: 0.1615
       Episode_Reward/lin_vel_z_l2: -0.0273
      Episode_Reward/ang_vel_xy_l2: -0.0327
     Episode_Reward/dof_torques_l2: -0.0338
         Episode_Reward/dof_acc_l2: -0.0737
     Episode_Reward/action_rate_l2: -0.0338
      Episode_Reward/feet_air_time: -0.0047
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6081
Metrics/base_velocity/error_vel_xy: 0.2677
Metrics/base_velocity/error_vel_yaw: 0.1816
      Episode_Termination/time_out: 0.2876
  Episode_Termination/base_contact: 0.7124
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 2.58s
                      Time elapsed: 00:10:38
                               ETA: 00:49:00

################################################################################
                     [1m Learning iteration 250/1400 [0m                      

                       Computation: 9719 steps/s (collection: 2.273s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 7.7588
                       Mean reward: 4.81
               Mean episode length: 427.48
Episode_Reward/track_lin_vel_xy_exp: 0.2887
Episode_Reward/track_ang_vel_z_exp: 0.1751
       Episode_Reward/lin_vel_z_l2: -0.0273
      Episode_Reward/ang_vel_xy_l2: -0.0353
     Episode_Reward/dof_torques_l2: -0.0380
         Episode_Reward/dof_acc_l2: -0.0751
     Episode_Reward/action_rate_l2: -0.0366
      Episode_Reward/feet_air_time: -0.0051
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6172
Metrics/base_velocity/error_vel_xy: 0.3008
Metrics/base_velocity/error_vel_yaw: 0.1917
      Episode_Termination/time_out: 0.2885
  Episode_Termination/base_contact: 0.7115
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 2.53s
                      Time elapsed: 00:10:41
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 251/1400 [0m                      

                       Computation: 9713 steps/s (collection: 2.274s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 7.7578
                       Mean reward: 5.90
               Mean episode length: 505.96
Episode_Reward/track_lin_vel_xy_exp: 0.3649
Episode_Reward/track_ang_vel_z_exp: 0.2143
       Episode_Reward/lin_vel_z_l2: -0.0355
      Episode_Reward/ang_vel_xy_l2: -0.0413
     Episode_Reward/dof_torques_l2: -0.0429
         Episode_Reward/dof_acc_l2: -0.0914
     Episode_Reward/action_rate_l2: -0.0444
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6211
Metrics/base_velocity/error_vel_xy: 0.3399
Metrics/base_velocity/error_vel_yaw: 0.2215
      Episode_Termination/time_out: 0.2885
  Episode_Termination/base_contact: 0.7115
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.53s
                      Time elapsed: 00:10:43
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 252/1400 [0m                      

                       Computation: 9640 steps/s (collection: 2.288s, learning 0.262s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 7.7396
                       Mean reward: 5.33
               Mean episode length: 488.58
Episode_Reward/track_lin_vel_xy_exp: 0.3289
Episode_Reward/track_ang_vel_z_exp: 0.2007
       Episode_Reward/lin_vel_z_l2: -0.0324
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.0426
         Episode_Reward/dof_acc_l2: -0.0900
     Episode_Reward/action_rate_l2: -0.0425
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6292
Metrics/base_velocity/error_vel_xy: 0.3503
Metrics/base_velocity/error_vel_yaw: 0.2301
      Episode_Termination/time_out: 0.2911
  Episode_Termination/base_contact: 0.7089
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 2.55s
                      Time elapsed: 00:10:46
                               ETA: 00:48:52

################################################################################
                     [1m Learning iteration 253/1400 [0m                      

                       Computation: 9527 steps/s (collection: 2.325s, learning 0.254s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 7.7299
                       Mean reward: 5.66
               Mean episode length: 507.67
Episode_Reward/track_lin_vel_xy_exp: 0.3484
Episode_Reward/track_ang_vel_z_exp: 0.2020
       Episode_Reward/lin_vel_z_l2: -0.0328
      Episode_Reward/ang_vel_xy_l2: -0.0386
     Episode_Reward/dof_torques_l2: -0.0443
         Episode_Reward/dof_acc_l2: -0.0882
     Episode_Reward/action_rate_l2: -0.0426
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6329
Metrics/base_velocity/error_vel_xy: 0.3197
Metrics/base_velocity/error_vel_yaw: 0.2235
      Episode_Termination/time_out: 0.2972
  Episode_Termination/base_contact: 0.7028
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 2.58s
                      Time elapsed: 00:10:48
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 254/1400 [0m                      

                       Computation: 9547 steps/s (collection: 2.319s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.7058
                       Mean reward: 5.74
               Mean episode length: 504.02
Episode_Reward/track_lin_vel_xy_exp: 0.3407
Episode_Reward/track_ang_vel_z_exp: 0.2017
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0420
     Episode_Reward/dof_torques_l2: -0.0414
         Episode_Reward/dof_acc_l2: -0.0928
     Episode_Reward/action_rate_l2: -0.0423
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6442
Metrics/base_velocity/error_vel_xy: 0.3297
Metrics/base_velocity/error_vel_yaw: 0.2217
      Episode_Termination/time_out: 0.2966
  Episode_Termination/base_contact: 0.7034
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 2.57s
                      Time elapsed: 00:10:51
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 255/1400 [0m                      

                       Computation: 9629 steps/s (collection: 2.298s, learning 0.254s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 7.6812
                       Mean reward: 5.05
               Mean episode length: 475.27
Episode_Reward/track_lin_vel_xy_exp: 0.2810
Episode_Reward/track_ang_vel_z_exp: 0.1690
       Episode_Reward/lin_vel_z_l2: -0.0327
      Episode_Reward/ang_vel_xy_l2: -0.0356
     Episode_Reward/dof_torques_l2: -0.0372
         Episode_Reward/dof_acc_l2: -0.0819
     Episode_Reward/action_rate_l2: -0.0361
      Episode_Reward/feet_air_time: -0.0049
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6479
Metrics/base_velocity/error_vel_xy: 0.2942
Metrics/base_velocity/error_vel_yaw: 0.1988
      Episode_Termination/time_out: 0.2985
  Episode_Termination/base_contact: 0.7015
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 2.55s
                      Time elapsed: 00:10:54
                               ETA: 00:48:45

################################################################################
                     [1m Learning iteration 256/1400 [0m                      

                       Computation: 9596 steps/s (collection: 2.307s, learning 0.254s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 7.6802
                       Mean reward: 5.35
               Mean episode length: 491.64
Episode_Reward/track_lin_vel_xy_exp: 0.3219
Episode_Reward/track_ang_vel_z_exp: 0.1923
       Episode_Reward/lin_vel_z_l2: -0.0329
      Episode_Reward/ang_vel_xy_l2: -0.0384
     Episode_Reward/dof_torques_l2: -0.0405
         Episode_Reward/dof_acc_l2: -0.0879
     Episode_Reward/action_rate_l2: -0.0406
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6577
Metrics/base_velocity/error_vel_xy: 0.3399
Metrics/base_velocity/error_vel_yaw: 0.2209
      Episode_Termination/time_out: 0.3022
  Episode_Termination/base_contact: 0.6978
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 2.56s
                      Time elapsed: 00:10:56
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 257/1400 [0m                      

                       Computation: 9645 steps/s (collection: 2.292s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.6795
                       Mean reward: 5.46
               Mean episode length: 498.43
Episode_Reward/track_lin_vel_xy_exp: 0.2938
Episode_Reward/track_ang_vel_z_exp: 0.1794
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0354
     Episode_Reward/dof_torques_l2: -0.0401
         Episode_Reward/dof_acc_l2: -0.0838
     Episode_Reward/action_rate_l2: -0.0380
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6677
Metrics/base_velocity/error_vel_xy: 0.3229
Metrics/base_velocity/error_vel_yaw: 0.2024
      Episode_Termination/time_out: 0.3019
  Episode_Termination/base_contact: 0.6981
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 2.55s
                      Time elapsed: 00:10:59
                               ETA: 00:48:40

################################################################################
                     [1m Learning iteration 258/1400 [0m                      

                       Computation: 9575 steps/s (collection: 2.303s, learning 0.263s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 7.6724
                       Mean reward: 5.56
               Mean episode length: 498.09
Episode_Reward/track_lin_vel_xy_exp: 0.3069
Episode_Reward/track_ang_vel_z_exp: 0.1767
       Episode_Reward/lin_vel_z_l2: -0.0297
      Episode_Reward/ang_vel_xy_l2: -0.0356
     Episode_Reward/dof_torques_l2: -0.0371
         Episode_Reward/dof_acc_l2: -0.0800
     Episode_Reward/action_rate_l2: -0.0373
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6745
Metrics/base_velocity/error_vel_xy: 0.2840
Metrics/base_velocity/error_vel_yaw: 0.1984
      Episode_Termination/time_out: 0.2987
  Episode_Termination/base_contact: 0.7013
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 2.57s
                      Time elapsed: 00:11:01
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 259/1400 [0m                      

                       Computation: 9589 steps/s (collection: 2.308s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 7.6797
                       Mean reward: 5.16
               Mean episode length: 452.89
Episode_Reward/track_lin_vel_xy_exp: 0.3080
Episode_Reward/track_ang_vel_z_exp: 0.1744
       Episode_Reward/lin_vel_z_l2: -0.0297
      Episode_Reward/ang_vel_xy_l2: -0.0361
     Episode_Reward/dof_torques_l2: -0.0353
         Episode_Reward/dof_acc_l2: -0.0759
     Episode_Reward/action_rate_l2: -0.0366
      Episode_Reward/feet_air_time: -0.0046
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6756
Metrics/base_velocity/error_vel_xy: 0.2729
Metrics/base_velocity/error_vel_yaw: 0.1964
      Episode_Termination/time_out: 0.2934
  Episode_Termination/base_contact: 0.7066
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 2.56s
                      Time elapsed: 00:11:04
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 260/1400 [0m                      

                       Computation: 9705 steps/s (collection: 2.277s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 7.6818
                       Mean reward: 5.42
               Mean episode length: 475.45
Episode_Reward/track_lin_vel_xy_exp: 0.3117
Episode_Reward/track_ang_vel_z_exp: 0.1898
       Episode_Reward/lin_vel_z_l2: -0.0318
      Episode_Reward/ang_vel_xy_l2: -0.0384
     Episode_Reward/dof_torques_l2: -0.0403
         Episode_Reward/dof_acc_l2: -0.0908
     Episode_Reward/action_rate_l2: -0.0394
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6774
Metrics/base_velocity/error_vel_xy: 0.3272
Metrics/base_velocity/error_vel_yaw: 0.2142
      Episode_Termination/time_out: 0.2914
  Episode_Termination/base_contact: 0.7086
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 2.53s
                      Time elapsed: 00:11:06
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 261/1400 [0m                      

                       Computation: 9646 steps/s (collection: 2.291s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 7.6605
                       Mean reward: 5.40
               Mean episode length: 506.33
Episode_Reward/track_lin_vel_xy_exp: 0.3154
Episode_Reward/track_ang_vel_z_exp: 0.1894
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0419
     Episode_Reward/dof_torques_l2: -0.0405
         Episode_Reward/dof_acc_l2: -0.0932
     Episode_Reward/action_rate_l2: -0.0408
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6811
Metrics/base_velocity/error_vel_xy: 0.3425
Metrics/base_velocity/error_vel_yaw: 0.2344
      Episode_Termination/time_out: 0.2911
  Episode_Termination/base_contact: 0.7089
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 2.55s
                      Time elapsed: 00:11:09
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 262/1400 [0m                      

                       Computation: 9482 steps/s (collection: 2.335s, learning 0.257s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.6513
                       Mean reward: 4.72
               Mean episode length: 447.10
Episode_Reward/track_lin_vel_xy_exp: 0.2950
Episode_Reward/track_ang_vel_z_exp: 0.1710
       Episode_Reward/lin_vel_z_l2: -0.0309
      Episode_Reward/ang_vel_xy_l2: -0.0363
     Episode_Reward/dof_torques_l2: -0.0361
         Episode_Reward/dof_acc_l2: -0.0800
     Episode_Reward/action_rate_l2: -0.0365
      Episode_Reward/feet_air_time: -0.0051
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6831
Metrics/base_velocity/error_vel_xy: 0.2808
Metrics/base_velocity/error_vel_yaw: 0.1947
      Episode_Termination/time_out: 0.2900
  Episode_Termination/base_contact: 0.7100
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 2.59s
                      Time elapsed: 00:11:11
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 263/1400 [0m                      

                       Computation: 9482 steps/s (collection: 2.335s, learning 0.257s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 7.6364
                       Mean reward: 4.65
               Mean episode length: 424.44
Episode_Reward/track_lin_vel_xy_exp: 0.2744
Episode_Reward/track_ang_vel_z_exp: 0.1680
       Episode_Reward/lin_vel_z_l2: -0.0278
      Episode_Reward/ang_vel_xy_l2: -0.0340
     Episode_Reward/dof_torques_l2: -0.0367
         Episode_Reward/dof_acc_l2: -0.0793
     Episode_Reward/action_rate_l2: -0.0350
      Episode_Reward/feet_air_time: -0.0052
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6838
Metrics/base_velocity/error_vel_xy: 0.2946
Metrics/base_velocity/error_vel_yaw: 0.1876
      Episode_Termination/time_out: 0.2906
  Episode_Termination/base_contact: 0.7094
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 2.59s
                      Time elapsed: 00:11:14
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 264/1400 [0m                      

                       Computation: 9543 steps/s (collection: 2.320s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 7.6421
                       Mean reward: 3.93
               Mean episode length: 397.08
Episode_Reward/track_lin_vel_xy_exp: 0.1776
Episode_Reward/track_ang_vel_z_exp: 0.1161
       Episode_Reward/lin_vel_z_l2: -0.0254
      Episode_Reward/ang_vel_xy_l2: -0.0269
     Episode_Reward/dof_torques_l2: -0.0275
         Episode_Reward/dof_acc_l2: -0.0626
     Episode_Reward/action_rate_l2: -0.0253
      Episode_Reward/feet_air_time: -0.0041
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6858
Metrics/base_velocity/error_vel_xy: 0.2555
Metrics/base_velocity/error_vel_yaw: 0.1540
      Episode_Termination/time_out: 0.2865
  Episode_Termination/base_contact: 0.7135
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 2.58s
                      Time elapsed: 00:11:17
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 265/1400 [0m                      

                       Computation: 9605 steps/s (collection: 2.297s, learning 0.262s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 7.6400
                       Mean reward: 4.53
               Mean episode length: 428.45
Episode_Reward/track_lin_vel_xy_exp: 0.3054
Episode_Reward/track_ang_vel_z_exp: 0.1761
       Episode_Reward/lin_vel_z_l2: -0.0280
      Episode_Reward/ang_vel_xy_l2: -0.0345
     Episode_Reward/dof_torques_l2: -0.0374
         Episode_Reward/dof_acc_l2: -0.0775
     Episode_Reward/action_rate_l2: -0.0368
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6881
Metrics/base_velocity/error_vel_xy: 0.2757
Metrics/base_velocity/error_vel_yaw: 0.1957
      Episode_Termination/time_out: 0.2850
  Episode_Termination/base_contact: 0.7150
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 2.56s
                      Time elapsed: 00:11:19
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 266/1400 [0m                      

                       Computation: 9672 steps/s (collection: 2.279s, learning 0.261s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 7.6373
                       Mean reward: 5.54
               Mean episode length: 485.73
Episode_Reward/track_lin_vel_xy_exp: 0.3516
Episode_Reward/track_ang_vel_z_exp: 0.2074
       Episode_Reward/lin_vel_z_l2: -0.0342
      Episode_Reward/ang_vel_xy_l2: -0.0428
     Episode_Reward/dof_torques_l2: -0.0442
         Episode_Reward/dof_acc_l2: -0.0936
     Episode_Reward/action_rate_l2: -0.0436
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6928
Metrics/base_velocity/error_vel_xy: 0.3448
Metrics/base_velocity/error_vel_yaw: 0.2349
      Episode_Termination/time_out: 0.2850
  Episode_Termination/base_contact: 0.7150
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 2.54s
                      Time elapsed: 00:11:22
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 267/1400 [0m                      

                       Computation: 9651 steps/s (collection: 2.284s, learning 0.263s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 7.6405
                       Mean reward: 5.52
               Mean episode length: 489.59
Episode_Reward/track_lin_vel_xy_exp: 0.3565
Episode_Reward/track_ang_vel_z_exp: 0.2012
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0421
     Episode_Reward/dof_torques_l2: -0.0421
         Episode_Reward/dof_acc_l2: -0.0968
     Episode_Reward/action_rate_l2: -0.0421
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7040
Metrics/base_velocity/error_vel_xy: 0.3030
Metrics/base_velocity/error_vel_yaw: 0.2217
      Episode_Termination/time_out: 0.2871
  Episode_Termination/base_contact: 0.7129
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 2.55s
                      Time elapsed: 00:11:24
                               ETA: 00:48:15

################################################################################
                     [1m Learning iteration 268/1400 [0m                      

                       Computation: 9553 steps/s (collection: 2.316s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 7.6282
                       Mean reward: 6.13
               Mean episode length: 517.28
Episode_Reward/track_lin_vel_xy_exp: 0.3432
Episode_Reward/track_ang_vel_z_exp: 0.1982
       Episode_Reward/lin_vel_z_l2: -0.0317
      Episode_Reward/ang_vel_xy_l2: -0.0392
     Episode_Reward/dof_torques_l2: -0.0431
         Episode_Reward/dof_acc_l2: -0.0879
     Episode_Reward/action_rate_l2: -0.0413
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7135
Metrics/base_velocity/error_vel_xy: 0.3055
Metrics/base_velocity/error_vel_yaw: 0.2128
      Episode_Termination/time_out: 0.2897
  Episode_Termination/base_contact: 0.7103
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 2.57s
                      Time elapsed: 00:11:27
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 269/1400 [0m                      

                       Computation: 9585 steps/s (collection: 2.310s, learning 0.254s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 7.6303
                       Mean reward: 5.58
               Mean episode length: 505.64
Episode_Reward/track_lin_vel_xy_exp: 0.3359
Episode_Reward/track_ang_vel_z_exp: 0.1971
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.0425
     Episode_Reward/dof_torques_l2: -0.0447
         Episode_Reward/dof_acc_l2: -0.0923
     Episode_Reward/action_rate_l2: -0.0422
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7192
Metrics/base_velocity/error_vel_xy: 0.3387
Metrics/base_velocity/error_vel_yaw: 0.2348
      Episode_Termination/time_out: 0.2905
  Episode_Termination/base_contact: 0.7095
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 2.56s
                      Time elapsed: 00:11:29
                               ETA: 00:48:10

################################################################################
                     [1m Learning iteration 270/1400 [0m                      

                       Computation: 9667 steps/s (collection: 2.285s, learning 0.257s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 7.6231
                       Mean reward: 5.95
               Mean episode length: 555.87
Episode_Reward/track_lin_vel_xy_exp: 0.4404
Episode_Reward/track_ang_vel_z_exp: 0.2531
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0509
         Episode_Reward/dof_acc_l2: -0.1106
     Episode_Reward/action_rate_l2: -0.0525
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0072
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7297
Metrics/base_velocity/error_vel_xy: 0.3779
Metrics/base_velocity/error_vel_yaw: 0.2699
      Episode_Termination/time_out: 0.2920
  Episode_Termination/base_contact: 0.7080
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 2.54s
                      Time elapsed: 00:11:32
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 271/1400 [0m                      

                       Computation: 9595 steps/s (collection: 2.305s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 7.6312
                       Mean reward: 6.25
               Mean episode length: 555.23
Episode_Reward/track_lin_vel_xy_exp: 0.3304
Episode_Reward/track_ang_vel_z_exp: 0.1932
       Episode_Reward/lin_vel_z_l2: -0.0327
      Episode_Reward/ang_vel_xy_l2: -0.0379
     Episode_Reward/dof_torques_l2: -0.0414
         Episode_Reward/dof_acc_l2: -0.0871
     Episode_Reward/action_rate_l2: -0.0407
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7359
Metrics/base_velocity/error_vel_xy: 0.3128
Metrics/base_velocity/error_vel_yaw: 0.2145
      Episode_Termination/time_out: 0.2954
  Episode_Termination/base_contact: 0.7046
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 2.56s
                      Time elapsed: 00:11:35
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 272/1400 [0m                      

                       Computation: 9526 steps/s (collection: 2.324s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 7.6266
                       Mean reward: 5.40
               Mean episode length: 487.03
Episode_Reward/track_lin_vel_xy_exp: 0.3003
Episode_Reward/track_ang_vel_z_exp: 0.1755
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0371
     Episode_Reward/dof_torques_l2: -0.0385
         Episode_Reward/dof_acc_l2: -0.0845
     Episode_Reward/action_rate_l2: -0.0378
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7438
Metrics/base_velocity/error_vel_xy: 0.2973
Metrics/base_velocity/error_vel_yaw: 0.2060
      Episode_Termination/time_out: 0.2949
  Episode_Termination/base_contact: 0.7051
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 2.58s
                      Time elapsed: 00:11:37
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 273/1400 [0m                      

                       Computation: 9484 steps/s (collection: 2.336s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 7.6301
                       Mean reward: 5.38
               Mean episode length: 475.98
Episode_Reward/track_lin_vel_xy_exp: 0.3934
Episode_Reward/track_ang_vel_z_exp: 0.2236
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0473
     Episode_Reward/dof_torques_l2: -0.0481
         Episode_Reward/dof_acc_l2: -0.1021
     Episode_Reward/action_rate_l2: -0.0478
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7522
Metrics/base_velocity/error_vel_xy: 0.3578
Metrics/base_velocity/error_vel_yaw: 0.2576
      Episode_Termination/time_out: 0.2937
  Episode_Termination/base_contact: 0.7063
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 2.59s
                      Time elapsed: 00:11:40
                               ETA: 00:48:00

################################################################################
                     [1m Learning iteration 274/1400 [0m                      

                       Computation: 9642 steps/s (collection: 2.294s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0190
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 7.6303
                       Mean reward: 6.46
               Mean episode length: 529.89
Episode_Reward/track_lin_vel_xy_exp: 0.3821
Episode_Reward/track_ang_vel_z_exp: 0.2063
       Episode_Reward/lin_vel_z_l2: -0.0311
      Episode_Reward/ang_vel_xy_l2: -0.0391
     Episode_Reward/dof_torques_l2: -0.0424
         Episode_Reward/dof_acc_l2: -0.0933
     Episode_Reward/action_rate_l2: -0.0426
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7537
Metrics/base_velocity/error_vel_xy: 0.2746
Metrics/base_velocity/error_vel_yaw: 0.2147
      Episode_Termination/time_out: 0.2958
  Episode_Termination/base_contact: 0.7042
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 2.55s
                      Time elapsed: 00:11:42
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 275/1400 [0m                      

                       Computation: 9563 steps/s (collection: 2.313s, learning 0.257s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 7.6258
                       Mean reward: 6.48
               Mean episode length: 522.00
Episode_Reward/track_lin_vel_xy_exp: 0.3876
Episode_Reward/track_ang_vel_z_exp: 0.2111
       Episode_Reward/lin_vel_z_l2: -0.0319
      Episode_Reward/ang_vel_xy_l2: -0.0401
     Episode_Reward/dof_torques_l2: -0.0423
         Episode_Reward/dof_acc_l2: -0.0870
     Episode_Reward/action_rate_l2: -0.0436
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7572
Metrics/base_velocity/error_vel_xy: 0.2991
Metrics/base_velocity/error_vel_yaw: 0.2291
      Episode_Termination/time_out: 0.2965
  Episode_Termination/base_contact: 0.7035
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 2.57s
                      Time elapsed: 00:11:45
                               ETA: 00:47:55

################################################################################
                     [1m Learning iteration 276/1400 [0m                      

                       Computation: 9604 steps/s (collection: 2.303s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 7.6313
                       Mean reward: 6.40
               Mean episode length: 547.41
Episode_Reward/track_lin_vel_xy_exp: 0.3836
Episode_Reward/track_ang_vel_z_exp: 0.2152
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0428
     Episode_Reward/dof_torques_l2: -0.0461
         Episode_Reward/dof_acc_l2: -0.1007
     Episode_Reward/action_rate_l2: -0.0454
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0131
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7585
Metrics/base_velocity/error_vel_xy: 0.3408
Metrics/base_velocity/error_vel_yaw: 0.2570
      Episode_Termination/time_out: 0.2992
  Episode_Termination/base_contact: 0.7008
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 2.56s
                      Time elapsed: 00:11:47
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 277/1400 [0m                      

                       Computation: 9522 steps/s (collection: 2.319s, learning 0.262s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 7.6308
                       Mean reward: 6.28
               Mean episode length: 550.69
Episode_Reward/track_lin_vel_xy_exp: 0.3746
Episode_Reward/track_ang_vel_z_exp: 0.2130
       Episode_Reward/lin_vel_z_l2: -0.0338
      Episode_Reward/ang_vel_xy_l2: -0.0425
     Episode_Reward/dof_torques_l2: -0.0433
         Episode_Reward/dof_acc_l2: -0.0936
     Episode_Reward/action_rate_l2: -0.0440
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7642
Metrics/base_velocity/error_vel_xy: 0.3230
Metrics/base_velocity/error_vel_yaw: 0.2301
      Episode_Termination/time_out: 0.3013
  Episode_Termination/base_contact: 0.6987
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 2.58s
                      Time elapsed: 00:11:50
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 278/1400 [0m                      

                       Computation: 9594 steps/s (collection: 2.307s, learning 0.254s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 7.6333
                       Mean reward: 6.31
               Mean episode length: 530.39
Episode_Reward/track_lin_vel_xy_exp: 0.3783
Episode_Reward/track_ang_vel_z_exp: 0.2139
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0428
     Episode_Reward/dof_torques_l2: -0.0439
         Episode_Reward/dof_acc_l2: -0.0979
     Episode_Reward/action_rate_l2: -0.0447
      Episode_Reward/feet_air_time: -0.0065
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7711
Metrics/base_velocity/error_vel_xy: 0.3273
Metrics/base_velocity/error_vel_yaw: 0.2383
      Episode_Termination/time_out: 0.3056
  Episode_Termination/base_contact: 0.6944
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 2.56s
                      Time elapsed: 00:11:53
                               ETA: 00:47:47

################################################################################
                     [1m Learning iteration 279/1400 [0m                      

                       Computation: 9726 steps/s (collection: 2.273s, learning 0.254s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 7.6333
                       Mean reward: 7.53
               Mean episode length: 595.77
Episode_Reward/track_lin_vel_xy_exp: 0.4478
Episode_Reward/track_ang_vel_z_exp: 0.2476
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0466
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.1018
     Episode_Reward/action_rate_l2: -0.0507
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7755
Metrics/base_velocity/error_vel_xy: 0.3455
Metrics/base_velocity/error_vel_yaw: 0.2603
      Episode_Termination/time_out: 0.3013
  Episode_Termination/base_contact: 0.6987
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 2.53s
                      Time elapsed: 00:11:55
                               ETA: 00:47:44

################################################################################
                     [1m Learning iteration 280/1400 [0m                      

                       Computation: 9768 steps/s (collection: 2.260s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.6247
                       Mean reward: 7.01
               Mean episode length: 560.46
Episode_Reward/track_lin_vel_xy_exp: 0.3596
Episode_Reward/track_ang_vel_z_exp: 0.2062
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0427
     Episode_Reward/dof_torques_l2: -0.0456
         Episode_Reward/dof_acc_l2: -0.0941
     Episode_Reward/action_rate_l2: -0.0434
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7817
Metrics/base_velocity/error_vel_xy: 0.3250
Metrics/base_velocity/error_vel_yaw: 0.2333
      Episode_Termination/time_out: 0.3037
  Episode_Termination/base_contact: 0.6963
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 2.52s
                      Time elapsed: 00:11:58
                               ETA: 00:47:42

################################################################################
                     [1m Learning iteration 281/1400 [0m                      

                       Computation: 9863 steps/s (collection: 2.235s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.6175
                       Mean reward: 5.84
               Mean episode length: 505.98
Episode_Reward/track_lin_vel_xy_exp: 0.3790
Episode_Reward/track_ang_vel_z_exp: 0.2172
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0422
     Episode_Reward/dof_torques_l2: -0.0476
         Episode_Reward/dof_acc_l2: -0.0971
     Episode_Reward/action_rate_l2: -0.0462
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7833
Metrics/base_velocity/error_vel_xy: 0.3548
Metrics/base_velocity/error_vel_yaw: 0.2543
      Episode_Termination/time_out: 0.3058
  Episode_Termination/base_contact: 0.6942
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 2.49s
                      Time elapsed: 00:12:00
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 282/1400 [0m                      

                       Computation: 9462 steps/s (collection: 2.341s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.6171
                       Mean reward: 5.61
               Mean episode length: 507.02
Episode_Reward/track_lin_vel_xy_exp: 0.3507
Episode_Reward/track_ang_vel_z_exp: 0.1984
       Episode_Reward/lin_vel_z_l2: -0.0324
      Episode_Reward/ang_vel_xy_l2: -0.0397
     Episode_Reward/dof_torques_l2: -0.0430
         Episode_Reward/dof_acc_l2: -0.0987
     Episode_Reward/action_rate_l2: -0.0423
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7888
Metrics/base_velocity/error_vel_xy: 0.3081
Metrics/base_velocity/error_vel_yaw: 0.2239
      Episode_Termination/time_out: 0.3090
  Episode_Termination/base_contact: 0.6910
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 2.60s
                      Time elapsed: 00:12:03
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 283/1400 [0m                      

                       Computation: 9502 steps/s (collection: 2.331s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 7.6158
                       Mean reward: 5.81
               Mean episode length: 510.31
Episode_Reward/track_lin_vel_xy_exp: 0.3593
Episode_Reward/track_ang_vel_z_exp: 0.2042
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0427
     Episode_Reward/dof_torques_l2: -0.0423
         Episode_Reward/dof_acc_l2: -0.0946
     Episode_Reward/action_rate_l2: -0.0432
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8019
Metrics/base_velocity/error_vel_xy: 0.3259
Metrics/base_velocity/error_vel_yaw: 0.2336
      Episode_Termination/time_out: 0.3051
  Episode_Termination/base_contact: 0.6949
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 2.59s
                      Time elapsed: 00:12:05
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 284/1400 [0m                      

                       Computation: 9520 steps/s (collection: 2.324s, learning 0.257s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 7.6064
                       Mean reward: 5.96
               Mean episode length: 520.61
Episode_Reward/track_lin_vel_xy_exp: 0.3595
Episode_Reward/track_ang_vel_z_exp: 0.2029
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0399
     Episode_Reward/dof_torques_l2: -0.0427
         Episode_Reward/dof_acc_l2: -0.0949
     Episode_Reward/action_rate_l2: -0.0436
      Episode_Reward/feet_air_time: -0.0065
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8094
Metrics/base_velocity/error_vel_xy: 0.3360
Metrics/base_velocity/error_vel_yaw: 0.2410
      Episode_Termination/time_out: 0.3050
  Episode_Termination/base_contact: 0.6950
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 2.58s
                      Time elapsed: 00:12:08
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 285/1400 [0m                      

                       Computation: 9493 steps/s (collection: 2.326s, learning 0.263s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.5993
                       Mean reward: 6.05
               Mean episode length: 515.46
Episode_Reward/track_lin_vel_xy_exp: 0.4149
Episode_Reward/track_ang_vel_z_exp: 0.2283
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0435
     Episode_Reward/dof_torques_l2: -0.0452
         Episode_Reward/dof_acc_l2: -0.0999
     Episode_Reward/action_rate_l2: -0.0474
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8138
Metrics/base_velocity/error_vel_xy: 0.3148
Metrics/base_velocity/error_vel_yaw: 0.2375
      Episode_Termination/time_out: 0.3029
  Episode_Termination/base_contact: 0.6971
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 2.59s
                      Time elapsed: 00:12:10
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 286/1400 [0m                      

                       Computation: 9568 steps/s (collection: 2.314s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 7.5929
                       Mean reward: 6.17
               Mean episode length: 514.38
Episode_Reward/track_lin_vel_xy_exp: 0.3537
Episode_Reward/track_ang_vel_z_exp: 0.2075
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0426
     Episode_Reward/dof_torques_l2: -0.0439
         Episode_Reward/dof_acc_l2: -0.1003
     Episode_Reward/action_rate_l2: -0.0434
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8168
Metrics/base_velocity/error_vel_xy: 0.3339
Metrics/base_velocity/error_vel_yaw: 0.2263
      Episode_Termination/time_out: 0.3059
  Episode_Termination/base_contact: 0.6941
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 2.57s
                      Time elapsed: 00:12:13
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 287/1400 [0m                      

                       Computation: 9517 steps/s (collection: 2.328s, learning 0.254s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 7.5894
                       Mean reward: 6.06
               Mean episode length: 524.48
Episode_Reward/track_lin_vel_xy_exp: 0.3513
Episode_Reward/track_ang_vel_z_exp: 0.2025
       Episode_Reward/lin_vel_z_l2: -0.0324
      Episode_Reward/ang_vel_xy_l2: -0.0415
     Episode_Reward/dof_torques_l2: -0.0409
         Episode_Reward/dof_acc_l2: -0.0987
     Episode_Reward/action_rate_l2: -0.0428
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8256
Metrics/base_velocity/error_vel_xy: 0.3158
Metrics/base_velocity/error_vel_yaw: 0.2213
      Episode_Termination/time_out: 0.3071
  Episode_Termination/base_contact: 0.6929
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 2.58s
                      Time elapsed: 00:12:16
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 288/1400 [0m                      

                       Computation: 9519 steps/s (collection: 2.326s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 7.5741
                       Mean reward: 5.53
               Mean episode length: 493.53
Episode_Reward/track_lin_vel_xy_exp: 0.3049
Episode_Reward/track_ang_vel_z_exp: 0.1761
       Episode_Reward/lin_vel_z_l2: -0.0317
      Episode_Reward/ang_vel_xy_l2: -0.0352
     Episode_Reward/dof_torques_l2: -0.0361
         Episode_Reward/dof_acc_l2: -0.0807
     Episode_Reward/action_rate_l2: -0.0366
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8353
Metrics/base_velocity/error_vel_xy: 0.2783
Metrics/base_velocity/error_vel_yaw: 0.1953
      Episode_Termination/time_out: 0.3081
  Episode_Termination/base_contact: 0.6919
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 2.58s
                      Time elapsed: 00:12:18
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 289/1400 [0m                      

                       Computation: 9576 steps/s (collection: 2.311s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.5586
                       Mean reward: 5.11
               Mean episode length: 478.54
Episode_Reward/track_lin_vel_xy_exp: 0.3656
Episode_Reward/track_ang_vel_z_exp: 0.2160
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.0439
         Episode_Reward/dof_acc_l2: -0.0919
     Episode_Reward/action_rate_l2: -0.0455
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8354
Metrics/base_velocity/error_vel_xy: 0.3400
Metrics/base_velocity/error_vel_yaw: 0.2292
      Episode_Termination/time_out: 0.3041
  Episode_Termination/base_contact: 0.6959
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 2.57s
                      Time elapsed: 00:12:21
                               ETA: 00:47:19

################################################################################
                     [1m Learning iteration 290/1400 [0m                      

                       Computation: 9492 steps/s (collection: 2.334s, learning 0.255s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 7.5632
                       Mean reward: 5.49
               Mean episode length: 495.13
Episode_Reward/track_lin_vel_xy_exp: 0.3706
Episode_Reward/track_ang_vel_z_exp: 0.2183
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0473
         Episode_Reward/dof_acc_l2: -0.1068
     Episode_Reward/action_rate_l2: -0.0465
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8459
Metrics/base_velocity/error_vel_xy: 0.3725
Metrics/base_velocity/error_vel_yaw: 0.2544
      Episode_Termination/time_out: 0.3007
  Episode_Termination/base_contact: 0.6993
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 2.59s
                      Time elapsed: 00:12:23
                               ETA: 00:47:17

################################################################################
                     [1m Learning iteration 291/1400 [0m                      

                       Computation: 9470 steps/s (collection: 2.339s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 7.5609
                       Mean reward: 5.97
               Mean episode length: 508.26
Episode_Reward/track_lin_vel_xy_exp: 0.3552
Episode_Reward/track_ang_vel_z_exp: 0.2001
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0390
     Episode_Reward/dof_torques_l2: -0.0416
         Episode_Reward/dof_acc_l2: -0.0912
     Episode_Reward/action_rate_l2: -0.0418
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8556
Metrics/base_velocity/error_vel_xy: 0.2949
Metrics/base_velocity/error_vel_yaw: 0.2116
      Episode_Termination/time_out: 0.3024
  Episode_Termination/base_contact: 0.6976
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 2.59s
                      Time elapsed: 00:12:26
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 292/1400 [0m                      

                       Computation: 9489 steps/s (collection: 2.334s, learning 0.256s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 7.5509
                       Mean reward: 5.82
               Mean episode length: 498.52
Episode_Reward/track_lin_vel_xy_exp: 0.3121
Episode_Reward/track_ang_vel_z_exp: 0.1732
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0358
     Episode_Reward/dof_torques_l2: -0.0366
         Episode_Reward/dof_acc_l2: -0.0786
     Episode_Reward/action_rate_l2: -0.0367
      Episode_Reward/feet_air_time: -0.0052
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8660
Metrics/base_velocity/error_vel_xy: 0.2584
Metrics/base_velocity/error_vel_yaw: 0.1943
      Episode_Termination/time_out: 0.2996
  Episode_Termination/base_contact: 0.7004
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 2.59s
                      Time elapsed: 00:12:28
                               ETA: 00:47:12

################################################################################
                     [1m Learning iteration 293/1400 [0m                      

                       Computation: 9512 steps/s (collection: 2.320s, learning 0.264s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 7.5415
                       Mean reward: 5.51
               Mean episode length: 490.41
Episode_Reward/track_lin_vel_xy_exp: 0.3636
Episode_Reward/track_ang_vel_z_exp: 0.2154
       Episode_Reward/lin_vel_z_l2: -0.0329
      Episode_Reward/ang_vel_xy_l2: -0.0431
     Episode_Reward/dof_torques_l2: -0.0464
         Episode_Reward/dof_acc_l2: -0.1026
     Episode_Reward/action_rate_l2: -0.0455
      Episode_Reward/feet_air_time: -0.0065
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8771
Metrics/base_velocity/error_vel_xy: 0.3544
Metrics/base_velocity/error_vel_yaw: 0.2378
      Episode_Termination/time_out: 0.3018
  Episode_Termination/base_contact: 0.6982
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 2.58s
                      Time elapsed: 00:12:31
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 294/1400 [0m                      

                       Computation: 9522 steps/s (collection: 2.322s, learning 0.259s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 7.5427
                       Mean reward: 6.35
               Mean episode length: 536.44
Episode_Reward/track_lin_vel_xy_exp: 0.3860
Episode_Reward/track_ang_vel_z_exp: 0.2127
       Episode_Reward/lin_vel_z_l2: -0.0335
      Episode_Reward/ang_vel_xy_l2: -0.0408
     Episode_Reward/dof_torques_l2: -0.0420
         Episode_Reward/dof_acc_l2: -0.0988
     Episode_Reward/action_rate_l2: -0.0442
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8851
Metrics/base_velocity/error_vel_xy: 0.3019
Metrics/base_velocity/error_vel_yaw: 0.2252
      Episode_Termination/time_out: 0.3020
  Episode_Termination/base_contact: 0.6980
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 2.58s
                      Time elapsed: 00:12:34
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 295/1400 [0m                      

                       Computation: 9611 steps/s (collection: 2.292s, learning 0.265s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 7.5441
                       Mean reward: 5.94
               Mean episode length: 500.11
Episode_Reward/track_lin_vel_xy_exp: 0.3539
Episode_Reward/track_ang_vel_z_exp: 0.1960
       Episode_Reward/lin_vel_z_l2: -0.0323
      Episode_Reward/ang_vel_xy_l2: -0.0371
     Episode_Reward/dof_torques_l2: -0.0403
         Episode_Reward/dof_acc_l2: -0.0911
     Episode_Reward/action_rate_l2: -0.0410
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8910
Metrics/base_velocity/error_vel_xy: 0.2819
Metrics/base_velocity/error_vel_yaw: 0.2113
      Episode_Termination/time_out: 0.2960
  Episode_Termination/base_contact: 0.7040
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 2.56s
                      Time elapsed: 00:12:36
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 296/1400 [0m                      

                       Computation: 9587 steps/s (collection: 2.308s, learning 0.255s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 7.5377
                       Mean reward: 5.80
               Mean episode length: 506.32
Episode_Reward/track_lin_vel_xy_exp: 0.3237
Episode_Reward/track_ang_vel_z_exp: 0.1865
       Episode_Reward/lin_vel_z_l2: -0.0313
      Episode_Reward/ang_vel_xy_l2: -0.0382
     Episode_Reward/dof_torques_l2: -0.0370
         Episode_Reward/dof_acc_l2: -0.0881
     Episode_Reward/action_rate_l2: -0.0394
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9010
Metrics/base_velocity/error_vel_xy: 0.3008
Metrics/base_velocity/error_vel_yaw: 0.2205
      Episode_Termination/time_out: 0.2946
  Episode_Termination/base_contact: 0.7054
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 2.56s
                      Time elapsed: 00:12:39
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 297/1400 [0m                      

                       Computation: 9498 steps/s (collection: 2.330s, learning 0.258s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 7.5203
                       Mean reward: 5.73
               Mean episode length: 501.27
Episode_Reward/track_lin_vel_xy_exp: 0.2862
Episode_Reward/track_ang_vel_z_exp: 0.1638
       Episode_Reward/lin_vel_z_l2: -0.0295
      Episode_Reward/ang_vel_xy_l2: -0.0341
     Episode_Reward/dof_torques_l2: -0.0334
         Episode_Reward/dof_acc_l2: -0.0764
     Episode_Reward/action_rate_l2: -0.0346
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9099
Metrics/base_velocity/error_vel_xy: 0.2612
Metrics/base_velocity/error_vel_yaw: 0.1820
      Episode_Termination/time_out: 0.2946
  Episode_Termination/base_contact: 0.7054
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 2.59s
                      Time elapsed: 00:12:41
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 298/1400 [0m                      

                       Computation: 9564 steps/s (collection: 2.313s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.5049
                       Mean reward: 5.30
               Mean episode length: 454.56
Episode_Reward/track_lin_vel_xy_exp: 0.3126
Episode_Reward/track_ang_vel_z_exp: 0.1781
       Episode_Reward/lin_vel_z_l2: -0.0295
      Episode_Reward/ang_vel_xy_l2: -0.0348
     Episode_Reward/dof_torques_l2: -0.0370
         Episode_Reward/dof_acc_l2: -0.0812
     Episode_Reward/action_rate_l2: -0.0381
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9147
Metrics/base_velocity/error_vel_xy: 0.2812
Metrics/base_velocity/error_vel_yaw: 0.2023
      Episode_Termination/time_out: 0.2927
  Episode_Termination/base_contact: 0.7073
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 2.57s
                      Time elapsed: 00:12:44
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 299/1400 [0m                      

                       Computation: 9536 steps/s (collection: 2.323s, learning 0.254s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.4922
                       Mean reward: 5.17
               Mean episode length: 459.53
Episode_Reward/track_lin_vel_xy_exp: 0.3352
Episode_Reward/track_ang_vel_z_exp: 0.1919
       Episode_Reward/lin_vel_z_l2: -0.0309
      Episode_Reward/ang_vel_xy_l2: -0.0375
     Episode_Reward/dof_torques_l2: -0.0373
         Episode_Reward/dof_acc_l2: -0.0896
     Episode_Reward/action_rate_l2: -0.0402
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9194
Metrics/base_velocity/error_vel_xy: 0.3008
Metrics/base_velocity/error_vel_yaw: 0.2135
      Episode_Termination/time_out: 0.2909
  Episode_Termination/base_contact: 0.7091
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 2.58s
                      Time elapsed: 00:12:47
                               ETA: 00:46:54

################################################################################
                     [1m Learning iteration 300/1400 [0m                      

                       Computation: 9579 steps/s (collection: 2.308s, learning 0.258s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 7.4837
                       Mean reward: 5.67
               Mean episode length: 484.58
Episode_Reward/track_lin_vel_xy_exp: 0.3624
Episode_Reward/track_ang_vel_z_exp: 0.2062
       Episode_Reward/lin_vel_z_l2: -0.0338
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.0406
         Episode_Reward/dof_acc_l2: -0.0903
     Episode_Reward/action_rate_l2: -0.0424
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9257
Metrics/base_velocity/error_vel_xy: 0.3020
Metrics/base_velocity/error_vel_yaw: 0.2120
      Episode_Termination/time_out: 0.2931
  Episode_Termination/base_contact: 0.7069
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 2.57s
                      Time elapsed: 00:12:49
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 301/1400 [0m                      

                       Computation: 9539 steps/s (collection: 2.320s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.4853
                       Mean reward: 4.94
               Mean episode length: 433.03
Episode_Reward/track_lin_vel_xy_exp: 0.2967
Episode_Reward/track_ang_vel_z_exp: 0.1738
       Episode_Reward/lin_vel_z_l2: -0.0267
      Episode_Reward/ang_vel_xy_l2: -0.0344
     Episode_Reward/dof_torques_l2: -0.0351
         Episode_Reward/dof_acc_l2: -0.0793
     Episode_Reward/action_rate_l2: -0.0361
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9307
Metrics/base_velocity/error_vel_xy: 0.2813
Metrics/base_velocity/error_vel_yaw: 0.1891
      Episode_Termination/time_out: 0.2924
  Episode_Termination/base_contact: 0.7076
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 2.58s
                      Time elapsed: 00:12:52
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 302/1400 [0m                      

                       Computation: 9559 steps/s (collection: 2.315s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 7.4930
                       Mean reward: 5.07
               Mean episode length: 450.02
Episode_Reward/track_lin_vel_xy_exp: 0.3657
Episode_Reward/track_ang_vel_z_exp: 0.2065
       Episode_Reward/lin_vel_z_l2: -0.0319
      Episode_Reward/ang_vel_xy_l2: -0.0381
     Episode_Reward/dof_torques_l2: -0.0420
         Episode_Reward/dof_acc_l2: -0.0894
     Episode_Reward/action_rate_l2: -0.0429
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9350
Metrics/base_velocity/error_vel_xy: 0.3158
Metrics/base_velocity/error_vel_yaw: 0.2236
      Episode_Termination/time_out: 0.2904
  Episode_Termination/base_contact: 0.7096
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 2.57s
                      Time elapsed: 00:12:54
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 303/1400 [0m                      

                       Computation: 9678 steps/s (collection: 2.285s, learning 0.254s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0194
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 7.4829
                       Mean reward: 5.48
               Mean episode length: 506.47
Episode_Reward/track_lin_vel_xy_exp: 0.3999
Episode_Reward/track_ang_vel_z_exp: 0.2315
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0446
         Episode_Reward/dof_acc_l2: -0.1039
     Episode_Reward/action_rate_l2: -0.0478
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0099
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9471
Metrics/base_velocity/error_vel_xy: 0.3579
Metrics/base_velocity/error_vel_yaw: 0.2512
      Episode_Termination/time_out: 0.2937
  Episode_Termination/base_contact: 0.7063
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 2.54s
                      Time elapsed: 00:12:57
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 304/1400 [0m                      

                       Computation: 9683 steps/s (collection: 2.283s, learning 0.255s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0188
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 7.4870
                       Mean reward: 6.20
               Mean episode length: 555.90
Episode_Reward/track_lin_vel_xy_exp: 0.4126
Episode_Reward/track_ang_vel_z_exp: 0.2359
       Episode_Reward/lin_vel_z_l2: -0.0344
      Episode_Reward/ang_vel_xy_l2: -0.0428
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.1071
     Episode_Reward/action_rate_l2: -0.0494
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9564
Metrics/base_velocity/error_vel_xy: 0.3533
Metrics/base_velocity/error_vel_yaw: 0.2522
      Episode_Termination/time_out: 0.3027
  Episode_Termination/base_contact: 0.6973
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 2.54s
                      Time elapsed: 00:12:59
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 305/1400 [0m                      

                       Computation: 9568 steps/s (collection: 2.312s, learning 0.257s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.4926
                       Mean reward: 5.82
               Mean episode length: 529.47
Episode_Reward/track_lin_vel_xy_exp: 0.2899
Episode_Reward/track_ang_vel_z_exp: 0.1691
       Episode_Reward/lin_vel_z_l2: -0.0268
      Episode_Reward/ang_vel_xy_l2: -0.0345
     Episode_Reward/dof_torques_l2: -0.0381
         Episode_Reward/dof_acc_l2: -0.0849
     Episode_Reward/action_rate_l2: -0.0374
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9583
Metrics/base_velocity/error_vel_xy: 0.3210
Metrics/base_velocity/error_vel_yaw: 0.2319
      Episode_Termination/time_out: 0.3058
  Episode_Termination/base_contact: 0.6942
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 2.57s
                      Time elapsed: 00:13:02
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 306/1400 [0m                      

                       Computation: 9669 steps/s (collection: 2.287s, learning 0.255s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 7.4658
                       Mean reward: 4.93
               Mean episode length: 465.41
Episode_Reward/track_lin_vel_xy_exp: 0.3325
Episode_Reward/track_ang_vel_z_exp: 0.1934
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0348
     Episode_Reward/dof_torques_l2: -0.0400
         Episode_Reward/dof_acc_l2: -0.0905
     Episode_Reward/action_rate_l2: -0.0405
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9628
Metrics/base_velocity/error_vel_xy: 0.3097
Metrics/base_velocity/error_vel_yaw: 0.2133
      Episode_Termination/time_out: 0.3087
  Episode_Termination/base_contact: 0.6913
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 2.54s
                      Time elapsed: 00:13:04
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 307/1400 [0m                      

                       Computation: 9638 steps/s (collection: 2.289s, learning 0.261s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 7.4597
                       Mean reward: 5.23
               Mean episode length: 468.13
Episode_Reward/track_lin_vel_xy_exp: 0.3979
Episode_Reward/track_ang_vel_z_exp: 0.2216
       Episode_Reward/lin_vel_z_l2: -0.0305
      Episode_Reward/ang_vel_xy_l2: -0.0397
     Episode_Reward/dof_torques_l2: -0.0430
         Episode_Reward/dof_acc_l2: -0.0913
     Episode_Reward/action_rate_l2: -0.0449
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9631
Metrics/base_velocity/error_vel_xy: 0.3006
Metrics/base_velocity/error_vel_yaw: 0.2191
      Episode_Termination/time_out: 0.3081
  Episode_Termination/base_contact: 0.6919
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 2.55s
                      Time elapsed: 00:13:07
                               ETA: 00:46:34

################################################################################
                     [1m Learning iteration 308/1400 [0m                      

                       Computation: 9651 steps/s (collection: 2.281s, learning 0.265s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 7.4451
                       Mean reward: 4.98
               Mean episode length: 448.09
Episode_Reward/track_lin_vel_xy_exp: 0.3189
Episode_Reward/track_ang_vel_z_exp: 0.1841
       Episode_Reward/lin_vel_z_l2: -0.0302
      Episode_Reward/ang_vel_xy_l2: -0.0360
     Episode_Reward/dof_torques_l2: -0.0393
         Episode_Reward/dof_acc_l2: -0.0850
     Episode_Reward/action_rate_l2: -0.0391
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9693
Metrics/base_velocity/error_vel_xy: 0.2978
Metrics/base_velocity/error_vel_yaw: 0.2129
      Episode_Termination/time_out: 0.3048
  Episode_Termination/base_contact: 0.6952
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 2.55s
                      Time elapsed: 00:13:10
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 309/1400 [0m                      

                       Computation: 9686 steps/s (collection: 2.280s, learning 0.257s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 7.4223
                       Mean reward: 6.00
               Mean episode length: 491.37
Episode_Reward/track_lin_vel_xy_exp: 0.3756
Episode_Reward/track_ang_vel_z_exp: 0.2060
       Episode_Reward/lin_vel_z_l2: -0.0319
      Episode_Reward/ang_vel_xy_l2: -0.0377
     Episode_Reward/dof_torques_l2: -0.0380
         Episode_Reward/dof_acc_l2: -0.0873
     Episode_Reward/action_rate_l2: -0.0422
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9703
Metrics/base_velocity/error_vel_xy: 0.2801
Metrics/base_velocity/error_vel_yaw: 0.2122
      Episode_Termination/time_out: 0.3014
  Episode_Termination/base_contact: 0.6986
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 2.54s
                      Time elapsed: 00:13:12
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 310/1400 [0m                      

                       Computation: 9659 steps/s (collection: 2.288s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 7.4014
                       Mean reward: 5.76
               Mean episode length: 478.66
Episode_Reward/track_lin_vel_xy_exp: 0.4190
Episode_Reward/track_ang_vel_z_exp: 0.2343
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0438
     Episode_Reward/dof_torques_l2: -0.0467
         Episode_Reward/dof_acc_l2: -0.1028
     Episode_Reward/action_rate_l2: -0.0492
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9734
Metrics/base_velocity/error_vel_xy: 0.3414
Metrics/base_velocity/error_vel_yaw: 0.2526
      Episode_Termination/time_out: 0.2976
  Episode_Termination/base_contact: 0.7024
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 2.54s
                      Time elapsed: 00:13:15
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 311/1400 [0m                      

                       Computation: 9606 steps/s (collection: 2.306s, learning 0.252s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 7.3843
                       Mean reward: 5.88
               Mean episode length: 500.91
Episode_Reward/track_lin_vel_xy_exp: 0.3650
Episode_Reward/track_ang_vel_z_exp: 0.2067
       Episode_Reward/lin_vel_z_l2: -0.0318
      Episode_Reward/ang_vel_xy_l2: -0.0394
     Episode_Reward/dof_torques_l2: -0.0408
         Episode_Reward/dof_acc_l2: -0.0915
     Episode_Reward/action_rate_l2: -0.0430
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0077
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9768
Metrics/base_velocity/error_vel_xy: 0.3136
Metrics/base_velocity/error_vel_yaw: 0.2253
      Episode_Termination/time_out: 0.2964
  Episode_Termination/base_contact: 0.7036
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.56s
                      Time elapsed: 00:13:17
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 312/1400 [0m                      

                       Computation: 9720 steps/s (collection: 2.274s, learning 0.255s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 7.3818
                       Mean reward: 6.01
               Mean episode length: 504.18
Episode_Reward/track_lin_vel_xy_exp: 0.4034
Episode_Reward/track_ang_vel_z_exp: 0.2246
       Episode_Reward/lin_vel_z_l2: -0.0317
      Episode_Reward/ang_vel_xy_l2: -0.0413
     Episode_Reward/dof_torques_l2: -0.0449
         Episode_Reward/dof_acc_l2: -0.0915
     Episode_Reward/action_rate_l2: -0.0465
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9866
Metrics/base_velocity/error_vel_xy: 0.3215
Metrics/base_velocity/error_vel_yaw: 0.2359
      Episode_Termination/time_out: 0.2959
  Episode_Termination/base_contact: 0.7041
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 2.53s
                      Time elapsed: 00:13:20
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 313/1400 [0m                      

                       Computation: 9715 steps/s (collection: 2.273s, learning 0.257s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 7.3811
                       Mean reward: 5.92
               Mean episode length: 487.87
Episode_Reward/track_lin_vel_xy_exp: 0.2724
Episode_Reward/track_ang_vel_z_exp: 0.1589
       Episode_Reward/lin_vel_z_l2: -0.0249
      Episode_Reward/ang_vel_xy_l2: -0.0306
     Episode_Reward/dof_torques_l2: -0.0310
         Episode_Reward/dof_acc_l2: -0.0753
     Episode_Reward/action_rate_l2: -0.0334
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9890
Metrics/base_velocity/error_vel_xy: 0.2562
Metrics/base_velocity/error_vel_yaw: 0.1742
      Episode_Termination/time_out: 0.2973
  Episode_Termination/base_contact: 0.7027
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 2.53s
                      Time elapsed: 00:13:22
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 314/1400 [0m                      

                       Computation: 9600 steps/s (collection: 2.304s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 7.3649
                       Mean reward: 5.16
               Mean episode length: 455.03
Episode_Reward/track_lin_vel_xy_exp: 0.2707
Episode_Reward/track_ang_vel_z_exp: 0.1616
       Episode_Reward/lin_vel_z_l2: -0.0255
      Episode_Reward/ang_vel_xy_l2: -0.0317
     Episode_Reward/dof_torques_l2: -0.0348
         Episode_Reward/dof_acc_l2: -0.0777
     Episode_Reward/action_rate_l2: -0.0343
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0106
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9921
Metrics/base_velocity/error_vel_xy: 0.2876
Metrics/base_velocity/error_vel_yaw: 0.1939
      Episode_Termination/time_out: 0.2998
  Episode_Termination/base_contact: 0.7002
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 2.56s
                      Time elapsed: 00:13:25
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 315/1400 [0m                      

                       Computation: 9585 steps/s (collection: 2.311s, learning 0.253s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 7.3427
                       Mean reward: 5.55
               Mean episode length: 492.74
Episode_Reward/track_lin_vel_xy_exp: 0.3797
Episode_Reward/track_ang_vel_z_exp: 0.2148
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0402
     Episode_Reward/dof_torques_l2: -0.0422
         Episode_Reward/dof_acc_l2: -0.0973
     Episode_Reward/action_rate_l2: -0.0444
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9982
Metrics/base_velocity/error_vel_xy: 0.3155
Metrics/base_velocity/error_vel_yaw: 0.2282
      Episode_Termination/time_out: 0.3024
  Episode_Termination/base_contact: 0.6976
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.56s
                      Time elapsed: 00:13:27
                               ETA: 00:46:13

################################################################################
                     [1m Learning iteration 316/1400 [0m                      

                       Computation: 9683 steps/s (collection: 2.273s, learning 0.264s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 7.3493
                       Mean reward: 6.32
               Mean episode length: 511.28
Episode_Reward/track_lin_vel_xy_exp: 0.3815
Episode_Reward/track_ang_vel_z_exp: 0.2051
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.0385
     Episode_Reward/dof_torques_l2: -0.0418
         Episode_Reward/dof_acc_l2: -0.0912
     Episode_Reward/action_rate_l2: -0.0437
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0050
Metrics/base_velocity/error_vel_xy: 0.2914
Metrics/base_velocity/error_vel_yaw: 0.2320
      Episode_Termination/time_out: 0.3036
  Episode_Termination/base_contact: 0.6964
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 2.54s
                      Time elapsed: 00:13:30
                               ETA: 00:46:11

################################################################################
                     [1m Learning iteration 317/1400 [0m                      

                       Computation: 9506 steps/s (collection: 2.330s, learning 0.255s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 7.3569
                       Mean reward: 7.08
               Mean episode length: 551.71
Episode_Reward/track_lin_vel_xy_exp: 0.4442
Episode_Reward/track_ang_vel_z_exp: 0.2488
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0492
         Episode_Reward/dof_acc_l2: -0.1057
     Episode_Reward/action_rate_l2: -0.0509
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0120
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0147
Metrics/base_velocity/error_vel_xy: 0.3431
Metrics/base_velocity/error_vel_yaw: 0.2515
      Episode_Termination/time_out: 0.3026
  Episode_Termination/base_contact: 0.6974
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 2.59s
                      Time elapsed: 00:13:32
                               ETA: 00:46:08

################################################################################
                     [1m Learning iteration 318/1400 [0m                      

                       Computation: 9680 steps/s (collection: 2.284s, learning 0.255s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 7.3513
                       Mean reward: 7.60
               Mean episode length: 605.93
Episode_Reward/track_lin_vel_xy_exp: 0.4706
Episode_Reward/track_ang_vel_z_exp: 0.2670
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.0498
     Episode_Reward/dof_torques_l2: -0.0545
         Episode_Reward/dof_acc_l2: -0.1224
     Episode_Reward/action_rate_l2: -0.0559
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0290
Metrics/base_velocity/error_vel_xy: 0.3860
Metrics/base_velocity/error_vel_yaw: 0.2766
      Episode_Termination/time_out: 0.3077
  Episode_Termination/base_contact: 0.6923
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 2.54s
                      Time elapsed: 00:13:35
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 319/1400 [0m                      

                       Computation: 9541 steps/s (collection: 2.320s, learning 0.255s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0186
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.3478
                       Mean reward: 6.68
               Mean episode length: 579.98
Episode_Reward/track_lin_vel_xy_exp: 0.3860
Episode_Reward/track_ang_vel_z_exp: 0.2227
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0459
         Episode_Reward/dof_acc_l2: -0.1023
     Episode_Reward/action_rate_l2: -0.0470
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0452
Metrics/base_velocity/error_vel_xy: 0.3500
Metrics/base_velocity/error_vel_yaw: 0.2431
      Episode_Termination/time_out: 0.3096
  Episode_Termination/base_contact: 0.6904
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.58s
                      Time elapsed: 00:13:38
                               ETA: 00:46:03

################################################################################
                     [1m Learning iteration 320/1400 [0m                      

                       Computation: 9586 steps/s (collection: 2.305s, learning 0.259s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 7.3436
                       Mean reward: 6.29
               Mean episode length: 543.93
Episode_Reward/track_lin_vel_xy_exp: 0.3527
Episode_Reward/track_ang_vel_z_exp: 0.2101
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0388
     Episode_Reward/dof_torques_l2: -0.0428
         Episode_Reward/dof_acc_l2: -0.0940
     Episode_Reward/action_rate_l2: -0.0437
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0599
Metrics/base_velocity/error_vel_xy: 0.3468
Metrics/base_velocity/error_vel_yaw: 0.2243
      Episode_Termination/time_out: 0.3108
  Episode_Termination/base_contact: 0.6892
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 2.56s
                      Time elapsed: 00:13:40
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 321/1400 [0m                      

                       Computation: 9617 steps/s (collection: 2.300s, learning 0.255s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 7.3379
                       Mean reward: 6.17
               Mean episode length: 528.63
Episode_Reward/track_lin_vel_xy_exp: 0.3110
Episode_Reward/track_ang_vel_z_exp: 0.1782
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.0364
     Episode_Reward/dof_torques_l2: -0.0355
         Episode_Reward/dof_acc_l2: -0.0822
     Episode_Reward/action_rate_l2: -0.0379
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0640
Metrics/base_velocity/error_vel_xy: 0.2879
Metrics/base_velocity/error_vel_yaw: 0.2038
      Episode_Termination/time_out: 0.3118
  Episode_Termination/base_contact: 0.6882
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 2.56s
                      Time elapsed: 00:13:43
                               ETA: 00:45:58

################################################################################
                     [1m Learning iteration 322/1400 [0m                      

                       Computation: 9710 steps/s (collection: 2.275s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 7.3514
                       Mean reward: 6.00
               Mean episode length: 500.80
Episode_Reward/track_lin_vel_xy_exp: 0.4270
Episode_Reward/track_ang_vel_z_exp: 0.2349
       Episode_Reward/lin_vel_z_l2: -0.0323
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0459
         Episode_Reward/dof_acc_l2: -0.0951
     Episode_Reward/action_rate_l2: -0.0476
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0641
Metrics/base_velocity/error_vel_xy: 0.3040
Metrics/base_velocity/error_vel_yaw: 0.2250
      Episode_Termination/time_out: 0.3094
  Episode_Termination/base_contact: 0.6906
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 2.53s
                      Time elapsed: 00:13:45
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 323/1400 [0m                      

                       Computation: 9667 steps/s (collection: 2.290s, learning 0.252s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.3567
                       Mean reward: 5.66
               Mean episode length: 487.33
Episode_Reward/track_lin_vel_xy_exp: 0.3406
Episode_Reward/track_ang_vel_z_exp: 0.2045
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.0402
     Episode_Reward/dof_torques_l2: -0.0428
         Episode_Reward/dof_acc_l2: -0.0950
     Episode_Reward/action_rate_l2: -0.0433
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0613
Metrics/base_velocity/error_vel_xy: 0.3551
Metrics/base_velocity/error_vel_yaw: 0.2310
      Episode_Termination/time_out: 0.3050
  Episode_Termination/base_contact: 0.6950
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.54s
                      Time elapsed: 00:13:48
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 324/1400 [0m                      

                       Computation: 9591 steps/s (collection: 2.307s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 7.3585
                       Mean reward: 6.07
               Mean episode length: 515.01
Episode_Reward/track_lin_vel_xy_exp: 0.3662
Episode_Reward/track_ang_vel_z_exp: 0.2098
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0383
     Episode_Reward/dof_torques_l2: -0.0419
         Episode_Reward/dof_acc_l2: -0.0910
     Episode_Reward/action_rate_l2: -0.0435
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0656
Metrics/base_velocity/error_vel_xy: 0.3169
Metrics/base_velocity/error_vel_yaw: 0.2219
      Episode_Termination/time_out: 0.3029
  Episode_Termination/base_contact: 0.6971
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 2.56s
                      Time elapsed: 00:13:50
                               ETA: 00:45:50

################################################################################
                     [1m Learning iteration 325/1400 [0m                      

                       Computation: 9609 steps/s (collection: 2.301s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.3447
                       Mean reward: 6.63
               Mean episode length: 539.93
Episode_Reward/track_lin_vel_xy_exp: 0.3828
Episode_Reward/track_ang_vel_z_exp: 0.2188
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0405
     Episode_Reward/dof_torques_l2: -0.0429
         Episode_Reward/dof_acc_l2: -0.0953
     Episode_Reward/action_rate_l2: -0.0448
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0793
Metrics/base_velocity/error_vel_xy: 0.3315
Metrics/base_velocity/error_vel_yaw: 0.2323
      Episode_Termination/time_out: 0.3031
  Episode_Termination/base_contact: 0.6969
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 2.56s
                      Time elapsed: 00:13:53
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 326/1400 [0m                      

                       Computation: 9626 steps/s (collection: 2.297s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 7.3356
                       Mean reward: 5.75
               Mean episode length: 496.23
Episode_Reward/track_lin_vel_xy_exp: 0.3057
Episode_Reward/track_ang_vel_z_exp: 0.1846
       Episode_Reward/lin_vel_z_l2: -0.0315
      Episode_Reward/ang_vel_xy_l2: -0.0368
     Episode_Reward/dof_torques_l2: -0.0379
         Episode_Reward/dof_acc_l2: -0.0876
     Episode_Reward/action_rate_l2: -0.0391
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0825
Metrics/base_velocity/error_vel_xy: 0.3255
Metrics/base_velocity/error_vel_yaw: 0.2111
      Episode_Termination/time_out: 0.3022
  Episode_Termination/base_contact: 0.6978
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 2.55s
                      Time elapsed: 00:13:55
                               ETA: 00:45:45

################################################################################
                     [1m Learning iteration 327/1400 [0m                      

                       Computation: 9501 steps/s (collection: 2.331s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0196
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 7.3249
                       Mean reward: 6.42
               Mean episode length: 528.84
Episode_Reward/track_lin_vel_xy_exp: 0.4492
Episode_Reward/track_ang_vel_z_exp: 0.2495
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0427
     Episode_Reward/dof_torques_l2: -0.0499
         Episode_Reward/dof_acc_l2: -0.1042
     Episode_Reward/action_rate_l2: -0.0510
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0868
Metrics/base_velocity/error_vel_xy: 0.3422
Metrics/base_velocity/error_vel_yaw: 0.2505
      Episode_Termination/time_out: 0.3005
  Episode_Termination/base_contact: 0.6995
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.59s
                      Time elapsed: 00:13:58
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 328/1400 [0m                      

                       Computation: 9714 steps/s (collection: 2.267s, learning 0.262s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 7.3247
                       Mean reward: 7.04
               Mean episode length: 574.26
Episode_Reward/track_lin_vel_xy_exp: 0.4008
Episode_Reward/track_ang_vel_z_exp: 0.2296
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.1107
     Episode_Reward/action_rate_l2: -0.0477
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0889
Metrics/base_velocity/error_vel_xy: 0.3536
Metrics/base_velocity/error_vel_yaw: 0.2502
      Episode_Termination/time_out: 0.3033
  Episode_Termination/base_contact: 0.6967
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 2.53s
                      Time elapsed: 00:14:01
                               ETA: 00:45:40

################################################################################
                     [1m Learning iteration 329/1400 [0m                      

                       Computation: 9556 steps/s (collection: 2.315s, learning 0.257s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 7.3080
                       Mean reward: 6.13
               Mean episode length: 525.05
Episode_Reward/track_lin_vel_xy_exp: 0.3854
Episode_Reward/track_ang_vel_z_exp: 0.2181
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0401
     Episode_Reward/dof_torques_l2: -0.0424
         Episode_Reward/dof_acc_l2: -0.0939
     Episode_Reward/action_rate_l2: -0.0449
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0919
Metrics/base_velocity/error_vel_xy: 0.3173
Metrics/base_velocity/error_vel_yaw: 0.2282
      Episode_Termination/time_out: 0.3068
  Episode_Termination/base_contact: 0.6932
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 2.57s
                      Time elapsed: 00:14:03
                               ETA: 00:45:37

################################################################################
                     [1m Learning iteration 330/1400 [0m                      

                       Computation: 9635 steps/s (collection: 2.295s, learning 0.255s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 7.2883
                       Mean reward: 6.11
               Mean episode length: 493.62
Episode_Reward/track_lin_vel_xy_exp: 0.2987
Episode_Reward/track_ang_vel_z_exp: 0.1718
       Episode_Reward/lin_vel_z_l2: -0.0263
      Episode_Reward/ang_vel_xy_l2: -0.0321
     Episode_Reward/dof_torques_l2: -0.0346
         Episode_Reward/dof_acc_l2: -0.0760
     Episode_Reward/action_rate_l2: -0.0350
      Episode_Reward/feet_air_time: -0.0051
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0958
Metrics/base_velocity/error_vel_xy: 0.2486
Metrics/base_velocity/error_vel_yaw: 0.1679
      Episode_Termination/time_out: 0.3054
  Episode_Termination/base_contact: 0.6946
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 2.55s
                      Time elapsed: 00:14:06
                               ETA: 00:45:35

################################################################################
                     [1m Learning iteration 331/1400 [0m                      

                       Computation: 9577 steps/s (collection: 2.302s, learning 0.264s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.2897
                       Mean reward: 5.22
               Mean episode length: 449.23
Episode_Reward/track_lin_vel_xy_exp: 0.3204
Episode_Reward/track_ang_vel_z_exp: 0.1877
       Episode_Reward/lin_vel_z_l2: -0.0303
      Episode_Reward/ang_vel_xy_l2: -0.0389
     Episode_Reward/dof_torques_l2: -0.0400
         Episode_Reward/dof_acc_l2: -0.0893
     Episode_Reward/action_rate_l2: -0.0399
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0939
Metrics/base_velocity/error_vel_xy: 0.3101
Metrics/base_velocity/error_vel_yaw: 0.2116
      Episode_Termination/time_out: 0.3022
  Episode_Termination/base_contact: 0.6978
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.57s
                      Time elapsed: 00:14:08
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 332/1400 [0m                      

                       Computation: 9520 steps/s (collection: 2.327s, learning 0.254s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 7.2964
                       Mean reward: 4.38
               Mean episode length: 408.46
Episode_Reward/track_lin_vel_xy_exp: 0.2733
Episode_Reward/track_ang_vel_z_exp: 0.1629
       Episode_Reward/lin_vel_z_l2: -0.0247
      Episode_Reward/ang_vel_xy_l2: -0.0298
     Episode_Reward/dof_torques_l2: -0.0338
         Episode_Reward/dof_acc_l2: -0.0762
     Episode_Reward/action_rate_l2: -0.0340
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0964
Metrics/base_velocity/error_vel_xy: 0.2727
Metrics/base_velocity/error_vel_yaw: 0.1794
      Episode_Termination/time_out: 0.2988
  Episode_Termination/base_contact: 0.7012
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 2.58s
                      Time elapsed: 00:14:11
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 333/1400 [0m                      

                       Computation: 9652 steps/s (collection: 2.290s, learning 0.256s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 7.2821
                       Mean reward: 4.97
               Mean episode length: 437.70
Episode_Reward/track_lin_vel_xy_exp: 0.2705
Episode_Reward/track_ang_vel_z_exp: 0.1619
       Episode_Reward/lin_vel_z_l2: -0.0252
      Episode_Reward/ang_vel_xy_l2: -0.0308
     Episode_Reward/dof_torques_l2: -0.0347
         Episode_Reward/dof_acc_l2: -0.0740
     Episode_Reward/action_rate_l2: -0.0343
      Episode_Reward/feet_air_time: -0.0056
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1027
Metrics/base_velocity/error_vel_xy: 0.2847
Metrics/base_velocity/error_vel_yaw: 0.1863
      Episode_Termination/time_out: 0.2895
  Episode_Termination/base_contact: 0.7105
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 2.55s
                      Time elapsed: 00:14:13
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 334/1400 [0m                      

                       Computation: 9639 steps/s (collection: 2.295s, learning 0.254s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 7.2639
                       Mean reward: 5.78
               Mean episode length: 488.22
Episode_Reward/track_lin_vel_xy_exp: 0.2979
Episode_Reward/track_ang_vel_z_exp: 0.1752
       Episode_Reward/lin_vel_z_l2: -0.0266
      Episode_Reward/ang_vel_xy_l2: -0.0331
     Episode_Reward/dof_torques_l2: -0.0355
         Episode_Reward/dof_acc_l2: -0.0769
     Episode_Reward/action_rate_l2: -0.0368
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1127
Metrics/base_velocity/error_vel_xy: 0.3052
Metrics/base_velocity/error_vel_yaw: 0.2012
      Episode_Termination/time_out: 0.2879
  Episode_Termination/base_contact: 0.7121
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 2.55s
                      Time elapsed: 00:14:16
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 335/1400 [0m                      

                       Computation: 9635 steps/s (collection: 2.292s, learning 0.258s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 7.2497
                       Mean reward: 6.34
               Mean episode length: 523.23
Episode_Reward/track_lin_vel_xy_exp: 0.4008
Episode_Reward/track_ang_vel_z_exp: 0.2236
       Episode_Reward/lin_vel_z_l2: -0.0334
      Episode_Reward/ang_vel_xy_l2: -0.0411
     Episode_Reward/dof_torques_l2: -0.0429
         Episode_Reward/dof_acc_l2: -0.0991
     Episode_Reward/action_rate_l2: -0.0464
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1132
Metrics/base_velocity/error_vel_xy: 0.3235
Metrics/base_velocity/error_vel_yaw: 0.2411
      Episode_Termination/time_out: 0.2863
  Episode_Termination/base_contact: 0.7137
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.55s
                      Time elapsed: 00:14:18
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 336/1400 [0m                      

                       Computation: 9593 steps/s (collection: 2.309s, learning 0.253s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.2383
                       Mean reward: 5.89
               Mean episode length: 492.53
Episode_Reward/track_lin_vel_xy_exp: 0.2747
Episode_Reward/track_ang_vel_z_exp: 0.1646
       Episode_Reward/lin_vel_z_l2: -0.0254
      Episode_Reward/ang_vel_xy_l2: -0.0315
     Episode_Reward/dof_torques_l2: -0.0330
         Episode_Reward/dof_acc_l2: -0.0741
     Episode_Reward/action_rate_l2: -0.0340
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1200
Metrics/base_velocity/error_vel_xy: 0.2802
Metrics/base_velocity/error_vel_yaw: 0.1790
      Episode_Termination/time_out: 0.2850
  Episode_Termination/base_contact: 0.7150
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 2.56s
                      Time elapsed: 00:14:21
                               ETA: 00:45:20

################################################################################
                     [1m Learning iteration 337/1400 [0m                      

                       Computation: 9706 steps/s (collection: 2.277s, learning 0.255s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0193
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 7.2336
                       Mean reward: 5.15
               Mean episode length: 441.15
Episode_Reward/track_lin_vel_xy_exp: 0.3886
Episode_Reward/track_ang_vel_z_exp: 0.2158
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0370
     Episode_Reward/dof_torques_l2: -0.0430
         Episode_Reward/dof_acc_l2: -0.0931
     Episode_Reward/action_rate_l2: -0.0436
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1228
Metrics/base_velocity/error_vel_xy: 0.2957
Metrics/base_velocity/error_vel_yaw: 0.2194
      Episode_Termination/time_out: 0.2900
  Episode_Termination/base_contact: 0.7100
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 2.53s
                      Time elapsed: 00:14:24
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 338/1400 [0m                      

                       Computation: 9546 steps/s (collection: 2.320s, learning 0.254s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 7.2340
                       Mean reward: 6.33
               Mean episode length: 519.29
Episode_Reward/track_lin_vel_xy_exp: 0.3676
Episode_Reward/track_ang_vel_z_exp: 0.2066
       Episode_Reward/lin_vel_z_l2: -0.0290
      Episode_Reward/ang_vel_xy_l2: -0.0361
     Episode_Reward/dof_torques_l2: -0.0405
         Episode_Reward/dof_acc_l2: -0.0891
     Episode_Reward/action_rate_l2: -0.0424
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1252
Metrics/base_velocity/error_vel_xy: 0.2962
Metrics/base_velocity/error_vel_yaw: 0.2172
      Episode_Termination/time_out: 0.2904
  Episode_Termination/base_contact: 0.7096
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 2.57s
                      Time elapsed: 00:14:26
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 339/1400 [0m                      

                       Computation: 9550 steps/s (collection: 2.317s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 7.2272
                       Mean reward: 5.71
               Mean episode length: 498.27
Episode_Reward/track_lin_vel_xy_exp: 0.3321
Episode_Reward/track_ang_vel_z_exp: 0.1951
       Episode_Reward/lin_vel_z_l2: -0.0294
      Episode_Reward/ang_vel_xy_l2: -0.0365
     Episode_Reward/dof_torques_l2: -0.0401
         Episode_Reward/dof_acc_l2: -0.0905
     Episode_Reward/action_rate_l2: -0.0413
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0259
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1267
Metrics/base_velocity/error_vel_xy: 0.3411
Metrics/base_velocity/error_vel_yaw: 0.2330
      Episode_Termination/time_out: 0.2830
  Episode_Termination/base_contact: 0.7170
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.57s
                      Time elapsed: 00:14:29
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 340/1400 [0m                      

                       Computation: 9731 steps/s (collection: 2.270s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 7.2138
                       Mean reward: 5.54
               Mean episode length: 491.64
Episode_Reward/track_lin_vel_xy_exp: 0.3276
Episode_Reward/track_ang_vel_z_exp: 0.1920
       Episode_Reward/lin_vel_z_l2: -0.0286
      Episode_Reward/ang_vel_xy_l2: -0.0372
     Episode_Reward/dof_torques_l2: -0.0386
         Episode_Reward/dof_acc_l2: -0.0850
     Episode_Reward/action_rate_l2: -0.0396
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1333
Metrics/base_velocity/error_vel_xy: 0.3173
Metrics/base_velocity/error_vel_yaw: 0.2155
      Episode_Termination/time_out: 0.2814
  Episode_Termination/base_contact: 0.7186
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 2.53s
                      Time elapsed: 00:14:31
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 341/1400 [0m                      

                       Computation: 9689 steps/s (collection: 2.275s, learning 0.262s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.1972
                       Mean reward: 5.58
               Mean episode length: 490.83
Episode_Reward/track_lin_vel_xy_exp: 0.3840
Episode_Reward/track_ang_vel_z_exp: 0.2239
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0414
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.1068
     Episode_Reward/action_rate_l2: -0.0476
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1361
Metrics/base_velocity/error_vel_xy: 0.3781
Metrics/base_velocity/error_vel_yaw: 0.2616
      Episode_Termination/time_out: 0.2831
  Episode_Termination/base_contact: 0.7169
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 2.54s
                      Time elapsed: 00:14:34
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 342/1400 [0m                      

                       Computation: 9697 steps/s (collection: 2.278s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0184
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 7.1859
                       Mean reward: 5.77
               Mean episode length: 509.21
Episode_Reward/track_lin_vel_xy_exp: 0.3948
Episode_Reward/track_ang_vel_z_exp: 0.2342
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.0481
         Episode_Reward/dof_acc_l2: -0.1033
     Episode_Reward/action_rate_l2: -0.0478
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1394
Metrics/base_velocity/error_vel_xy: 0.3811
Metrics/base_velocity/error_vel_yaw: 0.2478
      Episode_Termination/time_out: 0.2804
  Episode_Termination/base_contact: 0.7196
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 2.53s
                      Time elapsed: 00:14:36
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 343/1400 [0m                      

                       Computation: 9513 steps/s (collection: 2.328s, learning 0.255s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 7.1756
                       Mean reward: 6.53
               Mean episode length: 543.64
Episode_Reward/track_lin_vel_xy_exp: 0.4408
Episode_Reward/track_ang_vel_z_exp: 0.2466
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.1051
     Episode_Reward/action_rate_l2: -0.0513
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1436
Metrics/base_velocity/error_vel_xy: 0.3504
Metrics/base_velocity/error_vel_yaw: 0.2563
      Episode_Termination/time_out: 0.2841
  Episode_Termination/base_contact: 0.7159
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.58s
                      Time elapsed: 00:14:39
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 344/1400 [0m                      

                       Computation: 9550 steps/s (collection: 2.316s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 7.1692
                       Mean reward: 7.06
               Mean episode length: 536.26
Episode_Reward/track_lin_vel_xy_exp: 0.4182
Episode_Reward/track_ang_vel_z_exp: 0.2335
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.0388
     Episode_Reward/dof_torques_l2: -0.0423
         Episode_Reward/dof_acc_l2: -0.0953
     Episode_Reward/action_rate_l2: -0.0461
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1456
Metrics/base_velocity/error_vel_xy: 0.3029
Metrics/base_velocity/error_vel_yaw: 0.2101
      Episode_Termination/time_out: 0.2882
  Episode_Termination/base_contact: 0.7118
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 2.57s
                      Time elapsed: 00:14:41
                               ETA: 00:44:59

################################################################################
                     [1m Learning iteration 345/1400 [0m                      

                       Computation: 9419 steps/s (collection: 2.352s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 7.1731
                       Mean reward: 5.95
               Mean episode length: 486.96
Episode_Reward/track_lin_vel_xy_exp: 0.3678
Episode_Reward/track_ang_vel_z_exp: 0.2080
       Episode_Reward/lin_vel_z_l2: -0.0295
      Episode_Reward/ang_vel_xy_l2: -0.0377
     Episode_Reward/dof_torques_l2: -0.0419
         Episode_Reward/dof_acc_l2: -0.0928
     Episode_Reward/action_rate_l2: -0.0433
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1489
Metrics/base_velocity/error_vel_xy: 0.3165
Metrics/base_velocity/error_vel_yaw: 0.2280
      Episode_Termination/time_out: 0.2863
  Episode_Termination/base_contact: 0.7137
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 2.61s
                      Time elapsed: 00:14:44
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 346/1400 [0m                      

                       Computation: 9464 steps/s (collection: 2.340s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 7.1705
                       Mean reward: 6.12
               Mean episode length: 504.64
Episode_Reward/track_lin_vel_xy_exp: 0.3213
Episode_Reward/track_ang_vel_z_exp: 0.1848
       Episode_Reward/lin_vel_z_l2: -0.0275
      Episode_Reward/ang_vel_xy_l2: -0.0330
     Episode_Reward/dof_torques_l2: -0.0373
         Episode_Reward/dof_acc_l2: -0.0840
     Episode_Reward/action_rate_l2: -0.0390
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1507
Metrics/base_velocity/error_vel_xy: 0.2928
Metrics/base_velocity/error_vel_yaw: 0.2087
      Episode_Termination/time_out: 0.2893
  Episode_Termination/base_contact: 0.7107
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 2.60s
                      Time elapsed: 00:14:47
                               ETA: 00:44:54

################################################################################
                     [1m Learning iteration 347/1400 [0m                      

                       Computation: 9424 steps/s (collection: 2.352s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 7.1654
                       Mean reward: 6.67
               Mean episode length: 523.71
Episode_Reward/track_lin_vel_xy_exp: 0.3629
Episode_Reward/track_ang_vel_z_exp: 0.2079
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0377
     Episode_Reward/dof_torques_l2: -0.0416
         Episode_Reward/dof_acc_l2: -0.0922
     Episode_Reward/action_rate_l2: -0.0434
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1532
Metrics/base_velocity/error_vel_xy: 0.3248
Metrics/base_velocity/error_vel_yaw: 0.2236
      Episode_Termination/time_out: 0.2916
  Episode_Termination/base_contact: 0.7084
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.61s
                      Time elapsed: 00:14:49
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 348/1400 [0m                      

                       Computation: 9435 steps/s (collection: 2.349s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 7.1645
                       Mean reward: 5.56
               Mean episode length: 467.24
Episode_Reward/track_lin_vel_xy_exp: 0.2872
Episode_Reward/track_ang_vel_z_exp: 0.1661
       Episode_Reward/lin_vel_z_l2: -0.0279
      Episode_Reward/ang_vel_xy_l2: -0.0291
     Episode_Reward/dof_torques_l2: -0.0345
         Episode_Reward/dof_acc_l2: -0.0791
     Episode_Reward/action_rate_l2: -0.0350
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1569
Metrics/base_velocity/error_vel_xy: 0.2708
Metrics/base_velocity/error_vel_yaw: 0.1894
      Episode_Termination/time_out: 0.2939
  Episode_Termination/base_contact: 0.7061
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 2.60s
                      Time elapsed: 00:14:52
                               ETA: 00:44:49

################################################################################
                     [1m Learning iteration 349/1400 [0m                      

                       Computation: 9385 steps/s (collection: 2.360s, learning 0.258s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0189
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 7.1651
                       Mean reward: 4.73
               Mean episode length: 434.55
Episode_Reward/track_lin_vel_xy_exp: 0.3392
Episode_Reward/track_ang_vel_z_exp: 0.1972
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0353
     Episode_Reward/dof_torques_l2: -0.0411
         Episode_Reward/dof_acc_l2: -0.0875
     Episode_Reward/action_rate_l2: -0.0413
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1595
Metrics/base_velocity/error_vel_xy: 0.3237
Metrics/base_velocity/error_vel_yaw: 0.2224
      Episode_Termination/time_out: 0.2921
  Episode_Termination/base_contact: 0.7079
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 2.62s
                      Time elapsed: 00:14:54
                               ETA: 00:44:47

################################################################################
                     [1m Learning iteration 350/1400 [0m                      

                       Computation: 9438 steps/s (collection: 2.347s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 7.1584
                       Mean reward: 5.34
               Mean episode length: 483.75
Episode_Reward/track_lin_vel_xy_exp: 0.3995
Episode_Reward/track_ang_vel_z_exp: 0.2361
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0431
     Episode_Reward/dof_torques_l2: -0.0472
         Episode_Reward/dof_acc_l2: -0.1188
     Episode_Reward/action_rate_l2: -0.0485
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1631
Metrics/base_velocity/error_vel_xy: 0.3697
Metrics/base_velocity/error_vel_yaw: 0.2416
      Episode_Termination/time_out: 0.2889
  Episode_Termination/base_contact: 0.7111
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 2.60s
                      Time elapsed: 00:14:57
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 351/1400 [0m                      

                       Computation: 9409 steps/s (collection: 2.355s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.1386
                       Mean reward: 6.22
               Mean episode length: 529.98
Episode_Reward/track_lin_vel_xy_exp: 0.3645
Episode_Reward/track_ang_vel_z_exp: 0.2088
       Episode_Reward/lin_vel_z_l2: -0.0311
      Episode_Reward/ang_vel_xy_l2: -0.0379
     Episode_Reward/dof_torques_l2: -0.0407
         Episode_Reward/dof_acc_l2: -0.0945
     Episode_Reward/action_rate_l2: -0.0435
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1768
Metrics/base_velocity/error_vel_xy: 0.3268
Metrics/base_velocity/error_vel_yaw: 0.2323
      Episode_Termination/time_out: 0.2918
  Episode_Termination/base_contact: 0.7082
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.61s
                      Time elapsed: 00:15:00
                               ETA: 00:44:42

################################################################################
                     [1m Learning iteration 352/1400 [0m                      

                       Computation: 9499 steps/s (collection: 2.331s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0188
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.1283
                       Mean reward: 6.70
               Mean episode length: 527.35
Episode_Reward/track_lin_vel_xy_exp: 0.4463
Episode_Reward/track_ang_vel_z_exp: 0.2508
       Episode_Reward/lin_vel_z_l2: -0.0344
      Episode_Reward/ang_vel_xy_l2: -0.0436
     Episode_Reward/dof_torques_l2: -0.0488
         Episode_Reward/dof_acc_l2: -0.1022
     Episode_Reward/action_rate_l2: -0.0505
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1826
Metrics/base_velocity/error_vel_xy: 0.3388
Metrics/base_velocity/error_vel_yaw: 0.2392
      Episode_Termination/time_out: 0.2929
  Episode_Termination/base_contact: 0.7071
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 2.59s
                      Time elapsed: 00:15:02
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 353/1400 [0m                      

                       Computation: 9456 steps/s (collection: 2.334s, learning 0.265s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 7.1371
                       Mean reward: 7.63
               Mean episode length: 590.83
Episode_Reward/track_lin_vel_xy_exp: 0.4247
Episode_Reward/track_ang_vel_z_exp: 0.2392
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0416
     Episode_Reward/dof_torques_l2: -0.0458
         Episode_Reward/dof_acc_l2: -0.0982
     Episode_Reward/action_rate_l2: -0.0480
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1989
Metrics/base_velocity/error_vel_xy: 0.3362
Metrics/base_velocity/error_vel_yaw: 0.2386
      Episode_Termination/time_out: 0.2960
  Episode_Termination/base_contact: 0.7040
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 2.60s
                      Time elapsed: 00:15:05
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 354/1400 [0m                      

                       Computation: 9493 steps/s (collection: 2.329s, learning 0.260s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 7.1422
                       Mean reward: 7.45
               Mean episode length: 568.10
Episode_Reward/track_lin_vel_xy_exp: 0.3398
Episode_Reward/track_ang_vel_z_exp: 0.1904
       Episode_Reward/lin_vel_z_l2: -0.0253
      Episode_Reward/ang_vel_xy_l2: -0.0326
     Episode_Reward/dof_torques_l2: -0.0354
         Episode_Reward/dof_acc_l2: -0.0721
     Episode_Reward/action_rate_l2: -0.0379
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2014
Metrics/base_velocity/error_vel_xy: 0.2608
Metrics/base_velocity/error_vel_yaw: 0.1870
      Episode_Termination/time_out: 0.2950
  Episode_Termination/base_contact: 0.7050
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 2.59s
                      Time elapsed: 00:15:07
                               ETA: 00:44:35

################################################################################
                     [1m Learning iteration 355/1400 [0m                      

                       Computation: 9456 steps/s (collection: 2.342s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 7.1414
                       Mean reward: 5.96
               Mean episode length: 505.95
Episode_Reward/track_lin_vel_xy_exp: 0.3616
Episode_Reward/track_ang_vel_z_exp: 0.2137
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0400
     Episode_Reward/dof_torques_l2: -0.0460
         Episode_Reward/dof_acc_l2: -0.1056
     Episode_Reward/action_rate_l2: -0.0456
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2087
Metrics/base_velocity/error_vel_xy: 0.3759
Metrics/base_velocity/error_vel_yaw: 0.2554
      Episode_Termination/time_out: 0.2928
  Episode_Termination/base_contact: 0.7072
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 2.60s
                      Time elapsed: 00:15:10
                               ETA: 00:44:32

################################################################################
                     [1m Learning iteration 356/1400 [0m                      

                       Computation: 9572 steps/s (collection: 2.312s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.1540
                       Mean reward: 6.00
               Mean episode length: 528.59
Episode_Reward/track_lin_vel_xy_exp: 0.3480
Episode_Reward/track_ang_vel_z_exp: 0.1948
       Episode_Reward/lin_vel_z_l2: -0.0318
      Episode_Reward/ang_vel_xy_l2: -0.0366
     Episode_Reward/dof_torques_l2: -0.0397
         Episode_Reward/dof_acc_l2: -0.0893
     Episode_Reward/action_rate_l2: -0.0409
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0066
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2172
Metrics/base_velocity/error_vel_xy: 0.2967
Metrics/base_velocity/error_vel_yaw: 0.2232
      Episode_Termination/time_out: 0.2924
  Episode_Termination/base_contact: 0.7076
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 2.57s
                      Time elapsed: 00:15:13
                               ETA: 00:44:30

################################################################################
                     [1m Learning iteration 357/1400 [0m                      

                       Computation: 9634 steps/s (collection: 2.294s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0176
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 7.1617
                       Mean reward: 6.01
               Mean episode length: 514.52
Episode_Reward/track_lin_vel_xy_exp: 0.2765
Episode_Reward/track_ang_vel_z_exp: 0.1620
       Episode_Reward/lin_vel_z_l2: -0.0262
      Episode_Reward/ang_vel_xy_l2: -0.0305
     Episode_Reward/dof_torques_l2: -0.0320
         Episode_Reward/dof_acc_l2: -0.0752
     Episode_Reward/action_rate_l2: -0.0338
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2196
Metrics/base_velocity/error_vel_xy: 0.2822
Metrics/base_velocity/error_vel_yaw: 0.1947
      Episode_Termination/time_out: 0.2959
  Episode_Termination/base_contact: 0.7041
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 2.55s
                      Time elapsed: 00:15:15
                               ETA: 00:44:27

################################################################################
                     [1m Learning iteration 358/1400 [0m                      

                       Computation: 9713 steps/s (collection: 2.276s, learning 0.254s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 7.1669
                       Mean reward: 5.62
               Mean episode length: 486.93
Episode_Reward/track_lin_vel_xy_exp: 0.3112
Episode_Reward/track_ang_vel_z_exp: 0.1964
       Episode_Reward/lin_vel_z_l2: -0.0278
      Episode_Reward/ang_vel_xy_l2: -0.0348
     Episode_Reward/dof_torques_l2: -0.0392
         Episode_Reward/dof_acc_l2: -0.0884
     Episode_Reward/action_rate_l2: -0.0405
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2234
Metrics/base_velocity/error_vel_xy: 0.3520
Metrics/base_velocity/error_vel_yaw: 0.2111
      Episode_Termination/time_out: 0.2979
  Episode_Termination/base_contact: 0.7021
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 2.53s
                      Time elapsed: 00:15:18
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 359/1400 [0m                      

                       Computation: 9551 steps/s (collection: 2.319s, learning 0.254s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 7.1734
                       Mean reward: 6.31
               Mean episode length: 531.14
Episode_Reward/track_lin_vel_xy_exp: 0.3993
Episode_Reward/track_ang_vel_z_exp: 0.2325
       Episode_Reward/lin_vel_z_l2: -0.0327
      Episode_Reward/ang_vel_xy_l2: -0.0395
     Episode_Reward/dof_torques_l2: -0.0453
         Episode_Reward/dof_acc_l2: -0.0979
     Episode_Reward/action_rate_l2: -0.0465
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2317
Metrics/base_velocity/error_vel_xy: 0.3512
Metrics/base_velocity/error_vel_yaw: 0.2354
      Episode_Termination/time_out: 0.2944
  Episode_Termination/base_contact: 0.7056
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.57s
                      Time elapsed: 00:15:20
                               ETA: 00:44:22

################################################################################
                     [1m Learning iteration 360/1400 [0m                      

                       Computation: 9764 steps/s (collection: 2.261s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.1695
                       Mean reward: 6.24
               Mean episode length: 522.33
Episode_Reward/track_lin_vel_xy_exp: 0.3285
Episode_Reward/track_ang_vel_z_exp: 0.1858
       Episode_Reward/lin_vel_z_l2: -0.0279
      Episode_Reward/ang_vel_xy_l2: -0.0344
     Episode_Reward/dof_torques_l2: -0.0358
         Episode_Reward/dof_acc_l2: -0.0855
     Episode_Reward/action_rate_l2: -0.0389
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2335
Metrics/base_velocity/error_vel_xy: 0.3003
Metrics/base_velocity/error_vel_yaw: 0.2109
      Episode_Termination/time_out: 0.2914
  Episode_Termination/base_contact: 0.7086
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 2.52s
                      Time elapsed: 00:15:23
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 361/1400 [0m                      

                       Computation: 9732 steps/s (collection: 2.263s, learning 0.262s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 7.1553
                       Mean reward: 5.89
               Mean episode length: 501.35
Episode_Reward/track_lin_vel_xy_exp: 0.3361
Episode_Reward/track_ang_vel_z_exp: 0.2004
       Episode_Reward/lin_vel_z_l2: -0.0311
      Episode_Reward/ang_vel_xy_l2: -0.0371
     Episode_Reward/dof_torques_l2: -0.0411
         Episode_Reward/dof_acc_l2: -0.0935
     Episode_Reward/action_rate_l2: -0.0422
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2327
Metrics/base_velocity/error_vel_xy: 0.3382
Metrics/base_velocity/error_vel_yaw: 0.2206
      Episode_Termination/time_out: 0.2901
  Episode_Termination/base_contact: 0.7099
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 2.53s
                      Time elapsed: 00:15:25
                               ETA: 00:44:17

################################################################################
                     [1m Learning iteration 362/1400 [0m                      

                       Computation: 9644 steps/s (collection: 2.293s, learning 0.255s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 7.1365
                       Mean reward: 6.30
               Mean episode length: 503.05
Episode_Reward/track_lin_vel_xy_exp: 0.3505
Episode_Reward/track_ang_vel_z_exp: 0.1976
       Episode_Reward/lin_vel_z_l2: -0.0293
      Episode_Reward/ang_vel_xy_l2: -0.0335
     Episode_Reward/dof_torques_l2: -0.0379
         Episode_Reward/dof_acc_l2: -0.0833
     Episode_Reward/action_rate_l2: -0.0399
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2314
Metrics/base_velocity/error_vel_xy: 0.2903
Metrics/base_velocity/error_vel_yaw: 0.2097
      Episode_Termination/time_out: 0.2879
  Episode_Termination/base_contact: 0.7121
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 2.55s
                      Time elapsed: 00:15:28
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 363/1400 [0m                      

                       Computation: 9616 steps/s (collection: 2.301s, learning 0.255s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 7.1165
                       Mean reward: 6.32
               Mean episode length: 506.54
Episode_Reward/track_lin_vel_xy_exp: 0.3572
Episode_Reward/track_ang_vel_z_exp: 0.2053
       Episode_Reward/lin_vel_z_l2: -0.0311
      Episode_Reward/ang_vel_xy_l2: -0.0370
     Episode_Reward/dof_torques_l2: -0.0432
         Episode_Reward/dof_acc_l2: -0.0889
     Episode_Reward/action_rate_l2: -0.0422
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2411
Metrics/base_velocity/error_vel_xy: 0.3275
Metrics/base_velocity/error_vel_yaw: 0.2273
      Episode_Termination/time_out: 0.2882
  Episode_Termination/base_contact: 0.7118
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.56s
                      Time elapsed: 00:15:30
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 364/1400 [0m                      

                       Computation: 9732 steps/s (collection: 2.267s, learning 0.258s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 7.1014
                       Mean reward: 6.57
               Mean episode length: 518.78
Episode_Reward/track_lin_vel_xy_exp: 0.3582
Episode_Reward/track_ang_vel_z_exp: 0.1998
       Episode_Reward/lin_vel_z_l2: -0.0287
      Episode_Reward/ang_vel_xy_l2: -0.0360
     Episode_Reward/dof_torques_l2: -0.0385
         Episode_Reward/dof_acc_l2: -0.0817
     Episode_Reward/action_rate_l2: -0.0393
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2513
Metrics/base_velocity/error_vel_xy: 0.2762
Metrics/base_velocity/error_vel_yaw: 0.2006
      Episode_Termination/time_out: 0.2874
  Episode_Termination/base_contact: 0.7126
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 2.53s
                      Time elapsed: 00:15:33
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 365/1400 [0m                      

                       Computation: 9577 steps/s (collection: 2.311s, learning 0.255s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 7.0931
                       Mean reward: 6.91
               Mean episode length: 551.35
Episode_Reward/track_lin_vel_xy_exp: 0.4269
Episode_Reward/track_ang_vel_z_exp: 0.2372
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0414
     Episode_Reward/dof_torques_l2: -0.0443
         Episode_Reward/dof_acc_l2: -0.1006
     Episode_Reward/action_rate_l2: -0.0478
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2611
Metrics/base_velocity/error_vel_xy: 0.3358
Metrics/base_velocity/error_vel_yaw: 0.2475
      Episode_Termination/time_out: 0.2926
  Episode_Termination/base_contact: 0.7074
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 2.57s
                      Time elapsed: 00:15:36
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 366/1400 [0m                      

                       Computation: 9772 steps/s (collection: 2.258s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 7.0901
                       Mean reward: 6.88
               Mean episode length: 540.21
Episode_Reward/track_lin_vel_xy_exp: 0.3656
Episode_Reward/track_ang_vel_z_exp: 0.2039
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0355
     Episode_Reward/dof_torques_l2: -0.0394
         Episode_Reward/dof_acc_l2: -0.0913
     Episode_Reward/action_rate_l2: -0.0413
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2733
Metrics/base_velocity/error_vel_xy: 0.2853
Metrics/base_velocity/error_vel_yaw: 0.2079
      Episode_Termination/time_out: 0.2973
  Episode_Termination/base_contact: 0.7027
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 2.51s
                      Time elapsed: 00:15:38
                               ETA: 00:44:04

################################################################################
                     [1m Learning iteration 367/1400 [0m                      

                       Computation: 9677 steps/s (collection: 2.283s, learning 0.256s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 7.0740
                       Mean reward: 6.08
               Mean episode length: 513.69
Episode_Reward/track_lin_vel_xy_exp: 0.3100
Episode_Reward/track_ang_vel_z_exp: 0.1829
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0336
     Episode_Reward/dof_torques_l2: -0.0384
         Episode_Reward/dof_acc_l2: -0.0823
     Episode_Reward/action_rate_l2: -0.0377
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2803
Metrics/base_velocity/error_vel_xy: 0.3121
Metrics/base_velocity/error_vel_yaw: 0.2116
      Episode_Termination/time_out: 0.2954
  Episode_Termination/base_contact: 0.7046
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.54s
                      Time elapsed: 00:15:41
                               ETA: 00:44:01

################################################################################
                     [1m Learning iteration 368/1400 [0m                      

                       Computation: 9608 steps/s (collection: 2.302s, learning 0.255s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 7.0605
                       Mean reward: 6.74
               Mean episode length: 553.02
Episode_Reward/track_lin_vel_xy_exp: 0.4640
Episode_Reward/track_ang_vel_z_exp: 0.2575
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0444
     Episode_Reward/dof_torques_l2: -0.0503
         Episode_Reward/dof_acc_l2: -0.1129
     Episode_Reward/action_rate_l2: -0.0522
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2877
Metrics/base_velocity/error_vel_xy: 0.3528
Metrics/base_velocity/error_vel_yaw: 0.2602
      Episode_Termination/time_out: 0.2926
  Episode_Termination/base_contact: 0.7074
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 2.56s
                      Time elapsed: 00:15:43
                               ETA: 00:43:59

################################################################################
                     [1m Learning iteration 369/1400 [0m                      

                       Computation: 9733 steps/s (collection: 2.268s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 7.0513
                       Mean reward: 7.09
               Mean episode length: 569.47
Episode_Reward/track_lin_vel_xy_exp: 0.3804
Episode_Reward/track_ang_vel_z_exp: 0.2203
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0423
     Episode_Reward/dof_torques_l2: -0.0441
         Episode_Reward/dof_acc_l2: -0.1030
     Episode_Reward/action_rate_l2: -0.0454
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2953
Metrics/base_velocity/error_vel_xy: 0.3666
Metrics/base_velocity/error_vel_yaw: 0.2502
      Episode_Termination/time_out: 0.2949
  Episode_Termination/base_contact: 0.7051
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 2.53s
                      Time elapsed: 00:15:46
                               ETA: 00:43:56

################################################################################
                     [1m Learning iteration 370/1400 [0m                      

                       Computation: 9738 steps/s (collection: 2.265s, learning 0.259s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 7.0281
                       Mean reward: 7.11
               Mean episode length: 577.97
Episode_Reward/track_lin_vel_xy_exp: 0.3983
Episode_Reward/track_ang_vel_z_exp: 0.2213
       Episode_Reward/lin_vel_z_l2: -0.0323
      Episode_Reward/ang_vel_xy_l2: -0.0383
     Episode_Reward/dof_torques_l2: -0.0431
         Episode_Reward/dof_acc_l2: -0.1035
     Episode_Reward/action_rate_l2: -0.0451
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3074
Metrics/base_velocity/error_vel_xy: 0.3197
Metrics/base_velocity/error_vel_yaw: 0.2330
      Episode_Termination/time_out: 0.2974
  Episode_Termination/base_contact: 0.7026
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 2.52s
                      Time elapsed: 00:15:48
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 371/1400 [0m                      

                       Computation: 9576 steps/s (collection: 2.308s, learning 0.258s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 7.0205
                       Mean reward: 6.84
               Mean episode length: 560.78
Episode_Reward/track_lin_vel_xy_exp: 0.3494
Episode_Reward/track_ang_vel_z_exp: 0.2051
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.0376
     Episode_Reward/dof_torques_l2: -0.0416
         Episode_Reward/dof_acc_l2: -0.0958
     Episode_Reward/action_rate_l2: -0.0418
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3195
Metrics/base_velocity/error_vel_xy: 0.3285
Metrics/base_velocity/error_vel_yaw: 0.2214
      Episode_Termination/time_out: 0.3013
  Episode_Termination/base_contact: 0.6987
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.57s
                      Time elapsed: 00:15:51
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 372/1400 [0m                      

                       Computation: 9526 steps/s (collection: 2.323s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0186
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 7.0188
                       Mean reward: 6.60
               Mean episode length: 553.28
Episode_Reward/track_lin_vel_xy_exp: 0.3737
Episode_Reward/track_ang_vel_z_exp: 0.2174
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0405
     Episode_Reward/dof_torques_l2: -0.0449
         Episode_Reward/dof_acc_l2: -0.1013
     Episode_Reward/action_rate_l2: -0.0453
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3305
Metrics/base_velocity/error_vel_xy: 0.3674
Metrics/base_velocity/error_vel_yaw: 0.2518
      Episode_Termination/time_out: 0.3046
  Episode_Termination/base_contact: 0.6954
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 2.58s
                      Time elapsed: 00:15:53
                               ETA: 00:43:48

################################################################################
                     [1m Learning iteration 373/1400 [0m                      

                       Computation: 9643 steps/s (collection: 2.287s, learning 0.262s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 7.0305
                       Mean reward: 6.32
               Mean episode length: 534.46
Episode_Reward/track_lin_vel_xy_exp: 0.3365
Episode_Reward/track_ang_vel_z_exp: 0.1932
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0336
     Episode_Reward/dof_torques_l2: -0.0403
         Episode_Reward/dof_acc_l2: -0.0867
     Episode_Reward/action_rate_l2: -0.0401
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3402
Metrics/base_velocity/error_vel_xy: 0.3054
Metrics/base_velocity/error_vel_yaw: 0.2137
      Episode_Termination/time_out: 0.3073
  Episode_Termination/base_contact: 0.6927
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 2.55s
                      Time elapsed: 00:15:56
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 374/1400 [0m                      

                       Computation: 9642 steps/s (collection: 2.294s, learning 0.255s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 7.0492
                       Mean reward: 6.38
               Mean episode length: 528.13
Episode_Reward/track_lin_vel_xy_exp: 0.3582
Episode_Reward/track_ang_vel_z_exp: 0.2067
       Episode_Reward/lin_vel_z_l2: -0.0314
      Episode_Reward/ang_vel_xy_l2: -0.0365
     Episode_Reward/dof_torques_l2: -0.0408
         Episode_Reward/dof_acc_l2: -0.0936
     Episode_Reward/action_rate_l2: -0.0426
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3477
Metrics/base_velocity/error_vel_xy: 0.3269
Metrics/base_velocity/error_vel_yaw: 0.2255
      Episode_Termination/time_out: 0.3079
  Episode_Termination/base_contact: 0.6921
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 2.55s
                      Time elapsed: 00:15:58
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 375/1400 [0m                      

                       Computation: 9650 steps/s (collection: 2.283s, learning 0.264s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 7.0547
                       Mean reward: 7.02
               Mean episode length: 573.10
Episode_Reward/track_lin_vel_xy_exp: 0.4258
Episode_Reward/track_ang_vel_z_exp: 0.2338
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0426
     Episode_Reward/dof_torques_l2: -0.0478
         Episode_Reward/dof_acc_l2: -0.1110
     Episode_Reward/action_rate_l2: -0.0492
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3600
Metrics/base_velocity/error_vel_xy: 0.3486
Metrics/base_velocity/error_vel_yaw: 0.2762
      Episode_Termination/time_out: 0.3092
  Episode_Termination/base_contact: 0.6908
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.55s
                      Time elapsed: 00:16:01
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 376/1400 [0m                      

                       Computation: 9448 steps/s (collection: 2.341s, learning 0.260s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 7.0495
                       Mean reward: 7.24
               Mean episode length: 570.18
Episode_Reward/track_lin_vel_xy_exp: 0.3461
Episode_Reward/track_ang_vel_z_exp: 0.1924
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0338
     Episode_Reward/dof_torques_l2: -0.0385
         Episode_Reward/dof_acc_l2: -0.0893
     Episode_Reward/action_rate_l2: -0.0396
      Episode_Reward/feet_air_time: -0.0058
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3722
Metrics/base_velocity/error_vel_xy: 0.2753
Metrics/base_velocity/error_vel_yaw: 0.2042
      Episode_Termination/time_out: 0.3127
  Episode_Termination/base_contact: 0.6873
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 2.60s
                      Time elapsed: 00:16:04
                               ETA: 00:43:38

################################################################################
                     [1m Learning iteration 377/1400 [0m                      

                       Computation: 9661 steps/s (collection: 2.286s, learning 0.258s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 7.0509
                       Mean reward: 7.13
               Mean episode length: 556.05
Episode_Reward/track_lin_vel_xy_exp: 0.3786
Episode_Reward/track_ang_vel_z_exp: 0.2087
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0341
     Episode_Reward/dof_torques_l2: -0.0413
         Episode_Reward/dof_acc_l2: -0.0816
     Episode_Reward/action_rate_l2: -0.0428
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0142
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3767
Metrics/base_velocity/error_vel_xy: 0.3092
Metrics/base_velocity/error_vel_yaw: 0.2349
      Episode_Termination/time_out: 0.3107
  Episode_Termination/base_contact: 0.6893
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 2.54s
                      Time elapsed: 00:16:06
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 378/1400 [0m                      

                       Computation: 9478 steps/s (collection: 2.336s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 7.0571
                       Mean reward: 6.68
               Mean episode length: 521.02
Episode_Reward/track_lin_vel_xy_exp: 0.3879
Episode_Reward/track_ang_vel_z_exp: 0.2152
       Episode_Reward/lin_vel_z_l2: -0.0293
      Episode_Reward/ang_vel_xy_l2: -0.0373
     Episode_Reward/dof_torques_l2: -0.0431
         Episode_Reward/dof_acc_l2: -0.0920
     Episode_Reward/action_rate_l2: -0.0428
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3842
Metrics/base_velocity/error_vel_xy: 0.3077
Metrics/base_velocity/error_vel_yaw: 0.2247
      Episode_Termination/time_out: 0.3119
  Episode_Termination/base_contact: 0.6881
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 2.59s
                      Time elapsed: 00:16:09
                               ETA: 00:43:33

################################################################################
                     [1m Learning iteration 379/1400 [0m                      

                       Computation: 9475 steps/s (collection: 2.335s, learning 0.258s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 7.0538
                       Mean reward: 6.89
               Mean episode length: 553.39
Episode_Reward/track_lin_vel_xy_exp: 0.3656
Episode_Reward/track_ang_vel_z_exp: 0.2126
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0389
     Episode_Reward/dof_torques_l2: -0.0437
         Episode_Reward/dof_acc_l2: -0.1006
     Episode_Reward/action_rate_l2: -0.0443
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3937
Metrics/base_velocity/error_vel_xy: 0.3503
Metrics/base_velocity/error_vel_yaw: 0.2382
      Episode_Termination/time_out: 0.3141
  Episode_Termination/base_contact: 0.6859
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.59s
                      Time elapsed: 00:16:11
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 380/1400 [0m                      

                       Computation: 9411 steps/s (collection: 2.355s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.0450
                       Mean reward: 7.44
               Mean episode length: 590.98
Episode_Reward/track_lin_vel_xy_exp: 0.4358
Episode_Reward/track_ang_vel_z_exp: 0.2479
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0487
         Episode_Reward/dof_acc_l2: -0.1087
     Episode_Reward/action_rate_l2: -0.0497
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4022
Metrics/base_velocity/error_vel_xy: 0.3642
Metrics/base_velocity/error_vel_yaw: 0.2548
      Episode_Termination/time_out: 0.3188
  Episode_Termination/base_contact: 0.6812
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 2.61s
                      Time elapsed: 00:16:14
                               ETA: 00:43:28

################################################################################
                     [1m Learning iteration 381/1400 [0m                      

                       Computation: 9366 steps/s (collection: 2.367s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 7.0406
                       Mean reward: 7.23
               Mean episode length: 567.86
Episode_Reward/track_lin_vel_xy_exp: 0.3525
Episode_Reward/track_ang_vel_z_exp: 0.1959
       Episode_Reward/lin_vel_z_l2: -0.0280
      Episode_Reward/ang_vel_xy_l2: -0.0343
     Episode_Reward/dof_torques_l2: -0.0384
         Episode_Reward/dof_acc_l2: -0.0842
     Episode_Reward/action_rate_l2: -0.0397
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4125
Metrics/base_velocity/error_vel_xy: 0.2782
Metrics/base_velocity/error_vel_yaw: 0.2086
      Episode_Termination/time_out: 0.3255
  Episode_Termination/base_contact: 0.6745
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 2.62s
                      Time elapsed: 00:16:17
                               ETA: 00:43:26

################################################################################
                     [1m Learning iteration 382/1400 [0m                      

                       Computation: 9548 steps/s (collection: 2.315s, learning 0.259s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 7.0461
                       Mean reward: 6.94
               Mean episode length: 556.87
Episode_Reward/track_lin_vel_xy_exp: 0.3955
Episode_Reward/track_ang_vel_z_exp: 0.2323
       Episode_Reward/lin_vel_z_l2: -0.0328
      Episode_Reward/ang_vel_xy_l2: -0.0425
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.1046
     Episode_Reward/action_rate_l2: -0.0474
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4130
Metrics/base_velocity/error_vel_xy: 0.3695
Metrics/base_velocity/error_vel_yaw: 0.2531
      Episode_Termination/time_out: 0.3305
  Episode_Termination/base_contact: 0.6695
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 2.57s
                      Time elapsed: 00:16:19
                               ETA: 00:43:23

################################################################################
                     [1m Learning iteration 383/1400 [0m                      

                       Computation: 9496 steps/s (collection: 2.333s, learning 0.255s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 7.0455
                       Mean reward: 6.27
               Mean episode length: 545.86
Episode_Reward/track_lin_vel_xy_exp: 0.3651
Episode_Reward/track_ang_vel_z_exp: 0.2200
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0405
     Episode_Reward/dof_torques_l2: -0.0456
         Episode_Reward/dof_acc_l2: -0.1020
     Episode_Reward/action_rate_l2: -0.0452
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4197
Metrics/base_velocity/error_vel_xy: 0.3899
Metrics/base_velocity/error_vel_yaw: 0.2569
      Episode_Termination/time_out: 0.3302
  Episode_Termination/base_contact: 0.6698
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.59s
                      Time elapsed: 00:16:22
                               ETA: 00:43:21

################################################################################
                     [1m Learning iteration 384/1400 [0m                      

                       Computation: 9638 steps/s (collection: 2.293s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 7.0446
                       Mean reward: 6.28
               Mean episode length: 563.15
Episode_Reward/track_lin_vel_xy_exp: 0.4111
Episode_Reward/track_ang_vel_z_exp: 0.2340
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0420
     Episode_Reward/dof_torques_l2: -0.0496
         Episode_Reward/dof_acc_l2: -0.1068
     Episode_Reward/action_rate_l2: -0.0487
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4307
Metrics/base_velocity/error_vel_xy: 0.3634
Metrics/base_velocity/error_vel_yaw: 0.2623
      Episode_Termination/time_out: 0.3330
  Episode_Termination/base_contact: 0.6670
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 2.55s
                      Time elapsed: 00:16:24
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 385/1400 [0m                      

                       Computation: 9673 steps/s (collection: 2.286s, learning 0.255s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 7.0348
                       Mean reward: 6.62
               Mean episode length: 567.34
Episode_Reward/track_lin_vel_xy_exp: 0.3452
Episode_Reward/track_ang_vel_z_exp: 0.2012
       Episode_Reward/lin_vel_z_l2: -0.0293
      Episode_Reward/ang_vel_xy_l2: -0.0368
     Episode_Reward/dof_torques_l2: -0.0423
         Episode_Reward/dof_acc_l2: -0.0914
     Episode_Reward/action_rate_l2: -0.0420
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4385
Metrics/base_velocity/error_vel_xy: 0.3411
Metrics/base_velocity/error_vel_yaw: 0.2359
      Episode_Termination/time_out: 0.3368
  Episode_Termination/base_contact: 0.6632
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 2.54s
                      Time elapsed: 00:16:27
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 386/1400 [0m                      

                       Computation: 9632 steps/s (collection: 2.294s, learning 0.257s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 7.0217
                       Mean reward: 6.60
               Mean episode length: 551.62
Episode_Reward/track_lin_vel_xy_exp: 0.4168
Episode_Reward/track_ang_vel_z_exp: 0.2267
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0408
     Episode_Reward/dof_torques_l2: -0.0463
         Episode_Reward/dof_acc_l2: -0.1007
     Episode_Reward/action_rate_l2: -0.0461
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4517
Metrics/base_velocity/error_vel_xy: 0.3120
Metrics/base_velocity/error_vel_yaw: 0.2414
      Episode_Termination/time_out: 0.3407
  Episode_Termination/base_contact: 0.6593
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 2.55s
                      Time elapsed: 00:16:29
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 387/1400 [0m                      

                       Computation: 9709 steps/s (collection: 2.275s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.9976
                       Mean reward: 6.81
               Mean episode length: 559.99
Episode_Reward/track_lin_vel_xy_exp: 0.4230
Episode_Reward/track_ang_vel_z_exp: 0.2398
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0420
     Episode_Reward/dof_torques_l2: -0.0494
         Episode_Reward/dof_acc_l2: -0.1025
     Episode_Reward/action_rate_l2: -0.0491
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4576
Metrics/base_velocity/error_vel_xy: 0.3500
Metrics/base_velocity/error_vel_yaw: 0.2542
      Episode_Termination/time_out: 0.3431
  Episode_Termination/base_contact: 0.6569
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.53s
                      Time elapsed: 00:16:32
                               ETA: 00:43:10

################################################################################
                     [1m Learning iteration 388/1400 [0m                      

                       Computation: 9793 steps/s (collection: 2.244s, learning 0.265s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.9759
                       Mean reward: 7.15
               Mean episode length: 572.33
Episode_Reward/track_lin_vel_xy_exp: 0.4164
Episode_Reward/track_ang_vel_z_exp: 0.2315
       Episode_Reward/lin_vel_z_l2: -0.0305
      Episode_Reward/ang_vel_xy_l2: -0.0395
     Episode_Reward/dof_torques_l2: -0.0436
         Episode_Reward/dof_acc_l2: -0.0989
     Episode_Reward/action_rate_l2: -0.0461
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4637
Metrics/base_velocity/error_vel_xy: 0.3162
Metrics/base_velocity/error_vel_yaw: 0.2347
      Episode_Termination/time_out: 0.3392
  Episode_Termination/base_contact: 0.6608
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 2.51s
                      Time elapsed: 00:16:34
                               ETA: 00:43:08

################################################################################
                     [1m Learning iteration 389/1400 [0m                      

                       Computation: 9705 steps/s (collection: 2.268s, learning 0.264s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.9764
                       Mean reward: 6.43
               Mean episode length: 535.29
Episode_Reward/track_lin_vel_xy_exp: 0.3579
Episode_Reward/track_ang_vel_z_exp: 0.2036
       Episode_Reward/lin_vel_z_l2: -0.0299
      Episode_Reward/ang_vel_xy_l2: -0.0361
     Episode_Reward/dof_torques_l2: -0.0421
         Episode_Reward/dof_acc_l2: -0.0907
     Episode_Reward/action_rate_l2: -0.0418
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4728
Metrics/base_velocity/error_vel_xy: 0.3153
Metrics/base_velocity/error_vel_yaw: 0.2213
      Episode_Termination/time_out: 0.3384
  Episode_Termination/base_contact: 0.6616
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 2.53s
                      Time elapsed: 00:16:37
                               ETA: 00:43:05

################################################################################
                     [1m Learning iteration 390/1400 [0m                      

                       Computation: 9711 steps/s (collection: 2.274s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.9706
                       Mean reward: 6.96
               Mean episode length: 563.97
Episode_Reward/track_lin_vel_xy_exp: 0.4259
Episode_Reward/track_ang_vel_z_exp: 0.2398
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0412
     Episode_Reward/dof_torques_l2: -0.0468
         Episode_Reward/dof_acc_l2: -0.1041
     Episode_Reward/action_rate_l2: -0.0488
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4841
Metrics/base_velocity/error_vel_xy: 0.3510
Metrics/base_velocity/error_vel_yaw: 0.2514
      Episode_Termination/time_out: 0.3401
  Episode_Termination/base_contact: 0.6599
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 2.53s
                      Time elapsed: 00:16:39
                               ETA: 00:43:03

################################################################################
                     [1m Learning iteration 391/1400 [0m                      

                       Computation: 9567 steps/s (collection: 2.311s, learning 0.258s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.9764
                       Mean reward: 6.95
               Mean episode length: 561.93
Episode_Reward/track_lin_vel_xy_exp: 0.4505
Episode_Reward/track_ang_vel_z_exp: 0.2514
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0421
     Episode_Reward/dof_torques_l2: -0.0475
         Episode_Reward/dof_acc_l2: -0.1079
     Episode_Reward/action_rate_l2: -0.0513
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4921
Metrics/base_velocity/error_vel_xy: 0.3597
Metrics/base_velocity/error_vel_yaw: 0.2622
      Episode_Termination/time_out: 0.3416
  Episode_Termination/base_contact: 0.6584
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 2.57s
                      Time elapsed: 00:16:42
                               ETA: 00:43:00

################################################################################
                     [1m Learning iteration 392/1400 [0m                      

                       Computation: 9465 steps/s (collection: 2.337s, learning 0.259s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0184
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.9885
                       Mean reward: 6.96
               Mean episode length: 587.30
Episode_Reward/track_lin_vel_xy_exp: 0.4201
Episode_Reward/track_ang_vel_z_exp: 0.2463
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0425
     Episode_Reward/dof_torques_l2: -0.0499
         Episode_Reward/dof_acc_l2: -0.1083
     Episode_Reward/action_rate_l2: -0.0508
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0138
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4916
Metrics/base_velocity/error_vel_xy: 0.4108
Metrics/base_velocity/error_vel_yaw: 0.2829
      Episode_Termination/time_out: 0.3439
  Episode_Termination/base_contact: 0.6561
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 2.60s
                      Time elapsed: 00:16:45
                               ETA: 00:42:58

################################################################################
                     [1m Learning iteration 393/1400 [0m                      

                       Computation: 9573 steps/s (collection: 2.312s, learning 0.255s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 6.9784
                       Mean reward: 7.16
               Mean episode length: 592.56
Episode_Reward/track_lin_vel_xy_exp: 0.4427
Episode_Reward/track_ang_vel_z_exp: 0.2474
       Episode_Reward/lin_vel_z_l2: -0.0338
      Episode_Reward/ang_vel_xy_l2: -0.0427
     Episode_Reward/dof_torques_l2: -0.0501
         Episode_Reward/dof_acc_l2: -0.1060
     Episode_Reward/action_rate_l2: -0.0502
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5009
Metrics/base_velocity/error_vel_xy: 0.3599
Metrics/base_velocity/error_vel_yaw: 0.2634
      Episode_Termination/time_out: 0.3447
  Episode_Termination/base_contact: 0.6553
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 2.57s
                      Time elapsed: 00:16:47
                               ETA: 00:42:55

################################################################################
                     [1m Learning iteration 394/1400 [0m                      

                       Computation: 9545 steps/s (collection: 2.318s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.9663
                       Mean reward: 7.81
               Mean episode length: 599.91
Episode_Reward/track_lin_vel_xy_exp: 0.4708
Episode_Reward/track_ang_vel_z_exp: 0.2502
       Episode_Reward/lin_vel_z_l2: -0.0324
      Episode_Reward/ang_vel_xy_l2: -0.0421
     Episode_Reward/dof_torques_l2: -0.0459
         Episode_Reward/dof_acc_l2: -0.0975
     Episode_Reward/action_rate_l2: -0.0498
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5123
Metrics/base_velocity/error_vel_xy: 0.3097
Metrics/base_velocity/error_vel_yaw: 0.2462
      Episode_Termination/time_out: 0.3472
  Episode_Termination/base_contact: 0.6528
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 2.57s
                      Time elapsed: 00:16:50
                               ETA: 00:42:53

################################################################################
                     [1m Learning iteration 395/1400 [0m                      

                       Computation: 9559 steps/s (collection: 2.320s, learning 0.251s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.9657
                       Mean reward: 7.54
               Mean episode length: 568.09
Episode_Reward/track_lin_vel_xy_exp: 0.3685
Episode_Reward/track_ang_vel_z_exp: 0.2042
       Episode_Reward/lin_vel_z_l2: -0.0316
      Episode_Reward/ang_vel_xy_l2: -0.0385
     Episode_Reward/dof_torques_l2: -0.0398
         Episode_Reward/dof_acc_l2: -0.0909
     Episode_Reward/action_rate_l2: -0.0412
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5210
Metrics/base_velocity/error_vel_xy: 0.2950
Metrics/base_velocity/error_vel_yaw: 0.2179
      Episode_Termination/time_out: 0.3472
  Episode_Termination/base_contact: 0.6528
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 2.57s
                      Time elapsed: 00:16:52
                               ETA: 00:42:50

################################################################################
                     [1m Learning iteration 396/1400 [0m                      

                       Computation: 9823 steps/s (collection: 2.251s, learning 0.251s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.9527
                       Mean reward: 6.47
               Mean episode length: 529.08
Episode_Reward/track_lin_vel_xy_exp: 0.3451
Episode_Reward/track_ang_vel_z_exp: 0.2145
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0400
     Episode_Reward/dof_torques_l2: -0.0456
         Episode_Reward/dof_acc_l2: -0.1069
     Episode_Reward/action_rate_l2: -0.0458
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5289
Metrics/base_velocity/error_vel_xy: 0.4004
Metrics/base_velocity/error_vel_yaw: 0.2534
      Episode_Termination/time_out: 0.3432
  Episode_Termination/base_contact: 0.6568
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 2.50s
                      Time elapsed: 00:16:55
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 397/1400 [0m                      

                       Computation: 9643 steps/s (collection: 2.292s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 6.9459
                       Mean reward: 6.30
               Mean episode length: 529.74
Episode_Reward/track_lin_vel_xy_exp: 0.4015
Episode_Reward/track_ang_vel_z_exp: 0.2323
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0423
     Episode_Reward/dof_torques_l2: -0.0491
         Episode_Reward/dof_acc_l2: -0.1065
     Episode_Reward/action_rate_l2: -0.0486
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0067
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5378
Metrics/base_velocity/error_vel_xy: 0.3919
Metrics/base_velocity/error_vel_yaw: 0.2682
      Episode_Termination/time_out: 0.3475
  Episode_Termination/base_contact: 0.6525
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 2.55s
                      Time elapsed: 00:16:57
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 398/1400 [0m                      

                       Computation: 9749 steps/s (collection: 2.264s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.9392
                       Mean reward: 7.62
               Mean episode length: 606.76
Episode_Reward/track_lin_vel_xy_exp: 0.4613
Episode_Reward/track_ang_vel_z_exp: 0.2598
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0536
         Episode_Reward/dof_acc_l2: -0.1186
     Episode_Reward/action_rate_l2: -0.0535
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0083
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5485
Metrics/base_velocity/error_vel_xy: 0.4166
Metrics/base_velocity/error_vel_yaw: 0.3127
      Episode_Termination/time_out: 0.3524
  Episode_Termination/base_contact: 0.6476
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 2.52s
                      Time elapsed: 00:17:00
                               ETA: 00:42:42

################################################################################
                     [1m Learning iteration 399/1400 [0m                      

                       Computation: 9777 steps/s (collection: 2.256s, learning 0.258s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 6.9145
                       Mean reward: 7.26
               Mean episode length: 589.03
Episode_Reward/track_lin_vel_xy_exp: 0.3525
Episode_Reward/track_ang_vel_z_exp: 0.1936
       Episode_Reward/lin_vel_z_l2: -0.0294
      Episode_Reward/ang_vel_xy_l2: -0.0364
     Episode_Reward/dof_torques_l2: -0.0390
         Episode_Reward/dof_acc_l2: -0.0907
     Episode_Reward/action_rate_l2: -0.0405
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5503
Metrics/base_velocity/error_vel_xy: 0.2930
Metrics/base_velocity/error_vel_yaw: 0.2267
      Episode_Termination/time_out: 0.3516
  Episode_Termination/base_contact: 0.6484
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 2.51s
                      Time elapsed: 00:17:02
                               ETA: 00:42:39

################################################################################
                     [1m Learning iteration 400/1400 [0m                      

                       Computation: 9798 steps/s (collection: 2.245s, learning 0.264s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.9111
                       Mean reward: 6.90
               Mean episode length: 553.04
Episode_Reward/track_lin_vel_xy_exp: 0.4014
Episode_Reward/track_ang_vel_z_exp: 0.2299
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0403
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.1018
     Episode_Reward/action_rate_l2: -0.0465
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5500
Metrics/base_velocity/error_vel_xy: 0.3466
Metrics/base_velocity/error_vel_yaw: 0.2447
      Episode_Termination/time_out: 0.3519
  Episode_Termination/base_contact: 0.6481
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 2.51s
                      Time elapsed: 00:17:05
                               ETA: 00:42:37

################################################################################
                     [1m Learning iteration 401/1400 [0m                      

                       Computation: 9731 steps/s (collection: 2.270s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.9055
                       Mean reward: 7.57
               Mean episode length: 597.12
Episode_Reward/track_lin_vel_xy_exp: 0.4805
Episode_Reward/track_ang_vel_z_exp: 0.2678
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0540
         Episode_Reward/dof_acc_l2: -0.1214
     Episode_Reward/action_rate_l2: -0.0542
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5558
Metrics/base_velocity/error_vel_xy: 0.3843
Metrics/base_velocity/error_vel_yaw: 0.2875
      Episode_Termination/time_out: 0.3554
  Episode_Termination/base_contact: 0.6446
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 2.53s
                      Time elapsed: 00:17:07
                               ETA: 00:42:34

################################################################################
                     [1m Learning iteration 402/1400 [0m                      

                       Computation: 9782 steps/s (collection: 2.250s, learning 0.263s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 6.8954
                       Mean reward: 7.70
               Mean episode length: 618.96
Episode_Reward/track_lin_vel_xy_exp: 0.3455
Episode_Reward/track_ang_vel_z_exp: 0.2057
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0387
     Episode_Reward/dof_torques_l2: -0.0436
         Episode_Reward/dof_acc_l2: -0.0984
     Episode_Reward/action_rate_l2: -0.0434
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5630
Metrics/base_velocity/error_vel_xy: 0.3771
Metrics/base_velocity/error_vel_yaw: 0.2535
      Episode_Termination/time_out: 0.3605
  Episode_Termination/base_contact: 0.6395
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 2.51s
                      Time elapsed: 00:17:10
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 403/1400 [0m                      

                       Computation: 9669 steps/s (collection: 2.286s, learning 0.255s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 6.8789
                       Mean reward: 7.06
               Mean episode length: 581.27
Episode_Reward/track_lin_vel_xy_exp: 0.2556
Episode_Reward/track_ang_vel_z_exp: 0.1559
       Episode_Reward/lin_vel_z_l2: -0.0216
      Episode_Reward/ang_vel_xy_l2: -0.0312
     Episode_Reward/dof_torques_l2: -0.0339
         Episode_Reward/dof_acc_l2: -0.0767
     Episode_Reward/action_rate_l2: -0.0325
      Episode_Reward/feet_air_time: -0.0054
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5669
Metrics/base_velocity/error_vel_xy: 0.2829
Metrics/base_velocity/error_vel_yaw: 0.1793
      Episode_Termination/time_out: 0.3577
  Episode_Termination/base_contact: 0.6423
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.54s
                      Time elapsed: 00:17:13
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 404/1400 [0m                      

                       Computation: 9646 steps/s (collection: 2.291s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.8691
                       Mean reward: 7.51
               Mean episode length: 598.03
Episode_Reward/track_lin_vel_xy_exp: 0.4926
Episode_Reward/track_ang_vel_z_exp: 0.2722
       Episode_Reward/lin_vel_z_l2: -0.0378
      Episode_Reward/ang_vel_xy_l2: -0.0489
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1176
     Episode_Reward/action_rate_l2: -0.0550
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5802
Metrics/base_velocity/error_vel_xy: 0.3751
Metrics/base_velocity/error_vel_yaw: 0.2837
      Episode_Termination/time_out: 0.3573
  Episode_Termination/base_contact: 0.6427
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 2.55s
                      Time elapsed: 00:17:15
                               ETA: 00:42:26

################################################################################
                     [1m Learning iteration 405/1400 [0m                      

                       Computation: 9766 steps/s (collection: 2.261s, learning 0.255s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 6.8504
                       Mean reward: 7.59
               Mean episode length: 614.65
Episode_Reward/track_lin_vel_xy_exp: 0.3884
Episode_Reward/track_ang_vel_z_exp: 0.2257
       Episode_Reward/lin_vel_z_l2: -0.0344
      Episode_Reward/ang_vel_xy_l2: -0.0427
     Episode_Reward/dof_torques_l2: -0.0487
         Episode_Reward/dof_acc_l2: -0.1085
     Episode_Reward/action_rate_l2: -0.0465
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5950
Metrics/base_velocity/error_vel_xy: 0.3611
Metrics/base_velocity/error_vel_yaw: 0.2393
      Episode_Termination/time_out: 0.3587
  Episode_Termination/base_contact: 0.6413
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 2.52s
                      Time elapsed: 00:17:18
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 406/1400 [0m                      

                       Computation: 9627 steps/s (collection: 2.300s, learning 0.253s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.8548
                       Mean reward: 6.94
               Mean episode length: 589.82
Episode_Reward/track_lin_vel_xy_exp: 0.3219
Episode_Reward/track_ang_vel_z_exp: 0.1842
       Episode_Reward/lin_vel_z_l2: -0.0313
      Episode_Reward/ang_vel_xy_l2: -0.0363
     Episode_Reward/dof_torques_l2: -0.0391
         Episode_Reward/dof_acc_l2: -0.0934
     Episode_Reward/action_rate_l2: -0.0382
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6057
Metrics/base_velocity/error_vel_xy: 0.3033
Metrics/base_velocity/error_vel_yaw: 0.2223
      Episode_Termination/time_out: 0.3543
  Episode_Termination/base_contact: 0.6457
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 2.55s
                      Time elapsed: 00:17:20
                               ETA: 00:42:21

################################################################################
                     [1m Learning iteration 407/1400 [0m                      

                       Computation: 9599 steps/s (collection: 2.303s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.8560
                       Mean reward: 6.83
               Mean episode length: 576.69
Episode_Reward/track_lin_vel_xy_exp: 0.4667
Episode_Reward/track_ang_vel_z_exp: 0.2568
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0460
     Episode_Reward/dof_torques_l2: -0.0488
         Episode_Reward/dof_acc_l2: -0.1129
     Episode_Reward/action_rate_l2: -0.0519
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6099
Metrics/base_velocity/error_vel_xy: 0.3576
Metrics/base_velocity/error_vel_yaw: 0.2725
      Episode_Termination/time_out: 0.3538
  Episode_Termination/base_contact: 0.6462
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.56s
                      Time elapsed: 00:17:23
                               ETA: 00:42:18

################################################################################
                     [1m Learning iteration 408/1400 [0m                      

                       Computation: 9687 steps/s (collection: 2.284s, learning 0.253s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.8487
                       Mean reward: 7.43
               Mean episode length: 598.51
Episode_Reward/track_lin_vel_xy_exp: 0.4302
Episode_Reward/track_ang_vel_z_exp: 0.2372
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0440
     Episode_Reward/dof_torques_l2: -0.0468
         Episode_Reward/dof_acc_l2: -0.1081
     Episode_Reward/action_rate_l2: -0.0485
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6141
Metrics/base_velocity/error_vel_xy: 0.3377
Metrics/base_velocity/error_vel_yaw: 0.2569
      Episode_Termination/time_out: 0.3518
  Episode_Termination/base_contact: 0.6482
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 2.54s
                      Time elapsed: 00:17:25
                               ETA: 00:42:16

################################################################################
                     [1m Learning iteration 409/1400 [0m                      

                       Computation: 9779 steps/s (collection: 2.263s, learning 0.250s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.8638
                       Mean reward: 7.47
               Mean episode length: 571.02
Episode_Reward/track_lin_vel_xy_exp: 0.4176
Episode_Reward/track_ang_vel_z_exp: 0.2269
       Episode_Reward/lin_vel_z_l2: -0.0304
      Episode_Reward/ang_vel_xy_l2: -0.0398
     Episode_Reward/dof_torques_l2: -0.0436
         Episode_Reward/dof_acc_l2: -0.0955
     Episode_Reward/action_rate_l2: -0.0450
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6267
Metrics/base_velocity/error_vel_xy: 0.2968
Metrics/base_velocity/error_vel_yaw: 0.2282
      Episode_Termination/time_out: 0.3537
  Episode_Termination/base_contact: 0.6463
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 2.51s
                      Time elapsed: 00:17:28
                               ETA: 00:42:13

################################################################################
                     [1m Learning iteration 410/1400 [0m                      

                       Computation: 9692 steps/s (collection: 2.272s, learning 0.264s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 6.8645
                       Mean reward: 7.30
               Mean episode length: 579.22
Episode_Reward/track_lin_vel_xy_exp: 0.4112
Episode_Reward/track_ang_vel_z_exp: 0.2377
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0440
     Episode_Reward/dof_torques_l2: -0.0485
         Episode_Reward/dof_acc_l2: -0.1082
     Episode_Reward/action_rate_l2: -0.0482
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6380
Metrics/base_velocity/error_vel_xy: 0.3735
Metrics/base_velocity/error_vel_yaw: 0.2557
      Episode_Termination/time_out: 0.3568
  Episode_Termination/base_contact: 0.6432
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 2.54s
                      Time elapsed: 00:17:30
                               ETA: 00:42:11

################################################################################
                     [1m Learning iteration 411/1400 [0m                      

                       Computation: 9611 steps/s (collection: 2.300s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.8597
                       Mean reward: 6.52
               Mean episode length: 562.93
Episode_Reward/track_lin_vel_xy_exp: 0.3386
Episode_Reward/track_ang_vel_z_exp: 0.1930
       Episode_Reward/lin_vel_z_l2: -0.0302
      Episode_Reward/ang_vel_xy_l2: -0.0383
     Episode_Reward/dof_torques_l2: -0.0401
         Episode_Reward/dof_acc_l2: -0.0933
     Episode_Reward/action_rate_l2: -0.0405
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6482
Metrics/base_velocity/error_vel_xy: 0.3341
Metrics/base_velocity/error_vel_yaw: 0.2454
      Episode_Termination/time_out: 0.3563
  Episode_Termination/base_contact: 0.6437
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.56s
                      Time elapsed: 00:17:33
                               ETA: 00:42:08

################################################################################
                     [1m Learning iteration 412/1400 [0m                      

                       Computation: 9758 steps/s (collection: 2.261s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.8569
                       Mean reward: 6.07
               Mean episode length: 538.47
Episode_Reward/track_lin_vel_xy_exp: 0.3574
Episode_Reward/track_ang_vel_z_exp: 0.2073
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0390
     Episode_Reward/dof_torques_l2: -0.0416
         Episode_Reward/dof_acc_l2: -0.0950
     Episode_Reward/action_rate_l2: -0.0430
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6515
Metrics/base_velocity/error_vel_xy: 0.3600
Metrics/base_velocity/error_vel_yaw: 0.2467
      Episode_Termination/time_out: 0.3556
  Episode_Termination/base_contact: 0.6444
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 2.52s
                      Time elapsed: 00:17:35
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 413/1400 [0m                      

                       Computation: 9801 steps/s (collection: 2.251s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.8367
                       Mean reward: 6.57
               Mean episode length: 583.70
Episode_Reward/track_lin_vel_xy_exp: 0.4604
Episode_Reward/track_ang_vel_z_exp: 0.2693
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1215
     Episode_Reward/action_rate_l2: -0.0545
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6526
Metrics/base_velocity/error_vel_xy: 0.4311
Metrics/base_velocity/error_vel_yaw: 0.3010
      Episode_Termination/time_out: 0.3539
  Episode_Termination/base_contact: 0.6461
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 2.51s
                      Time elapsed: 00:17:38
                               ETA: 00:42:03

################################################################################
                     [1m Learning iteration 414/1400 [0m                      

                       Computation: 9565 steps/s (collection: 2.313s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.8255
                       Mean reward: 7.71
               Mean episode length: 634.28
Episode_Reward/track_lin_vel_xy_exp: 0.4522
Episode_Reward/track_ang_vel_z_exp: 0.2583
       Episode_Reward/lin_vel_z_l2: -0.0359
      Episode_Reward/ang_vel_xy_l2: -0.0456
     Episode_Reward/dof_torques_l2: -0.0501
         Episode_Reward/dof_acc_l2: -0.1135
     Episode_Reward/action_rate_l2: -0.0514
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6660
Metrics/base_velocity/error_vel_xy: 0.3801
Metrics/base_velocity/error_vel_yaw: 0.2673
      Episode_Termination/time_out: 0.3620
  Episode_Termination/base_contact: 0.6380
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 2.57s
                      Time elapsed: 00:17:40
                               ETA: 00:42:00

################################################################################
                     [1m Learning iteration 415/1400 [0m                      

                       Computation: 9536 steps/s (collection: 2.319s, learning 0.258s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.8268
                       Mean reward: 7.48
               Mean episode length: 599.98
Episode_Reward/track_lin_vel_xy_exp: 0.3471
Episode_Reward/track_ang_vel_z_exp: 0.1952
       Episode_Reward/lin_vel_z_l2: -0.0309
      Episode_Reward/ang_vel_xy_l2: -0.0366
     Episode_Reward/dof_torques_l2: -0.0399
         Episode_Reward/dof_acc_l2: -0.0849
     Episode_Reward/action_rate_l2: -0.0400
      Episode_Reward/feet_air_time: -0.0057
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6691
Metrics/base_velocity/error_vel_xy: 0.3040
Metrics/base_velocity/error_vel_yaw: 0.2186
      Episode_Termination/time_out: 0.3659
  Episode_Termination/base_contact: 0.6341
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.58s
                      Time elapsed: 00:17:43
                               ETA: 00:41:58

################################################################################
                     [1m Learning iteration 416/1400 [0m                      

                       Computation: 9641 steps/s (collection: 2.291s, learning 0.258s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 6.8271
                       Mean reward: 7.44
               Mean episode length: 581.55
Episode_Reward/track_lin_vel_xy_exp: 0.5145
Episode_Reward/track_ang_vel_z_exp: 0.2852
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0482
     Episode_Reward/dof_torques_l2: -0.0553
         Episode_Reward/dof_acc_l2: -0.1170
     Episode_Reward/action_rate_l2: -0.0566
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6769
Metrics/base_velocity/error_vel_xy: 0.3860
Metrics/base_velocity/error_vel_yaw: 0.2828
      Episode_Termination/time_out: 0.3687
  Episode_Termination/base_contact: 0.6313
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 2.55s
                      Time elapsed: 00:17:46
                               ETA: 00:41:55

################################################################################
                     [1m Learning iteration 417/1400 [0m                      

                       Computation: 9510 steps/s (collection: 2.328s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.8278
                       Mean reward: 8.78
               Mean episode length: 660.15
Episode_Reward/track_lin_vel_xy_exp: 0.4619
Episode_Reward/track_ang_vel_z_exp: 0.2545
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0498
         Episode_Reward/dof_acc_l2: -0.1147
     Episode_Reward/action_rate_l2: -0.0514
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6877
Metrics/base_velocity/error_vel_xy: 0.3612
Metrics/base_velocity/error_vel_yaw: 0.2718
      Episode_Termination/time_out: 0.3703
  Episode_Termination/base_contact: 0.6297
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 2.58s
                      Time elapsed: 00:17:48
                               ETA: 00:41:53

################################################################################
                     [1m Learning iteration 418/1400 [0m                      

                       Computation: 9794 steps/s (collection: 2.257s, learning 0.252s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 6.8341
                       Mean reward: 8.13
               Mean episode length: 638.61
Episode_Reward/track_lin_vel_xy_exp: 0.3621
Episode_Reward/track_ang_vel_z_exp: 0.2119
       Episode_Reward/lin_vel_z_l2: -0.0319
      Episode_Reward/ang_vel_xy_l2: -0.0394
     Episode_Reward/dof_torques_l2: -0.0454
         Episode_Reward/dof_acc_l2: -0.1015
     Episode_Reward/action_rate_l2: -0.0438
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7013
Metrics/base_velocity/error_vel_xy: 0.3598
Metrics/base_velocity/error_vel_yaw: 0.2465
      Episode_Termination/time_out: 0.3722
  Episode_Termination/base_contact: 0.6278
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 2.51s
                      Time elapsed: 00:17:51
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 419/1400 [0m                      

                       Computation: 9672 steps/s (collection: 2.287s, learning 0.253s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.8342
                       Mean reward: 6.94
               Mean episode length: 561.07
Episode_Reward/track_lin_vel_xy_exp: 0.2736
Episode_Reward/track_ang_vel_z_exp: 0.1573
       Episode_Reward/lin_vel_z_l2: -0.0236
      Episode_Reward/ang_vel_xy_l2: -0.0295
     Episode_Reward/dof_torques_l2: -0.0335
         Episode_Reward/dof_acc_l2: -0.0755
     Episode_Reward/action_rate_l2: -0.0331
      Episode_Reward/feet_air_time: -0.0053
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7024
Metrics/base_velocity/error_vel_xy: 0.2675
Metrics/base_velocity/error_vel_yaw: 0.1929
      Episode_Termination/time_out: 0.3702
  Episode_Termination/base_contact: 0.6298
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.54s
                      Time elapsed: 00:17:53
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 420/1400 [0m                      

                       Computation: 9593 steps/s (collection: 2.301s, learning 0.261s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 6.8378
                       Mean reward: 6.06
               Mean episode length: 522.66
Episode_Reward/track_lin_vel_xy_exp: 0.4064
Episode_Reward/track_ang_vel_z_exp: 0.2311
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0428
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.1064
     Episode_Reward/action_rate_l2: -0.0476
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7015
Metrics/base_velocity/error_vel_xy: 0.3799
Metrics/base_velocity/error_vel_yaw: 0.2735
      Episode_Termination/time_out: 0.3662
  Episode_Termination/base_contact: 0.6338
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 2.56s
                      Time elapsed: 00:17:56
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 421/1400 [0m                      

                       Computation: 9356 steps/s (collection: 2.371s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.8389
                       Mean reward: 6.55
               Mean episode length: 549.96
Episode_Reward/track_lin_vel_xy_exp: 0.3726
Episode_Reward/track_ang_vel_z_exp: 0.2109
       Episode_Reward/lin_vel_z_l2: -0.0299
      Episode_Reward/ang_vel_xy_l2: -0.0391
     Episode_Reward/dof_torques_l2: -0.0430
         Episode_Reward/dof_acc_l2: -0.0932
     Episode_Reward/action_rate_l2: -0.0433
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7043
Metrics/base_velocity/error_vel_xy: 0.3246
Metrics/base_velocity/error_vel_yaw: 0.2361
      Episode_Termination/time_out: 0.3632
  Episode_Termination/base_contact: 0.6368
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 2.63s
                      Time elapsed: 00:17:58
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 422/1400 [0m                      

                       Computation: 9622 steps/s (collection: 2.300s, learning 0.254s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.8398
                       Mean reward: 6.53
               Mean episode length: 539.64
Episode_Reward/track_lin_vel_xy_exp: 0.4343
Episode_Reward/track_ang_vel_z_exp: 0.2417
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0434
     Episode_Reward/dof_torques_l2: -0.0463
         Episode_Reward/dof_acc_l2: -0.1054
     Episode_Reward/action_rate_l2: -0.0486
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7141
Metrics/base_velocity/error_vel_xy: 0.3454
Metrics/base_velocity/error_vel_yaw: 0.2556
      Episode_Termination/time_out: 0.3671
  Episode_Termination/base_contact: 0.6329
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 2.55s
                      Time elapsed: 00:18:01
                               ETA: 00:41:40

################################################################################
                     [1m Learning iteration 423/1400 [0m                      

                       Computation: 9667 steps/s (collection: 2.290s, learning 0.252s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 6.8337
                       Mean reward: 7.40
               Mean episode length: 604.94
Episode_Reward/track_lin_vel_xy_exp: 0.4941
Episode_Reward/track_ang_vel_z_exp: 0.2865
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0518
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1250
     Episode_Reward/action_rate_l2: -0.0573
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0076
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7323
Metrics/base_velocity/error_vel_xy: 0.4443
Metrics/base_velocity/error_vel_yaw: 0.3020
      Episode_Termination/time_out: 0.3757
  Episode_Termination/base_contact: 0.6243
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.54s
                      Time elapsed: 00:18:03
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 424/1400 [0m                      

                       Computation: 9734 steps/s (collection: 2.267s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 6.8318
                       Mean reward: 6.51
               Mean episode length: 555.33
Episode_Reward/track_lin_vel_xy_exp: 0.3417
Episode_Reward/track_ang_vel_z_exp: 0.1928
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0374
     Episode_Reward/dof_torques_l2: -0.0398
         Episode_Reward/dof_acc_l2: -0.0914
     Episode_Reward/action_rate_l2: -0.0395
      Episode_Reward/feet_air_time: -0.0060
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7402
Metrics/base_velocity/error_vel_xy: 0.3044
Metrics/base_velocity/error_vel_yaw: 0.2244
      Episode_Termination/time_out: 0.3755
  Episode_Termination/base_contact: 0.6245
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 2.52s
                      Time elapsed: 00:18:06
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 425/1400 [0m                      

                       Computation: 9713 steps/s (collection: 2.274s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.8273
                       Mean reward: 6.72
               Mean episode length: 546.10
Episode_Reward/track_lin_vel_xy_exp: 0.3931
Episode_Reward/track_ang_vel_z_exp: 0.2184
       Episode_Reward/lin_vel_z_l2: -0.0285
      Episode_Reward/ang_vel_xy_l2: -0.0394
     Episode_Reward/dof_torques_l2: -0.0423
         Episode_Reward/dof_acc_l2: -0.0988
     Episode_Reward/action_rate_l2: -0.0434
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7465
Metrics/base_velocity/error_vel_xy: 0.3190
Metrics/base_velocity/error_vel_yaw: 0.2389
      Episode_Termination/time_out: 0.3726
  Episode_Termination/base_contact: 0.6274
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 2.53s
                      Time elapsed: 00:18:09
                               ETA: 00:41:32

################################################################################
                     [1m Learning iteration 426/1400 [0m                      

                       Computation: 9742 steps/s (collection: 2.265s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.8197
                       Mean reward: 7.18
               Mean episode length: 576.19
Episode_Reward/track_lin_vel_xy_exp: 0.4187
Episode_Reward/track_ang_vel_z_exp: 0.2390
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0487
         Episode_Reward/dof_acc_l2: -0.1109
     Episode_Reward/action_rate_l2: -0.0486
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7512
Metrics/base_velocity/error_vel_xy: 0.3826
Metrics/base_velocity/error_vel_yaw: 0.2774
      Episode_Termination/time_out: 0.3700
  Episode_Termination/base_contact: 0.6300
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 2.52s
                      Time elapsed: 00:18:11
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 427/1400 [0m                      

                       Computation: 9388 steps/s (collection: 2.361s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.8001
                       Mean reward: 7.13
               Mean episode length: 589.49
Episode_Reward/track_lin_vel_xy_exp: 0.4137
Episode_Reward/track_ang_vel_z_exp: 0.2320
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0477
         Episode_Reward/dof_acc_l2: -0.1082
     Episode_Reward/action_rate_l2: -0.0471
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7635
Metrics/base_velocity/error_vel_xy: 0.3511
Metrics/base_velocity/error_vel_yaw: 0.2535
      Episode_Termination/time_out: 0.3727
  Episode_Termination/base_contact: 0.6273
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.62s
                      Time elapsed: 00:18:14
                               ETA: 00:41:27

################################################################################
                     [1m Learning iteration 428/1400 [0m                      

                       Computation: 9410 steps/s (collection: 2.354s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.7917
                       Mean reward: 6.70
               Mean episode length: 578.17
Episode_Reward/track_lin_vel_xy_exp: 0.3659
Episode_Reward/track_ang_vel_z_exp: 0.1984
       Episode_Reward/lin_vel_z_l2: -0.0314
      Episode_Reward/ang_vel_xy_l2: -0.0391
     Episode_Reward/dof_torques_l2: -0.0427
         Episode_Reward/dof_acc_l2: -0.0971
     Episode_Reward/action_rate_l2: -0.0413
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7678
Metrics/base_velocity/error_vel_xy: 0.3079
Metrics/base_velocity/error_vel_yaw: 0.2506
      Episode_Termination/time_out: 0.3685
  Episode_Termination/base_contact: 0.6315
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 2.61s
                      Time elapsed: 00:18:16
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 429/1400 [0m                      

                       Computation: 9541 steps/s (collection: 2.320s, learning 0.255s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 6.7886
                       Mean reward: 6.45
               Mean episode length: 562.68
Episode_Reward/track_lin_vel_xy_exp: 0.4164
Episode_Reward/track_ang_vel_z_exp: 0.2411
       Episode_Reward/lin_vel_z_l2: -0.0353
      Episode_Reward/ang_vel_xy_l2: -0.0441
     Episode_Reward/dof_torques_l2: -0.0504
         Episode_Reward/dof_acc_l2: -0.1058
     Episode_Reward/action_rate_l2: -0.0492
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7747
Metrics/base_velocity/error_vel_xy: 0.3765
Metrics/base_velocity/error_vel_yaw: 0.2640
      Episode_Termination/time_out: 0.3701
  Episode_Termination/base_contact: 0.6299
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 2.58s
                      Time elapsed: 00:18:19
                               ETA: 00:41:22

################################################################################
                     [1m Learning iteration 430/1400 [0m                      

                       Computation: 9827 steps/s (collection: 2.245s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.7876
                       Mean reward: 6.51
               Mean episode length: 561.61
Episode_Reward/track_lin_vel_xy_exp: 0.3727
Episode_Reward/track_ang_vel_z_exp: 0.2052
       Episode_Reward/lin_vel_z_l2: -0.0289
      Episode_Reward/ang_vel_xy_l2: -0.0369
     Episode_Reward/dof_torques_l2: -0.0405
         Episode_Reward/dof_acc_l2: -0.0923
     Episode_Reward/action_rate_l2: -0.0410
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7728
Metrics/base_velocity/error_vel_xy: 0.3025
Metrics/base_velocity/error_vel_yaw: 0.2299
      Episode_Termination/time_out: 0.3683
  Episode_Termination/base_contact: 0.6317
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 2.50s
                      Time elapsed: 00:18:21
                               ETA: 00:41:19

################################################################################
                     [1m Learning iteration 431/1400 [0m                      

                       Computation: 9683 steps/s (collection: 2.285s, learning 0.253s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 6.7795
                       Mean reward: 6.62
               Mean episode length: 577.59
Episode_Reward/track_lin_vel_xy_exp: 0.5223
Episode_Reward/track_ang_vel_z_exp: 0.2891
       Episode_Reward/lin_vel_z_l2: -0.0412
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.1277
     Episode_Reward/action_rate_l2: -0.0578
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7831
Metrics/base_velocity/error_vel_xy: 0.4155
Metrics/base_velocity/error_vel_yaw: 0.3063
      Episode_Termination/time_out: 0.3708
  Episode_Termination/base_contact: 0.6292
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.54s
                      Time elapsed: 00:18:24
                               ETA: 00:41:17

################################################################################
                     [1m Learning iteration 432/1400 [0m                      

                       Computation: 9713 steps/s (collection: 2.275s, learning 0.255s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.7668
                       Mean reward: 7.06
               Mean episode length: 595.70
Episode_Reward/track_lin_vel_xy_exp: 0.3547
Episode_Reward/track_ang_vel_z_exp: 0.2023
       Episode_Reward/lin_vel_z_l2: -0.0313
      Episode_Reward/ang_vel_xy_l2: -0.0388
     Episode_Reward/dof_torques_l2: -0.0408
         Episode_Reward/dof_acc_l2: -0.0938
     Episode_Reward/action_rate_l2: -0.0410
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7849
Metrics/base_velocity/error_vel_xy: 0.3168
Metrics/base_velocity/error_vel_yaw: 0.2319
      Episode_Termination/time_out: 0.3718
  Episode_Termination/base_contact: 0.6282
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 2.53s
                      Time elapsed: 00:18:26
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 433/1400 [0m                      

                       Computation: 9677 steps/s (collection: 2.284s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.7503
                       Mean reward: 6.68
               Mean episode length: 565.99
Episode_Reward/track_lin_vel_xy_exp: 0.3831
Episode_Reward/track_ang_vel_z_exp: 0.2204
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.0405
     Episode_Reward/dof_torques_l2: -0.0467
         Episode_Reward/dof_acc_l2: -0.1010
     Episode_Reward/action_rate_l2: -0.0459
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7817
Metrics/base_velocity/error_vel_xy: 0.3797
Metrics/base_velocity/error_vel_yaw: 0.2680
      Episode_Termination/time_out: 0.3698
  Episode_Termination/base_contact: 0.6302
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 2.54s
                      Time elapsed: 00:18:29
                               ETA: 00:41:12

################################################################################
                     [1m Learning iteration 434/1400 [0m                      

                       Computation: 9564 steps/s (collection: 2.314s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.7414
                       Mean reward: 7.01
               Mean episode length: 585.59
Episode_Reward/track_lin_vel_xy_exp: 0.4059
Episode_Reward/track_ang_vel_z_exp: 0.2364
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0445
     Episode_Reward/dof_torques_l2: -0.0481
         Episode_Reward/dof_acc_l2: -0.1136
     Episode_Reward/action_rate_l2: -0.0476
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7849
Metrics/base_velocity/error_vel_xy: 0.3896
Metrics/base_velocity/error_vel_yaw: 0.2555
      Episode_Termination/time_out: 0.3649
  Episode_Termination/base_contact: 0.6351
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 2.57s
                      Time elapsed: 00:18:32
                               ETA: 00:41:09

################################################################################
                     [1m Learning iteration 435/1400 [0m                      

                       Computation: 9676 steps/s (collection: 2.288s, learning 0.252s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 6.7275
                       Mean reward: 6.43
               Mean episode length: 551.45
Episode_Reward/track_lin_vel_xy_exp: 0.3655
Episode_Reward/track_ang_vel_z_exp: 0.2053
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.0379
     Episode_Reward/dof_torques_l2: -0.0421
         Episode_Reward/dof_acc_l2: -0.0943
     Episode_Reward/action_rate_l2: -0.0413
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7931
Metrics/base_velocity/error_vel_xy: 0.3170
Metrics/base_velocity/error_vel_yaw: 0.2312
      Episode_Termination/time_out: 0.3631
  Episode_Termination/base_contact: 0.6369
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.54s
                      Time elapsed: 00:18:34
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 436/1400 [0m                      

                       Computation: 9670 steps/s (collection: 2.284s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.7240
                       Mean reward: 6.57
               Mean episode length: 551.50
Episode_Reward/track_lin_vel_xy_exp: 0.4490
Episode_Reward/track_ang_vel_z_exp: 0.2517
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0472
     Episode_Reward/dof_torques_l2: -0.0511
         Episode_Reward/dof_acc_l2: -0.1141
     Episode_Reward/action_rate_l2: -0.0502
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8013
Metrics/base_velocity/error_vel_xy: 0.3674
Metrics/base_velocity/error_vel_yaw: 0.2682
      Episode_Termination/time_out: 0.3584
  Episode_Termination/base_contact: 0.6416
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 2.54s
                      Time elapsed: 00:18:37
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 437/1400 [0m                      

                       Computation: 9581 steps/s (collection: 2.309s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0186
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.7222
                       Mean reward: 7.45
               Mean episode length: 580.47
Episode_Reward/track_lin_vel_xy_exp: 0.4596
Episode_Reward/track_ang_vel_z_exp: 0.2504
       Episode_Reward/lin_vel_z_l2: -0.0335
      Episode_Reward/ang_vel_xy_l2: -0.0433
     Episode_Reward/dof_torques_l2: -0.0467
         Episode_Reward/dof_acc_l2: -0.1002
     Episode_Reward/action_rate_l2: -0.0493
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8073
Metrics/base_velocity/error_vel_xy: 0.3297
Metrics/base_velocity/error_vel_yaw: 0.2534
      Episode_Termination/time_out: 0.3627
  Episode_Termination/base_contact: 0.6373
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 2.56s
                      Time elapsed: 00:18:39
                               ETA: 00:41:01

################################################################################
                     [1m Learning iteration 438/1400 [0m                      

                       Computation: 9622 steps/s (collection: 2.298s, learning 0.256s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.7357
                       Mean reward: 7.28
               Mean episode length: 576.57
Episode_Reward/track_lin_vel_xy_exp: 0.4076
Episode_Reward/track_ang_vel_z_exp: 0.2323
       Episode_Reward/lin_vel_z_l2: -0.0342
      Episode_Reward/ang_vel_xy_l2: -0.0435
     Episode_Reward/dof_torques_l2: -0.0480
         Episode_Reward/dof_acc_l2: -0.1101
     Episode_Reward/action_rate_l2: -0.0477
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8143
Metrics/base_velocity/error_vel_xy: 0.3725
Metrics/base_velocity/error_vel_yaw: 0.2726
      Episode_Termination/time_out: 0.3634
  Episode_Termination/base_contact: 0.6366
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 2.55s
                      Time elapsed: 00:18:42
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 439/1400 [0m                      

                       Computation: 9702 steps/s (collection: 2.270s, learning 0.263s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 6.7399
                       Mean reward: 7.40
               Mean episode length: 593.27
Episode_Reward/track_lin_vel_xy_exp: 0.4497
Episode_Reward/track_ang_vel_z_exp: 0.2598
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0488
         Episode_Reward/dof_acc_l2: -0.1171
     Episode_Reward/action_rate_l2: -0.0519
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8141
Metrics/base_velocity/error_vel_xy: 0.4027
Metrics/base_velocity/error_vel_yaw: 0.2847
      Episode_Termination/time_out: 0.3605
  Episode_Termination/base_contact: 0.6395
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.53s
                      Time elapsed: 00:18:44
                               ETA: 00:40:56

################################################################################
                     [1m Learning iteration 440/1400 [0m                      

                       Computation: 9521 steps/s (collection: 2.324s, learning 0.258s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.7403
                       Mean reward: 7.22
               Mean episode length: 592.72
Episode_Reward/track_lin_vel_xy_exp: 0.3877
Episode_Reward/track_ang_vel_z_exp: 0.2218
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0406
     Episode_Reward/dof_torques_l2: -0.0493
         Episode_Reward/dof_acc_l2: -0.1082
     Episode_Reward/action_rate_l2: -0.0469
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8157
Metrics/base_velocity/error_vel_xy: 0.3706
Metrics/base_velocity/error_vel_yaw: 0.2668
      Episode_Termination/time_out: 0.3624
  Episode_Termination/base_contact: 0.6376
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 2.58s
                      Time elapsed: 00:18:47
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 441/1400 [0m                      

                       Computation: 9539 steps/s (collection: 2.319s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.7292
                       Mean reward: 6.93
               Mean episode length: 592.55
Episode_Reward/track_lin_vel_xy_exp: 0.4111
Episode_Reward/track_ang_vel_z_exp: 0.2249
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0419
     Episode_Reward/dof_torques_l2: -0.0463
         Episode_Reward/dof_acc_l2: -0.1027
     Episode_Reward/action_rate_l2: -0.0466
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8101
Metrics/base_velocity/error_vel_xy: 0.3458
Metrics/base_velocity/error_vel_yaw: 0.2759
      Episode_Termination/time_out: 0.3614
  Episode_Termination/base_contact: 0.6386
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 2.58s
                      Time elapsed: 00:18:49
                               ETA: 00:40:51

################################################################################
                     [1m Learning iteration 442/1400 [0m                      

                       Computation: 9523 steps/s (collection: 2.324s, learning 0.257s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 6.7164
                       Mean reward: 7.89
               Mean episode length: 624.47
Episode_Reward/track_lin_vel_xy_exp: 0.5112
Episode_Reward/track_ang_vel_z_exp: 0.2797
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0547
         Episode_Reward/dof_acc_l2: -0.1193
     Episode_Reward/action_rate_l2: -0.0564
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8179
Metrics/base_velocity/error_vel_xy: 0.3828
Metrics/base_velocity/error_vel_yaw: 0.2946
      Episode_Termination/time_out: 0.3622
  Episode_Termination/base_contact: 0.6378
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 2.58s
                      Time elapsed: 00:18:52
                               ETA: 00:40:49

################################################################################
                     [1m Learning iteration 443/1400 [0m                      

                       Computation: 9537 steps/s (collection: 2.319s, learning 0.258s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.7193
                       Mean reward: 8.47
               Mean episode length: 641.59
Episode_Reward/track_lin_vel_xy_exp: 0.5402
Episode_Reward/track_ang_vel_z_exp: 0.2882
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0489
     Episode_Reward/dof_torques_l2: -0.0563
         Episode_Reward/dof_acc_l2: -0.1182
     Episode_Reward/action_rate_l2: -0.0572
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8309
Metrics/base_velocity/error_vel_xy: 0.3603
Metrics/base_velocity/error_vel_yaw: 0.2930
      Episode_Termination/time_out: 0.3635
  Episode_Termination/base_contact: 0.6365
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.58s
                      Time elapsed: 00:18:55
                               ETA: 00:40:46

################################################################################
                     [1m Learning iteration 444/1400 [0m                      

                       Computation: 9503 steps/s (collection: 2.326s, learning 0.259s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.7106
                       Mean reward: 8.03
               Mean episode length: 612.55
Episode_Reward/track_lin_vel_xy_exp: 0.4444
Episode_Reward/track_ang_vel_z_exp: 0.2496
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0450
     Episode_Reward/dof_torques_l2: -0.0493
         Episode_Reward/dof_acc_l2: -0.1113
     Episode_Reward/action_rate_l2: -0.0496
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8355
Metrics/base_velocity/error_vel_xy: 0.3542
Metrics/base_velocity/error_vel_yaw: 0.2586
      Episode_Termination/time_out: 0.3663
  Episode_Termination/base_contact: 0.6337
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 2.59s
                      Time elapsed: 00:18:57
                               ETA: 00:40:44

################################################################################
                     [1m Learning iteration 445/1400 [0m                      

                       Computation: 9580 steps/s (collection: 2.302s, learning 0.263s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.7104
                       Mean reward: 7.14
               Mean episode length: 582.25
Episode_Reward/track_lin_vel_xy_exp: 0.3940
Episode_Reward/track_ang_vel_z_exp: 0.2292
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.0419
     Episode_Reward/dof_torques_l2: -0.0506
         Episode_Reward/dof_acc_l2: -0.1065
     Episode_Reward/action_rate_l2: -0.0480
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0090
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8379
Metrics/base_velocity/error_vel_xy: 0.4025
Metrics/base_velocity/error_vel_yaw: 0.2820
      Episode_Termination/time_out: 0.3661
  Episode_Termination/base_contact: 0.6339
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 2.57s
                      Time elapsed: 00:19:00
                               ETA: 00:40:41

################################################################################
                     [1m Learning iteration 446/1400 [0m                      

                       Computation: 9572 steps/s (collection: 2.303s, learning 0.264s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 6.7024
                       Mean reward: 6.97
               Mean episode length: 566.06
Episode_Reward/track_lin_vel_xy_exp: 0.4093
Episode_Reward/track_ang_vel_z_exp: 0.2248
       Episode_Reward/lin_vel_z_l2: -0.0314
      Episode_Reward/ang_vel_xy_l2: -0.0410
     Episode_Reward/dof_torques_l2: -0.0439
         Episode_Reward/dof_acc_l2: -0.0962
     Episode_Reward/action_rate_l2: -0.0448
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0074
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8478
Metrics/base_velocity/error_vel_xy: 0.3152
Metrics/base_velocity/error_vel_yaw: 0.2426
      Episode_Termination/time_out: 0.3658
  Episode_Termination/base_contact: 0.6342
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 2.57s
                      Time elapsed: 00:19:02
                               ETA: 00:40:39

################################################################################
                     [1m Learning iteration 447/1400 [0m                      

                       Computation: 9656 steps/s (collection: 2.290s, learning 0.255s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.6987
                       Mean reward: 6.98
               Mean episode length: 578.87
Episode_Reward/track_lin_vel_xy_exp: 0.4288
Episode_Reward/track_ang_vel_z_exp: 0.2358
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0466
         Episode_Reward/dof_acc_l2: -0.1090
     Episode_Reward/action_rate_l2: -0.0474
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8594
Metrics/base_velocity/error_vel_xy: 0.3513
Metrics/base_velocity/error_vel_yaw: 0.2672
      Episode_Termination/time_out: 0.3661
  Episode_Termination/base_contact: 0.6339
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.54s
                      Time elapsed: 00:19:05
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 448/1400 [0m                      

                       Computation: 9797 steps/s (collection: 2.251s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 6.6830
                       Mean reward: 7.10
               Mean episode length: 589.74
Episode_Reward/track_lin_vel_xy_exp: 0.4044
Episode_Reward/track_ang_vel_z_exp: 0.2278
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.0440
     Episode_Reward/dof_torques_l2: -0.0479
         Episode_Reward/dof_acc_l2: -0.1178
     Episode_Reward/action_rate_l2: -0.0482
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8654
Metrics/base_velocity/error_vel_xy: 0.4070
Metrics/base_velocity/error_vel_yaw: 0.3020
      Episode_Termination/time_out: 0.3654
  Episode_Termination/base_contact: 0.6346
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 2.51s
                      Time elapsed: 00:19:07
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 449/1400 [0m                      

                       Computation: 9728 steps/s (collection: 2.271s, learning 0.255s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.6763
                       Mean reward: 8.10
               Mean episode length: 655.23
Episode_Reward/track_lin_vel_xy_exp: 0.4977
Episode_Reward/track_ang_vel_z_exp: 0.2788
       Episode_Reward/lin_vel_z_l2: -0.0378
      Episode_Reward/ang_vel_xy_l2: -0.0487
     Episode_Reward/dof_torques_l2: -0.0553
         Episode_Reward/dof_acc_l2: -0.1181
     Episode_Reward/action_rate_l2: -0.0557
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8729
Metrics/base_velocity/error_vel_xy: 0.4054
Metrics/base_velocity/error_vel_yaw: 0.2959
      Episode_Termination/time_out: 0.3653
  Episode_Termination/base_contact: 0.6347
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 2.53s
                      Time elapsed: 00:19:10
                               ETA: 00:40:31

################################################################################
                     [1m Learning iteration 450/1400 [0m                      

                       Computation: 9689 steps/s (collection: 2.279s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.6682
                       Mean reward: 7.88
               Mean episode length: 624.02
Episode_Reward/track_lin_vel_xy_exp: 0.3149
Episode_Reward/track_ang_vel_z_exp: 0.1883
       Episode_Reward/lin_vel_z_l2: -0.0303
      Episode_Reward/ang_vel_xy_l2: -0.0359
     Episode_Reward/dof_torques_l2: -0.0399
         Episode_Reward/dof_acc_l2: -0.0933
     Episode_Reward/action_rate_l2: -0.0385
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8796
Metrics/base_velocity/error_vel_xy: 0.3300
Metrics/base_velocity/error_vel_yaw: 0.2165
      Episode_Termination/time_out: 0.3675
  Episode_Termination/base_contact: 0.6325
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 2.54s
                      Time elapsed: 00:19:12
                               ETA: 00:40:28

################################################################################
                     [1m Learning iteration 451/1400 [0m                      

                       Computation: 9586 steps/s (collection: 2.307s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.6663
                       Mean reward: 7.72
               Mean episode length: 614.90
Episode_Reward/track_lin_vel_xy_exp: 0.5123
Episode_Reward/track_ang_vel_z_exp: 0.2829
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0500
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1253
     Episode_Reward/action_rate_l2: -0.0573
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8807
Metrics/base_velocity/error_vel_xy: 0.4036
Metrics/base_velocity/error_vel_yaw: 0.3077
      Episode_Termination/time_out: 0.3669
  Episode_Termination/base_contact: 0.6331
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.56s
                      Time elapsed: 00:19:15
                               ETA: 00:40:26

################################################################################
                     [1m Learning iteration 452/1400 [0m                      

                       Computation: 9677 steps/s (collection: 2.287s, learning 0.253s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.6662
                       Mean reward: 7.47
               Mean episode length: 600.87
Episode_Reward/track_lin_vel_xy_exp: 0.3669
Episode_Reward/track_ang_vel_z_exp: 0.2098
       Episode_Reward/lin_vel_z_l2: -0.0316
      Episode_Reward/ang_vel_xy_l2: -0.0392
     Episode_Reward/dof_torques_l2: -0.0423
         Episode_Reward/dof_acc_l2: -0.0920
     Episode_Reward/action_rate_l2: -0.0423
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0079
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8905
Metrics/base_velocity/error_vel_xy: 0.3301
Metrics/base_velocity/error_vel_yaw: 0.2374
      Episode_Termination/time_out: 0.3657
  Episode_Termination/base_contact: 0.6343
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 2.54s
                      Time elapsed: 00:19:18
                               ETA: 00:40:23

################################################################################
                     [1m Learning iteration 453/1400 [0m                      

                       Computation: 9621 steps/s (collection: 2.291s, learning 0.263s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.6618
                       Mean reward: 7.44
               Mean episode length: 599.90
Episode_Reward/track_lin_vel_xy_exp: 0.4162
Episode_Reward/track_ang_vel_z_exp: 0.2287
       Episode_Reward/lin_vel_z_l2: -0.0324
      Episode_Reward/ang_vel_xy_l2: -0.0402
     Episode_Reward/dof_torques_l2: -0.0450
         Episode_Reward/dof_acc_l2: -0.0990
     Episode_Reward/action_rate_l2: -0.0458
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8949
Metrics/base_velocity/error_vel_xy: 0.3192
Metrics/base_velocity/error_vel_yaw: 0.2421
      Episode_Termination/time_out: 0.3652
  Episode_Termination/base_contact: 0.6348
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 2.55s
                      Time elapsed: 00:19:20
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 454/1400 [0m                      

                       Computation: 9468 steps/s (collection: 2.340s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 6.6630
                       Mean reward: 6.72
               Mean episode length: 570.48
Episode_Reward/track_lin_vel_xy_exp: 0.3532
Episode_Reward/track_ang_vel_z_exp: 0.2075
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.0414
     Episode_Reward/dof_torques_l2: -0.0457
         Episode_Reward/dof_acc_l2: -0.1026
     Episode_Reward/action_rate_l2: -0.0438
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0084
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8943
Metrics/base_velocity/error_vel_xy: 0.3816
Metrics/base_velocity/error_vel_yaw: 0.2719
      Episode_Termination/time_out: 0.3638
  Episode_Termination/base_contact: 0.6362
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 2.60s
                      Time elapsed: 00:19:23
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 455/1400 [0m                      

                       Computation: 9755 steps/s (collection: 2.262s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.6570
                       Mean reward: 6.95
               Mean episode length: 600.98
Episode_Reward/track_lin_vel_xy_exp: 0.4788
Episode_Reward/track_ang_vel_z_exp: 0.2729
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0480
     Episode_Reward/dof_torques_l2: -0.0546
         Episode_Reward/dof_acc_l2: -0.1218
     Episode_Reward/action_rate_l2: -0.0550
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9055
Metrics/base_velocity/error_vel_xy: 0.4164
Metrics/base_velocity/error_vel_yaw: 0.2940
      Episode_Termination/time_out: 0.3619
  Episode_Termination/base_contact: 0.6381
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 2.52s
                      Time elapsed: 00:19:25
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 456/1400 [0m                      

                       Computation: 9725 steps/s (collection: 2.264s, learning 0.262s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.6518
                       Mean reward: 7.80
               Mean episode length: 648.45
Episode_Reward/track_lin_vel_xy_exp: 0.4547
Episode_Reward/track_ang_vel_z_exp: 0.2544
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0436
     Episode_Reward/dof_torques_l2: -0.0522
         Episode_Reward/dof_acc_l2: -0.1151
     Episode_Reward/action_rate_l2: -0.0518
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9204
Metrics/base_velocity/error_vel_xy: 0.3790
Metrics/base_velocity/error_vel_yaw: 0.2795
      Episode_Termination/time_out: 0.3620
  Episode_Termination/base_contact: 0.6380
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 2.53s
                      Time elapsed: 00:19:28
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 457/1400 [0m                      

                       Computation: 9623 steps/s (collection: 2.297s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 6.6526
                       Mean reward: 7.08
               Mean episode length: 586.60
Episode_Reward/track_lin_vel_xy_exp: 0.3432
Episode_Reward/track_ang_vel_z_exp: 0.2011
       Episode_Reward/lin_vel_z_l2: -0.0298
      Episode_Reward/ang_vel_xy_l2: -0.0362
     Episode_Reward/dof_torques_l2: -0.0423
         Episode_Reward/dof_acc_l2: -0.0910
     Episode_Reward/action_rate_l2: -0.0412
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9235
Metrics/base_velocity/error_vel_xy: 0.3379
Metrics/base_velocity/error_vel_yaw: 0.2302
      Episode_Termination/time_out: 0.3652
  Episode_Termination/base_contact: 0.6348
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 2.55s
                      Time elapsed: 00:19:30
                               ETA: 00:40:10

################################################################################
                     [1m Learning iteration 458/1400 [0m                      

                       Computation: 9689 steps/s (collection: 2.279s, learning 0.258s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.6412
                       Mean reward: 6.99
               Mean episode length: 575.75
Episode_Reward/track_lin_vel_xy_exp: 0.4517
Episode_Reward/track_ang_vel_z_exp: 0.2504
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0446
     Episode_Reward/dof_torques_l2: -0.0512
         Episode_Reward/dof_acc_l2: -0.1125
     Episode_Reward/action_rate_l2: -0.0498
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9253
Metrics/base_velocity/error_vel_xy: 0.3614
Metrics/base_velocity/error_vel_yaw: 0.2660
      Episode_Termination/time_out: 0.3621
  Episode_Termination/base_contact: 0.6379
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 2.54s
                      Time elapsed: 00:19:33
                               ETA: 00:40:07

################################################################################
                     [1m Learning iteration 459/1400 [0m                      

                       Computation: 9647 steps/s (collection: 2.293s, learning 0.254s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.6364
                       Mean reward: 7.57
               Mean episode length: 596.83
Episode_Reward/track_lin_vel_xy_exp: 0.4351
Episode_Reward/track_ang_vel_z_exp: 0.2423
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.0423
     Episode_Reward/dof_torques_l2: -0.0472
         Episode_Reward/dof_acc_l2: -0.1016
     Episode_Reward/action_rate_l2: -0.0484
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9332
Metrics/base_velocity/error_vel_xy: 0.3513
Metrics/base_velocity/error_vel_yaw: 0.2635
      Episode_Termination/time_out: 0.3608
  Episode_Termination/base_contact: 0.6392
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.55s
                      Time elapsed: 00:19:35
                               ETA: 00:40:05

################################################################################
                     [1m Learning iteration 460/1400 [0m                      

                       Computation: 9797 steps/s (collection: 2.250s, learning 0.258s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 6.6320
                       Mean reward: 8.02
               Mean episode length: 632.84
Episode_Reward/track_lin_vel_xy_exp: 0.4313
Episode_Reward/track_ang_vel_z_exp: 0.2519
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0519
         Episode_Reward/dof_acc_l2: -0.1200
     Episode_Reward/action_rate_l2: -0.0511
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9360
Metrics/base_velocity/error_vel_xy: 0.4156
Metrics/base_velocity/error_vel_yaw: 0.2859
      Episode_Termination/time_out: 0.3591
  Episode_Termination/base_contact: 0.6409
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 2.51s
                      Time elapsed: 00:19:38
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 461/1400 [0m                      

                       Computation: 9707 steps/s (collection: 2.276s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.6199
                       Mean reward: 8.38
               Mean episode length: 660.91
Episode_Reward/track_lin_vel_xy_exp: 0.4024
Episode_Reward/track_ang_vel_z_exp: 0.2317
       Episode_Reward/lin_vel_z_l2: -0.0314
      Episode_Reward/ang_vel_xy_l2: -0.0425
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.1024
     Episode_Reward/action_rate_l2: -0.0463
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9386
Metrics/base_velocity/error_vel_xy: 0.3631
Metrics/base_velocity/error_vel_yaw: 0.2529
      Episode_Termination/time_out: 0.3612
  Episode_Termination/base_contact: 0.6388
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 2.53s
                      Time elapsed: 00:19:40
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 462/1400 [0m                      

                       Computation: 9737 steps/s (collection: 2.267s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.6110
                       Mean reward: 7.53
               Mean episode length: 630.77
Episode_Reward/track_lin_vel_xy_exp: 0.3870
Episode_Reward/track_ang_vel_z_exp: 0.2164
       Episode_Reward/lin_vel_z_l2: -0.0304
      Episode_Reward/ang_vel_xy_l2: -0.0384
     Episode_Reward/dof_torques_l2: -0.0437
         Episode_Reward/dof_acc_l2: -0.0976
     Episode_Reward/action_rate_l2: -0.0445
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9386
Metrics/base_velocity/error_vel_xy: 0.3349
Metrics/base_velocity/error_vel_yaw: 0.2524
      Episode_Termination/time_out: 0.3647
  Episode_Termination/base_contact: 0.6353
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 2.52s
                      Time elapsed: 00:19:43
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 463/1400 [0m                      

                       Computation: 9760 steps/s (collection: 2.256s, learning 0.262s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.6078
                       Mean reward: 7.84
               Mean episode length: 640.91
Episode_Reward/track_lin_vel_xy_exp: 0.4307
Episode_Reward/track_ang_vel_z_exp: 0.2524
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0448
     Episode_Reward/dof_torques_l2: -0.0506
         Episode_Reward/dof_acc_l2: -0.1198
     Episode_Reward/action_rate_l2: -0.0508
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9402
Metrics/base_velocity/error_vel_xy: 0.4162
Metrics/base_velocity/error_vel_yaw: 0.2736
      Episode_Termination/time_out: 0.3645
  Episode_Termination/base_contact: 0.6355
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 2.52s
                      Time elapsed: 00:19:45
                               ETA: 00:39:54

################################################################################
                     [1m Learning iteration 464/1400 [0m                      

                       Computation: 9691 steps/s (collection: 2.284s, learning 0.252s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.5991
                       Mean reward: 7.96
               Mean episode length: 643.51
Episode_Reward/track_lin_vel_xy_exp: 0.4488
Episode_Reward/track_ang_vel_z_exp: 0.2523
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0457
     Episode_Reward/dof_torques_l2: -0.0511
         Episode_Reward/dof_acc_l2: -0.1151
     Episode_Reward/action_rate_l2: -0.0506
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9459
Metrics/base_velocity/error_vel_xy: 0.3650
Metrics/base_velocity/error_vel_yaw: 0.2664
      Episode_Termination/time_out: 0.3641
  Episode_Termination/base_contact: 0.6359
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 2.54s
                      Time elapsed: 00:19:48
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 465/1400 [0m                      

                       Computation: 9750 steps/s (collection: 2.267s, learning 0.254s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.5779
                       Mean reward: 8.51
               Mean episode length: 654.90
Episode_Reward/track_lin_vel_xy_exp: 0.4748
Episode_Reward/track_ang_vel_z_exp: 0.2567
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0447
     Episode_Reward/dof_torques_l2: -0.0485
         Episode_Reward/dof_acc_l2: -0.1151
     Episode_Reward/action_rate_l2: -0.0510
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9544
Metrics/base_velocity/error_vel_xy: 0.3457
Metrics/base_velocity/error_vel_yaw: 0.2787
      Episode_Termination/time_out: 0.3667
  Episode_Termination/base_contact: 0.6333
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 2.52s
                      Time elapsed: 00:19:50
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 466/1400 [0m                      

                       Computation: 9676 steps/s (collection: 2.279s, learning 0.260s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.5618
                       Mean reward: 8.70
               Mean episode length: 658.07
Episode_Reward/track_lin_vel_xy_exp: 0.4321
Episode_Reward/track_ang_vel_z_exp: 0.2445
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.0414
     Episode_Reward/dof_torques_l2: -0.0487
         Episode_Reward/dof_acc_l2: -0.1014
     Episode_Reward/action_rate_l2: -0.0486
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9574
Metrics/base_velocity/error_vel_xy: 0.3757
Metrics/base_velocity/error_vel_yaw: 0.2689
      Episode_Termination/time_out: 0.3709
  Episode_Termination/base_contact: 0.6291
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 2.54s
                      Time elapsed: 00:19:53
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 467/1400 [0m                      

                       Computation: 9480 steps/s (collection: 2.336s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 6.5660
                       Mean reward: 9.24
               Mean episode length: 689.66
Episode_Reward/track_lin_vel_xy_exp: 0.5221
Episode_Reward/track_ang_vel_z_exp: 0.2846
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1202
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9663
Metrics/base_velocity/error_vel_xy: 0.3740
Metrics/base_velocity/error_vel_yaw: 0.2895
      Episode_Termination/time_out: 0.3767
  Episode_Termination/base_contact: 0.6233
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.59s
                      Time elapsed: 00:19:56
                               ETA: 00:39:44

################################################################################
                     [1m Learning iteration 468/1400 [0m                      

                       Computation: 9647 steps/s (collection: 2.290s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 6.5624
                       Mean reward: 8.65
               Mean episode length: 662.94
Episode_Reward/track_lin_vel_xy_exp: 0.4435
Episode_Reward/track_ang_vel_z_exp: 0.2502
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0506
         Episode_Reward/dof_acc_l2: -0.1128
     Episode_Reward/action_rate_l2: -0.0500
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9729
Metrics/base_velocity/error_vel_xy: 0.3703
Metrics/base_velocity/error_vel_yaw: 0.2695
      Episode_Termination/time_out: 0.3772
  Episode_Termination/base_contact: 0.6228
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 2.55s
                      Time elapsed: 00:19:58
                               ETA: 00:39:42

################################################################################
                     [1m Learning iteration 469/1400 [0m                      

                       Computation: 9673 steps/s (collection: 2.284s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 6.5589
                       Mean reward: 8.03
               Mean episode length: 629.96
Episode_Reward/track_lin_vel_xy_exp: 0.4496
Episode_Reward/track_ang_vel_z_exp: 0.2483
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0494
         Episode_Reward/dof_acc_l2: -0.1063
     Episode_Reward/action_rate_l2: -0.0491
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0084
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9800
Metrics/base_velocity/error_vel_xy: 0.3426
Metrics/base_velocity/error_vel_yaw: 0.2579
      Episode_Termination/time_out: 0.3769
  Episode_Termination/base_contact: 0.6231
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 2.54s
                      Time elapsed: 00:20:01
                               ETA: 00:39:39

################################################################################
                     [1m Learning iteration 470/1400 [0m                      

                       Computation: 9715 steps/s (collection: 2.266s, learning 0.263s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.5515
                       Mean reward: 8.38
               Mean episode length: 654.95
Episode_Reward/track_lin_vel_xy_exp: 0.4438
Episode_Reward/track_ang_vel_z_exp: 0.2576
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0452
     Episode_Reward/dof_torques_l2: -0.0550
         Episode_Reward/dof_acc_l2: -0.1137
     Episode_Reward/action_rate_l2: -0.0522
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9846
Metrics/base_velocity/error_vel_xy: 0.4168
Metrics/base_velocity/error_vel_yaw: 0.2901
      Episode_Termination/time_out: 0.3812
  Episode_Termination/base_contact: 0.6188
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 2.53s
                      Time elapsed: 00:20:03
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 471/1400 [0m                      

                       Computation: 9779 steps/s (collection: 2.257s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.5353
                       Mean reward: 8.25
               Mean episode length: 638.62
Episode_Reward/track_lin_vel_xy_exp: 0.4015
Episode_Reward/track_ang_vel_z_exp: 0.2253
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.0393
     Episode_Reward/dof_torques_l2: -0.0450
         Episode_Reward/dof_acc_l2: -0.0963
     Episode_Reward/action_rate_l2: -0.0442
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9868
Metrics/base_velocity/error_vel_xy: 0.3168
Metrics/base_velocity/error_vel_yaw: 0.2331
      Episode_Termination/time_out: 0.3822
  Episode_Termination/base_contact: 0.6178
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 2.51s
                      Time elapsed: 00:20:06
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 472/1400 [0m                      

                       Computation: 9649 steps/s (collection: 2.293s, learning 0.254s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.5151
                       Mean reward: 7.28
               Mean episode length: 590.01
Episode_Reward/track_lin_vel_xy_exp: 0.2985
Episode_Reward/track_ang_vel_z_exp: 0.1811
       Episode_Reward/lin_vel_z_l2: -0.0296
      Episode_Reward/ang_vel_xy_l2: -0.0357
     Episode_Reward/dof_torques_l2: -0.0412
         Episode_Reward/dof_acc_l2: -0.0928
     Episode_Reward/action_rate_l2: -0.0386
      Episode_Reward/feet_air_time: -0.0065
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9904
Metrics/base_velocity/error_vel_xy: 0.3569
Metrics/base_velocity/error_vel_yaw: 0.2438
      Episode_Termination/time_out: 0.3794
  Episode_Termination/base_contact: 0.6206
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 2.55s
                      Time elapsed: 00:20:08
                               ETA: 00:39:31

################################################################################
                     [1m Learning iteration 473/1400 [0m                      

                       Computation: 9611 steps/s (collection: 2.302s, learning 0.255s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 6.4955
                       Mean reward: 7.32
               Mean episode length: 596.25
Episode_Reward/track_lin_vel_xy_exp: 0.4284
Episode_Reward/track_ang_vel_z_exp: 0.2423
       Episode_Reward/lin_vel_z_l2: -0.0355
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0486
         Episode_Reward/dof_acc_l2: -0.1101
     Episode_Reward/action_rate_l2: -0.0487
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9915
Metrics/base_velocity/error_vel_xy: 0.3711
Metrics/base_velocity/error_vel_yaw: 0.2711
      Episode_Termination/time_out: 0.3759
  Episode_Termination/base_contact: 0.6241
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 2.56s
                      Time elapsed: 00:20:11
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 474/1400 [0m                      

                       Computation: 9740 steps/s (collection: 2.267s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.4908
                       Mean reward: 7.34
               Mean episode length: 602.03
Episode_Reward/track_lin_vel_xy_exp: 0.4723
Episode_Reward/track_ang_vel_z_exp: 0.2637
       Episode_Reward/lin_vel_z_l2: -0.0331
      Episode_Reward/ang_vel_xy_l2: -0.0433
     Episode_Reward/dof_torques_l2: -0.0501
         Episode_Reward/dof_acc_l2: -0.1030
     Episode_Reward/action_rate_l2: -0.0520
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9965
Metrics/base_velocity/error_vel_xy: 0.3773
Metrics/base_velocity/error_vel_yaw: 0.2747
      Episode_Termination/time_out: 0.3808
  Episode_Termination/base_contact: 0.6201
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 2.52s
                      Time elapsed: 00:20:13
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 475/1400 [0m                      

                       Computation: 9836 steps/s (collection: 2.243s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.4838
                       Mean reward: 8.12
               Mean episode length: 644.38
Episode_Reward/track_lin_vel_xy_exp: 0.4758
Episode_Reward/track_ang_vel_z_exp: 0.2614
       Episode_Reward/lin_vel_z_l2: -0.0330
      Episode_Reward/ang_vel_xy_l2: -0.0426
     Episode_Reward/dof_torques_l2: -0.0479
         Episode_Reward/dof_acc_l2: -0.1065
     Episode_Reward/action_rate_l2: -0.0498
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9991
Metrics/base_velocity/error_vel_xy: 0.3472
Metrics/base_velocity/error_vel_yaw: 0.2640
      Episode_Termination/time_out: 0.3855
  Episode_Termination/base_contact: 0.6155
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.50s
                      Time elapsed: 00:20:16
                               ETA: 00:39:23

################################################################################
                     [1m Learning iteration 476/1400 [0m                      

                       Computation: 9685 steps/s (collection: 2.284s, learning 0.254s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 6.4806
                       Mean reward: 7.86
               Mean episode length: 633.07
Episode_Reward/track_lin_vel_xy_exp: 0.4774
Episode_Reward/track_ang_vel_z_exp: 0.2712
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0501
         Episode_Reward/dof_acc_l2: -0.1152
     Episode_Reward/action_rate_l2: -0.0527
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0054
Metrics/base_velocity/error_vel_xy: 0.4150
Metrics/base_velocity/error_vel_yaw: 0.2929
      Episode_Termination/time_out: 0.3840
  Episode_Termination/base_contact: 0.6169
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 2.54s
                      Time elapsed: 00:20:18
                               ETA: 00:39:21

################################################################################
                     [1m Learning iteration 477/1400 [0m                      

                       Computation: 9654 steps/s (collection: 2.291s, learning 0.254s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.4708
                       Mean reward: 8.05
               Mean episode length: 661.76
Episode_Reward/track_lin_vel_xy_exp: 0.4727
Episode_Reward/track_ang_vel_z_exp: 0.2707
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0461
     Episode_Reward/dof_torques_l2: -0.0547
         Episode_Reward/dof_acc_l2: -0.1209
     Episode_Reward/action_rate_l2: -0.0537
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0091
Metrics/base_velocity/error_vel_xy: 0.4183
Metrics/base_velocity/error_vel_yaw: 0.2915
      Episode_Termination/time_out: 0.3819
  Episode_Termination/base_contact: 0.6191
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 2.55s
                      Time elapsed: 00:20:21
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 478/1400 [0m                      

                       Computation: 9736 steps/s (collection: 2.266s, learning 0.258s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.4556
                       Mean reward: 7.72
               Mean episode length: 645.02
Episode_Reward/track_lin_vel_xy_exp: 0.4120
Episode_Reward/track_ang_vel_z_exp: 0.2292
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0423
     Episode_Reward/dof_torques_l2: -0.0489
         Episode_Reward/dof_acc_l2: -0.1103
     Episode_Reward/action_rate_l2: -0.0468
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0191
Metrics/base_velocity/error_vel_xy: 0.3600
Metrics/base_velocity/error_vel_yaw: 0.2674
      Episode_Termination/time_out: 0.3860
  Episode_Termination/base_contact: 0.6149
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 2.52s
                      Time elapsed: 00:20:23
                               ETA: 00:39:15

################################################################################
                     [1m Learning iteration 479/1400 [0m                      

                       Computation: 9728 steps/s (collection: 2.269s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.4420
                       Mean reward: 7.87
               Mean episode length: 643.69
Episode_Reward/track_lin_vel_xy_exp: 0.4549
Episode_Reward/track_ang_vel_z_exp: 0.2528
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.0495
         Episode_Reward/dof_acc_l2: -0.1229
     Episode_Reward/action_rate_l2: -0.0508
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0205
Metrics/base_velocity/error_vel_xy: 0.3746
Metrics/base_velocity/error_vel_yaw: 0.2843
      Episode_Termination/time_out: 0.3872
  Episode_Termination/base_contact: 0.6138
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.53s
                      Time elapsed: 00:20:26
                               ETA: 00:39:13

################################################################################
                     [1m Learning iteration 480/1400 [0m                      

                       Computation: 9862 steps/s (collection: 2.240s, learning 0.252s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.4493
                       Mean reward: 8.30
               Mean episode length: 645.32
Episode_Reward/track_lin_vel_xy_exp: 0.4708
Episode_Reward/track_ang_vel_z_exp: 0.2591
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0456
     Episode_Reward/dof_torques_l2: -0.0498
         Episode_Reward/dof_acc_l2: -0.1127
     Episode_Reward/action_rate_l2: -0.0503
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0223
Metrics/base_velocity/error_vel_xy: 0.3481
Metrics/base_velocity/error_vel_yaw: 0.2615
      Episode_Termination/time_out: 0.3861
  Episode_Termination/base_contact: 0.6149
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 2.49s
                      Time elapsed: 00:20:29
                               ETA: 00:39:10

################################################################################
                     [1m Learning iteration 481/1400 [0m                      

                       Computation: 9710 steps/s (collection: 2.274s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.4578
                       Mean reward: 7.70
               Mean episode length: 617.17
Episode_Reward/track_lin_vel_xy_exp: 0.4096
Episode_Reward/track_ang_vel_z_exp: 0.2363
       Episode_Reward/lin_vel_z_l2: -0.0331
      Episode_Reward/ang_vel_xy_l2: -0.0406
     Episode_Reward/dof_torques_l2: -0.0478
         Episode_Reward/dof_acc_l2: -0.1072
     Episode_Reward/action_rate_l2: -0.0472
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0246
Metrics/base_velocity/error_vel_xy: 0.3626
Metrics/base_velocity/error_vel_yaw: 0.2539
      Episode_Termination/time_out: 0.3874
  Episode_Termination/base_contact: 0.6136
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 2.53s
                      Time elapsed: 00:20:31
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 482/1400 [0m                      

                       Computation: 9683 steps/s (collection: 2.283s, learning 0.255s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.4720
                       Mean reward: 7.80
               Mean episode length: 636.30
Episode_Reward/track_lin_vel_xy_exp: 0.4539
Episode_Reward/track_ang_vel_z_exp: 0.2641
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0543
         Episode_Reward/dof_acc_l2: -0.1183
     Episode_Reward/action_rate_l2: -0.0519
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0359
Metrics/base_velocity/error_vel_xy: 0.4162
Metrics/base_velocity/error_vel_yaw: 0.2853
      Episode_Termination/time_out: 0.3867
  Episode_Termination/base_contact: 0.6143
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 2.54s
                      Time elapsed: 00:20:34
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 483/1400 [0m                      

                       Computation: 9424 steps/s (collection: 2.357s, learning 0.250s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 6.4767
                       Mean reward: 7.21
               Mean episode length: 611.85
Episode_Reward/track_lin_vel_xy_exp: 0.3627
Episode_Reward/track_ang_vel_z_exp: 0.2147
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0390
     Episode_Reward/dof_torques_l2: -0.0441
         Episode_Reward/dof_acc_l2: -0.1018
     Episode_Reward/action_rate_l2: -0.0435
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0414
Metrics/base_velocity/error_vel_xy: 0.3691
Metrics/base_velocity/error_vel_yaw: 0.2492
      Episode_Termination/time_out: 0.3934
  Episode_Termination/base_contact: 0.6076
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.61s
                      Time elapsed: 00:20:36
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 484/1400 [0m                      

                       Computation: 9469 steps/s (collection: 2.329s, learning 0.267s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.4698
                       Mean reward: 7.97
               Mean episode length: 631.82
Episode_Reward/track_lin_vel_xy_exp: 0.4404
Episode_Reward/track_ang_vel_z_exp: 0.2460
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0483
         Episode_Reward/dof_acc_l2: -0.1051
     Episode_Reward/action_rate_l2: -0.0480
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0470
Metrics/base_velocity/error_vel_xy: 0.3404
Metrics/base_velocity/error_vel_yaw: 0.2473
      Episode_Termination/time_out: 0.3917
  Episode_Termination/base_contact: 0.6093
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 2.60s
                      Time elapsed: 00:20:39
                               ETA: 00:39:00

################################################################################
                     [1m Learning iteration 485/1400 [0m                      

                       Computation: 9563 steps/s (collection: 2.313s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.4676
                       Mean reward: 7.99
               Mean episode length: 639.96
Episode_Reward/track_lin_vel_xy_exp: 0.4740
Episode_Reward/track_ang_vel_z_exp: 0.2821
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0535
     Episode_Reward/dof_torques_l2: -0.0622
         Episode_Reward/dof_acc_l2: -0.1348
     Episode_Reward/action_rate_l2: -0.0575
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0509
Metrics/base_velocity/error_vel_xy: 0.4921
Metrics/base_velocity/error_vel_yaw: 0.3403
      Episode_Termination/time_out: 0.3911
  Episode_Termination/base_contact: 0.6099
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 2.57s
                      Time elapsed: 00:20:41
                               ETA: 00:38:58

################################################################################
                     [1m Learning iteration 486/1400 [0m                      

                       Computation: 9784 steps/s (collection: 2.247s, learning 0.265s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.4714
                       Mean reward: 8.56
               Mean episode length: 669.24
Episode_Reward/track_lin_vel_xy_exp: 0.4646
Episode_Reward/track_ang_vel_z_exp: 0.2621
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0547
         Episode_Reward/dof_acc_l2: -0.1135
     Episode_Reward/action_rate_l2: -0.0527
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0579
Metrics/base_velocity/error_vel_xy: 0.4100
Metrics/base_velocity/error_vel_yaw: 0.2970
      Episode_Termination/time_out: 0.3923
  Episode_Termination/base_contact: 0.6086
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 2.51s
                      Time elapsed: 00:20:44
                               ETA: 00:38:55

################################################################################
                     [1m Learning iteration 487/1400 [0m                      

                       Computation: 9670 steps/s (collection: 2.287s, learning 0.254s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.4785
                       Mean reward: 8.02
               Mean episode length: 659.89
Episode_Reward/track_lin_vel_xy_exp: 0.4092
Episode_Reward/track_ang_vel_z_exp: 0.2350
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0435
     Episode_Reward/dof_torques_l2: -0.0474
         Episode_Reward/dof_acc_l2: -0.1088
     Episode_Reward/action_rate_l2: -0.0469
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0525
Metrics/base_velocity/error_vel_xy: 0.3936
Metrics/base_velocity/error_vel_yaw: 0.2755
      Episode_Termination/time_out: 0.3914
  Episode_Termination/base_contact: 0.6095
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.54s
                      Time elapsed: 00:20:46
                               ETA: 00:38:52

################################################################################
                     [1m Learning iteration 488/1400 [0m                      

                       Computation: 9636 steps/s (collection: 2.286s, learning 0.264s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 6.4783
                       Mean reward: 8.31
               Mean episode length: 658.15
Episode_Reward/track_lin_vel_xy_exp: 0.5056
Episode_Reward/track_ang_vel_z_exp: 0.2786
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0531
         Episode_Reward/dof_acc_l2: -0.1190
     Episode_Reward/action_rate_l2: -0.0543
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0574
Metrics/base_velocity/error_vel_xy: 0.3823
Metrics/base_velocity/error_vel_yaw: 0.2871
      Episode_Termination/time_out: 0.3930
  Episode_Termination/base_contact: 0.6070
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 2.55s
                      Time elapsed: 00:20:49
                               ETA: 00:38:50

################################################################################
                     [1m Learning iteration 489/1400 [0m                      

                       Computation: 9726 steps/s (collection: 2.272s, learning 0.255s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.4637
                       Mean reward: 8.59
               Mean episode length: 661.72
Episode_Reward/track_lin_vel_xy_exp: 0.4799
Episode_Reward/track_ang_vel_z_exp: 0.2702
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0478
     Episode_Reward/dof_torques_l2: -0.0503
         Episode_Reward/dof_acc_l2: -0.1120
     Episode_Reward/action_rate_l2: -0.0519
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0619
Metrics/base_velocity/error_vel_xy: 0.3754
Metrics/base_velocity/error_vel_yaw: 0.2685
      Episode_Termination/time_out: 0.3975
  Episode_Termination/base_contact: 0.6025
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 2.53s
                      Time elapsed: 00:20:51
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 490/1400 [0m                      

                       Computation: 9748 steps/s (collection: 2.260s, learning 0.261s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.4634
                       Mean reward: 9.12
               Mean episode length: 691.28
Episode_Reward/track_lin_vel_xy_exp: 0.5523
Episode_Reward/track_ang_vel_z_exp: 0.3147
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.0560
     Episode_Reward/dof_torques_l2: -0.0576
         Episode_Reward/dof_acc_l2: -0.1399
     Episode_Reward/action_rate_l2: -0.0604
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0660
Metrics/base_velocity/error_vel_xy: 0.4439
Metrics/base_velocity/error_vel_yaw: 0.3128
      Episode_Termination/time_out: 0.3994
  Episode_Termination/base_contact: 0.6006
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 2.52s
                      Time elapsed: 00:20:54
                               ETA: 00:38:45

################################################################################
                     [1m Learning iteration 491/1400 [0m                      

                       Computation: 9728 steps/s (collection: 2.265s, learning 0.261s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 6.4773
                       Mean reward: 8.70
               Mean episode length: 674.09
Episode_Reward/track_lin_vel_xy_exp: 0.4293
Episode_Reward/track_ang_vel_z_exp: 0.2427
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0431
     Episode_Reward/dof_torques_l2: -0.0501
         Episode_Reward/dof_acc_l2: -0.1096
     Episode_Reward/action_rate_l2: -0.0487
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0715
Metrics/base_velocity/error_vel_xy: 0.3825
Metrics/base_velocity/error_vel_yaw: 0.2750
      Episode_Termination/time_out: 0.3984
  Episode_Termination/base_contact: 0.6016
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.53s
                      Time elapsed: 00:20:57
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 492/1400 [0m                      

                       Computation: 9530 steps/s (collection: 2.323s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 6.4729
                       Mean reward: 8.65
               Mean episode length: 689.23
Episode_Reward/track_lin_vel_xy_exp: 0.4287
Episode_Reward/track_ang_vel_z_exp: 0.2553
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0450
     Episode_Reward/dof_torques_l2: -0.0525
         Episode_Reward/dof_acc_l2: -0.1179
     Episode_Reward/action_rate_l2: -0.0498
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0765
Metrics/base_velocity/error_vel_xy: 0.4268
Metrics/base_velocity/error_vel_yaw: 0.2735
      Episode_Termination/time_out: 0.3965
  Episode_Termination/base_contact: 0.6035
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 2.58s
                      Time elapsed: 00:20:59
                               ETA: 00:38:39

################################################################################
                     [1m Learning iteration 493/1400 [0m                      

                       Computation: 9518 steps/s (collection: 2.326s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.4598
                       Mean reward: 8.51
               Mean episode length: 686.94
Episode_Reward/track_lin_vel_xy_exp: 0.5018
Episode_Reward/track_ang_vel_z_exp: 0.2795
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.0535
         Episode_Reward/dof_acc_l2: -0.1266
     Episode_Reward/action_rate_l2: -0.0547
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0835
Metrics/base_velocity/error_vel_xy: 0.3961
Metrics/base_velocity/error_vel_yaw: 0.2953
      Episode_Termination/time_out: 0.3989
  Episode_Termination/base_contact: 0.6011
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 2.58s
                      Time elapsed: 00:21:02
                               ETA: 00:38:37

################################################################################
                     [1m Learning iteration 494/1400 [0m                      

                       Computation: 9745 steps/s (collection: 2.268s, learning 0.254s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.4549
                       Mean reward: 9.15
               Mean episode length: 718.54
Episode_Reward/track_lin_vel_xy_exp: 0.5411
Episode_Reward/track_ang_vel_z_exp: 0.2992
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1189
     Episode_Reward/action_rate_l2: -0.0576
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0236
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0836
Metrics/base_velocity/error_vel_xy: 0.4132
Metrics/base_velocity/error_vel_yaw: 0.3109
      Episode_Termination/time_out: 0.4027
  Episode_Termination/base_contact: 0.5973
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 2.52s
                      Time elapsed: 00:21:04
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 495/1400 [0m                      

                       Computation: 9565 steps/s (collection: 2.316s, learning 0.253s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.4432
                       Mean reward: 8.79
               Mean episode length: 694.34
Episode_Reward/track_lin_vel_xy_exp: 0.4559
Episode_Reward/track_ang_vel_z_exp: 0.2581
       Episode_Reward/lin_vel_z_l2: -0.0359
      Episode_Reward/ang_vel_xy_l2: -0.0437
     Episode_Reward/dof_torques_l2: -0.0521
         Episode_Reward/dof_acc_l2: -0.1070
     Episode_Reward/action_rate_l2: -0.0504
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0882
Metrics/base_velocity/error_vel_xy: 0.3854
Metrics/base_velocity/error_vel_yaw: 0.2725
      Episode_Termination/time_out: 0.4051
  Episode_Termination/base_contact: 0.5949
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.57s
                      Time elapsed: 00:21:07
                               ETA: 00:38:32

################################################################################
                     [1m Learning iteration 496/1400 [0m                      

                       Computation: 9756 steps/s (collection: 2.262s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 6.4458
                       Mean reward: 9.33
               Mean episode length: 708.43
Episode_Reward/track_lin_vel_xy_exp: 0.5156
Episode_Reward/track_ang_vel_z_exp: 0.2936
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0542
         Episode_Reward/dof_acc_l2: -0.1288
     Episode_Reward/action_rate_l2: -0.0562
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0968
Metrics/base_velocity/error_vel_xy: 0.4148
Metrics/base_velocity/error_vel_yaw: 0.2950
      Episode_Termination/time_out: 0.4052
  Episode_Termination/base_contact: 0.5948
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 2.52s
                      Time elapsed: 00:21:09
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 497/1400 [0m                      

                       Computation: 9863 steps/s (collection: 2.236s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.4335
                       Mean reward: 8.77
               Mean episode length: 684.84
Episode_Reward/track_lin_vel_xy_exp: 0.5136
Episode_Reward/track_ang_vel_z_exp: 0.2837
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0544
         Episode_Reward/dof_acc_l2: -0.1223
     Episode_Reward/action_rate_l2: -0.0554
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1017
Metrics/base_velocity/error_vel_xy: 0.4044
Metrics/base_velocity/error_vel_yaw: 0.3028
      Episode_Termination/time_out: 0.4065
  Episode_Termination/base_contact: 0.5935
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 2.49s
                      Time elapsed: 00:21:12
                               ETA: 00:38:26

################################################################################
                     [1m Learning iteration 498/1400 [0m                      

                       Computation: 9822 steps/s (collection: 2.251s, learning 0.251s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.4179
                       Mean reward: 9.15
               Mean episode length: 698.24
Episode_Reward/track_lin_vel_xy_exp: 0.4786
Episode_Reward/track_ang_vel_z_exp: 0.2640
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0458
     Episode_Reward/dof_torques_l2: -0.0509
         Episode_Reward/dof_acc_l2: -0.1149
     Episode_Reward/action_rate_l2: -0.0507
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1100
Metrics/base_velocity/error_vel_xy: 0.3633
Metrics/base_velocity/error_vel_yaw: 0.2693
      Episode_Termination/time_out: 0.4098
  Episode_Termination/base_contact: 0.5902
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 2.50s
                      Time elapsed: 00:21:14
                               ETA: 00:38:24

################################################################################
                     [1m Learning iteration 499/1400 [0m                      

                       Computation: 9922 steps/s (collection: 2.215s, learning 0.262s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 6.4116
                       Mean reward: 8.67
               Mean episode length: 677.96
Episode_Reward/track_lin_vel_xy_exp: 0.4856
Episode_Reward/track_ang_vel_z_exp: 0.2632
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0468
     Episode_Reward/dof_torques_l2: -0.0519
         Episode_Reward/dof_acc_l2: -0.1114
     Episode_Reward/action_rate_l2: -0.0513
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1184
Metrics/base_velocity/error_vel_xy: 0.3541
Metrics/base_velocity/error_vel_yaw: 0.2789
      Episode_Termination/time_out: 0.4069
  Episode_Termination/base_contact: 0.5931
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.48s
                      Time elapsed: 00:21:17
                               ETA: 00:38:21

################################################################################
                     [1m Learning iteration 500/1400 [0m                      

                       Computation: 9598 steps/s (collection: 2.303s, learning 0.258s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 6.4218
                       Mean reward: 9.36
               Mean episode length: 711.27
Episode_Reward/track_lin_vel_xy_exp: 0.5799
Episode_Reward/track_ang_vel_z_exp: 0.3177
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.0512
     Episode_Reward/dof_torques_l2: -0.0596
         Episode_Reward/dof_acc_l2: -0.1304
     Episode_Reward/action_rate_l2: -0.0604
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1368
Metrics/base_velocity/error_vel_xy: 0.4110
Metrics/base_velocity/error_vel_yaw: 0.3050
      Episode_Termination/time_out: 0.4181
  Episode_Termination/base_contact: 0.5819
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 2.56s
                      Time elapsed: 00:21:19
                               ETA: 00:38:19

################################################################################
                     [1m Learning iteration 501/1400 [0m                      

                       Computation: 9753 steps/s (collection: 2.262s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.4297
                       Mean reward: 9.42
               Mean episode length: 718.04
Episode_Reward/track_lin_vel_xy_exp: 0.5101
Episode_Reward/track_ang_vel_z_exp: 0.2953
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0475
     Episode_Reward/dof_torques_l2: -0.0558
         Episode_Reward/dof_acc_l2: -0.1258
     Episode_Reward/action_rate_l2: -0.0565
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1437
Metrics/base_velocity/error_vel_xy: 0.4391
Metrics/base_velocity/error_vel_yaw: 0.2930
      Episode_Termination/time_out: 0.4259
  Episode_Termination/base_contact: 0.5741
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 2.52s
                      Time elapsed: 00:21:22
                               ETA: 00:38:16

################################################################################
                     [1m Learning iteration 502/1400 [0m                      

                       Computation: 9798 steps/s (collection: 2.243s, learning 0.265s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 6.4450
                       Mean reward: 7.91
               Mean episode length: 643.87
Episode_Reward/track_lin_vel_xy_exp: 0.3917
Episode_Reward/track_ang_vel_z_exp: 0.2248
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0409
     Episode_Reward/dof_torques_l2: -0.0459
         Episode_Reward/dof_acc_l2: -0.1096
     Episode_Reward/action_rate_l2: -0.0454
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1471
Metrics/base_velocity/error_vel_xy: 0.3677
Metrics/base_velocity/error_vel_yaw: 0.2667
      Episode_Termination/time_out: 0.4246
  Episode_Termination/base_contact: 0.5754
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 2.51s
                      Time elapsed: 00:21:24
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 503/1400 [0m                      

                       Computation: 9721 steps/s (collection: 2.273s, learning 0.255s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.4650
                       Mean reward: 7.74
               Mean episode length: 648.82
Episode_Reward/track_lin_vel_xy_exp: 0.5059
Episode_Reward/track_ang_vel_z_exp: 0.2794
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0462
     Episode_Reward/dof_torques_l2: -0.0546
         Episode_Reward/dof_acc_l2: -0.1268
     Episode_Reward/action_rate_l2: -0.0558
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1501
Metrics/base_velocity/error_vel_xy: 0.4147
Metrics/base_velocity/error_vel_yaw: 0.3163
      Episode_Termination/time_out: 0.4274
  Episode_Termination/base_contact: 0.5726
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.53s
                      Time elapsed: 00:21:27
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 504/1400 [0m                      

                       Computation: 9764 steps/s (collection: 2.263s, learning 0.254s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.4953
                       Mean reward: 7.38
               Mean episode length: 632.00
Episode_Reward/track_lin_vel_xy_exp: 0.4550
Episode_Reward/track_ang_vel_z_exp: 0.2677
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0456
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1223
     Episode_Reward/action_rate_l2: -0.0525
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1508
Metrics/base_velocity/error_vel_xy: 0.4412
Metrics/base_velocity/error_vel_yaw: 0.2819
      Episode_Termination/time_out: 0.4318
  Episode_Termination/base_contact: 0.5682
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 2.52s
                      Time elapsed: 00:21:29
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 505/1400 [0m                      

                       Computation: 9673 steps/s (collection: 2.285s, learning 0.255s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.4973
                       Mean reward: 8.85
               Mean episode length: 699.00
Episode_Reward/track_lin_vel_xy_exp: 0.4962
Episode_Reward/track_ang_vel_z_exp: 0.2801
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0458
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1206
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1594
Metrics/base_velocity/error_vel_xy: 0.4039
Metrics/base_velocity/error_vel_yaw: 0.2902
      Episode_Termination/time_out: 0.4312
  Episode_Termination/base_contact: 0.5688
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 2.54s
                      Time elapsed: 00:21:32
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 506/1400 [0m                      

                       Computation: 9728 steps/s (collection: 2.271s, learning 0.255s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 6.4864
                       Mean reward: 8.64
               Mean episode length: 689.09
Episode_Reward/track_lin_vel_xy_exp: 0.4956
Episode_Reward/track_ang_vel_z_exp: 0.2817
       Episode_Reward/lin_vel_z_l2: -0.0389
      Episode_Reward/ang_vel_xy_l2: -0.0496
     Episode_Reward/dof_torques_l2: -0.0544
         Episode_Reward/dof_acc_l2: -0.1205
     Episode_Reward/action_rate_l2: -0.0554
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1695
Metrics/base_velocity/error_vel_xy: 0.4272
Metrics/base_velocity/error_vel_yaw: 0.3047
      Episode_Termination/time_out: 0.4367
  Episode_Termination/base_contact: 0.5633
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 2.53s
                      Time elapsed: 00:21:34
                               ETA: 00:38:03

################################################################################
                     [1m Learning iteration 507/1400 [0m                      

                       Computation: 9628 steps/s (collection: 2.300s, learning 0.252s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.4865
                       Mean reward: 9.05
               Mean episode length: 703.68
Episode_Reward/track_lin_vel_xy_exp: 0.4830
Episode_Reward/track_ang_vel_z_exp: 0.2722
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0559
         Episode_Reward/dof_acc_l2: -0.1154
     Episode_Reward/action_rate_l2: -0.0537
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1775
Metrics/base_velocity/error_vel_xy: 0.4123
Metrics/base_velocity/error_vel_yaw: 0.3041
      Episode_Termination/time_out: 0.4351
  Episode_Termination/base_contact: 0.5649
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.55s
                      Time elapsed: 00:21:37
                               ETA: 00:38:00

################################################################################
                     [1m Learning iteration 508/1400 [0m                      

                       Computation: 9708 steps/s (collection: 2.275s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.4830
                       Mean reward: 8.21
               Mean episode length: 687.53
Episode_Reward/track_lin_vel_xy_exp: 0.4350
Episode_Reward/track_ang_vel_z_exp: 0.2657
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1243
     Episode_Reward/action_rate_l2: -0.0529
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1896
Metrics/base_velocity/error_vel_xy: 0.4835
Metrics/base_velocity/error_vel_yaw: 0.3049
      Episode_Termination/time_out: 0.4358
  Episode_Termination/base_contact: 0.5642
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 2.53s
                      Time elapsed: 00:21:40
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 509/1400 [0m                      

                       Computation: 9715 steps/s (collection: 2.273s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.4724
                       Mean reward: 7.91
               Mean episode length: 679.94
Episode_Reward/track_lin_vel_xy_exp: 0.4717
Episode_Reward/track_ang_vel_z_exp: 0.2794
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.0478
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1268
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2018
Metrics/base_velocity/error_vel_xy: 0.4468
Metrics/base_velocity/error_vel_yaw: 0.2964
      Episode_Termination/time_out: 0.4340
  Episode_Termination/base_contact: 0.5660
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 2.53s
                      Time elapsed: 00:21:42
                               ETA: 00:37:55

################################################################################
                     [1m Learning iteration 510/1400 [0m                      

                       Computation: 9693 steps/s (collection: 2.278s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 6.4626
                       Mean reward: 7.98
               Mean episode length: 679.81
Episode_Reward/track_lin_vel_xy_exp: 0.4095
Episode_Reward/track_ang_vel_z_exp: 0.2445
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0439
     Episode_Reward/dof_torques_l2: -0.0506
         Episode_Reward/dof_acc_l2: -0.1282
     Episode_Reward/action_rate_l2: -0.0490
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2115
Metrics/base_velocity/error_vel_xy: 0.4268
Metrics/base_velocity/error_vel_yaw: 0.2885
      Episode_Termination/time_out: 0.4367
  Episode_Termination/base_contact: 0.5633
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 2.54s
                      Time elapsed: 00:21:45
                               ETA: 00:37:53

################################################################################
                     [1m Learning iteration 511/1400 [0m                      

                       Computation: 9696 steps/s (collection: 2.279s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.4631
                       Mean reward: 8.80
               Mean episode length: 704.02
Episode_Reward/track_lin_vel_xy_exp: 0.5445
Episode_Reward/track_ang_vel_z_exp: 0.3069
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.0517
     Episode_Reward/dof_torques_l2: -0.0595
         Episode_Reward/dof_acc_l2: -0.1313
     Episode_Reward/action_rate_l2: -0.0592
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2276
Metrics/base_velocity/error_vel_xy: 0.4493
Metrics/base_velocity/error_vel_yaw: 0.3241
      Episode_Termination/time_out: 0.4403
  Episode_Termination/base_contact: 0.5597
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.53s
                      Time elapsed: 00:21:47
                               ETA: 00:37:50

################################################################################
                     [1m Learning iteration 512/1400 [0m                      

                       Computation: 9734 steps/s (collection: 2.271s, learning 0.254s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.4636
                       Mean reward: 8.54
               Mean episode length: 697.51
Episode_Reward/track_lin_vel_xy_exp: 0.4846
Episode_Reward/track_ang_vel_z_exp: 0.2889
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.0482
     Episode_Reward/dof_torques_l2: -0.0575
         Episode_Reward/dof_acc_l2: -0.1279
     Episode_Reward/action_rate_l2: -0.0565
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2376
Metrics/base_velocity/error_vel_xy: 0.4754
Metrics/base_velocity/error_vel_yaw: 0.3054
      Episode_Termination/time_out: 0.4434
  Episode_Termination/base_contact: 0.5566
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 2.52s
                      Time elapsed: 00:21:50
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 513/1400 [0m                      

                       Computation: 9739 steps/s (collection: 2.267s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.4693
                       Mean reward: 8.29
               Mean episode length: 674.92
Episode_Reward/track_lin_vel_xy_exp: 0.4217
Episode_Reward/track_ang_vel_z_exp: 0.2366
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.0474
         Episode_Reward/dof_acc_l2: -0.1089
     Episode_Reward/action_rate_l2: -0.0466
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2399
Metrics/base_velocity/error_vel_xy: 0.3348
Metrics/base_velocity/error_vel_yaw: 0.2436
      Episode_Termination/time_out: 0.4438
  Episode_Termination/base_contact: 0.5562
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 2.52s
                      Time elapsed: 00:21:52
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 514/1400 [0m                      

                       Computation: 9488 steps/s (collection: 2.329s, learning 0.261s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.4776
                       Mean reward: 7.83
               Mean episode length: 627.89
Episode_Reward/track_lin_vel_xy_exp: 0.4442
Episode_Reward/track_ang_vel_z_exp: 0.2506
       Episode_Reward/lin_vel_z_l2: -0.0355
      Episode_Reward/ang_vel_xy_l2: -0.0443
     Episode_Reward/dof_torques_l2: -0.0494
         Episode_Reward/dof_acc_l2: -0.1128
     Episode_Reward/action_rate_l2: -0.0493
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2391
Metrics/base_velocity/error_vel_xy: 0.3700
Metrics/base_velocity/error_vel_yaw: 0.2732
      Episode_Termination/time_out: 0.4388
  Episode_Termination/base_contact: 0.5612
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 2.59s
                      Time elapsed: 00:21:55
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 515/1400 [0m                      

                       Computation: 9585 steps/s (collection: 2.309s, learning 0.255s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 6.4801
                       Mean reward: 7.38
               Mean episode length: 615.44
Episode_Reward/track_lin_vel_xy_exp: 0.4187
Episode_Reward/track_ang_vel_z_exp: 0.2435
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0441
     Episode_Reward/dof_torques_l2: -0.0514
         Episode_Reward/dof_acc_l2: -0.1183
     Episode_Reward/action_rate_l2: -0.0487
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0169
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2474
Metrics/base_velocity/error_vel_xy: 0.4106
Metrics/base_velocity/error_vel_yaw: 0.2842
      Episode_Termination/time_out: 0.4397
  Episode_Termination/base_contact: 0.5603
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.56s
                      Time elapsed: 00:21:57
                               ETA: 00:37:40

################################################################################
                     [1m Learning iteration 516/1400 [0m                      

                       Computation: 9904 steps/s (collection: 2.219s, learning 0.263s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.4689
                       Mean reward: 7.49
               Mean episode length: 613.86
Episode_Reward/track_lin_vel_xy_exp: 0.3337
Episode_Reward/track_ang_vel_z_exp: 0.1831
       Episode_Reward/lin_vel_z_l2: -0.0280
      Episode_Reward/ang_vel_xy_l2: -0.0327
     Episode_Reward/dof_torques_l2: -0.0375
         Episode_Reward/dof_acc_l2: -0.0852
     Episode_Reward/action_rate_l2: -0.0365
      Episode_Reward/feet_air_time: -0.0055
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2517
Metrics/base_velocity/error_vel_xy: 0.2673
Metrics/base_velocity/error_vel_yaw: 0.2016
      Episode_Termination/time_out: 0.4398
  Episode_Termination/base_contact: 0.5602
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 2.48s
                      Time elapsed: 00:22:00
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 517/1400 [0m                      

                       Computation: 9829 steps/s (collection: 2.239s, learning 0.261s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 6.4525
                       Mean reward: 6.61
               Mean episode length: 574.24
Episode_Reward/track_lin_vel_xy_exp: 0.3796
Episode_Reward/track_ang_vel_z_exp: 0.2144
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0419
     Episode_Reward/dof_torques_l2: -0.0445
         Episode_Reward/dof_acc_l2: -0.1004
     Episode_Reward/action_rate_l2: -0.0443
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2530
Metrics/base_velocity/error_vel_xy: 0.3728
Metrics/base_velocity/error_vel_yaw: 0.2730
      Episode_Termination/time_out: 0.4357
  Episode_Termination/base_contact: 0.5643
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 2.50s
                      Time elapsed: 00:22:02
                               ETA: 00:37:34

################################################################################
                     [1m Learning iteration 518/1400 [0m                      

                       Computation: 9764 steps/s (collection: 2.261s, learning 0.256s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.4443
                       Mean reward: 7.27
               Mean episode length: 617.17
Episode_Reward/track_lin_vel_xy_exp: 0.4540
Episode_Reward/track_ang_vel_z_exp: 0.2632
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.0482
     Episode_Reward/dof_torques_l2: -0.0533
         Episode_Reward/dof_acc_l2: -0.1199
     Episode_Reward/action_rate_l2: -0.0525
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0076
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2559
Metrics/base_velocity/error_vel_xy: 0.4332
Metrics/base_velocity/error_vel_yaw: 0.3032
      Episode_Termination/time_out: 0.4322
  Episode_Termination/base_contact: 0.5678
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 2.52s
                      Time elapsed: 00:22:05
                               ETA: 00:37:32

################################################################################
                     [1m Learning iteration 519/1400 [0m                      

                       Computation: 9604 steps/s (collection: 2.302s, learning 0.257s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 6.4320
                       Mean reward: 8.03
               Mean episode length: 673.39
Episode_Reward/track_lin_vel_xy_exp: 0.4773
Episode_Reward/track_ang_vel_z_exp: 0.2724
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0527
         Episode_Reward/dof_acc_l2: -0.1170
     Episode_Reward/action_rate_l2: -0.0533
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2649
Metrics/base_velocity/error_vel_xy: 0.3967
Metrics/base_velocity/error_vel_yaw: 0.2765
      Episode_Termination/time_out: 0.4375
  Episode_Termination/base_contact: 0.5625
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.56s
                      Time elapsed: 00:22:07
                               ETA: 00:37:29

################################################################################
                     [1m Learning iteration 520/1400 [0m                      

                       Computation: 9724 steps/s (collection: 2.269s, learning 0.258s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.4274
                       Mean reward: 8.35
               Mean episode length: 678.34
Episode_Reward/track_lin_vel_xy_exp: 0.4613
Episode_Reward/track_ang_vel_z_exp: 0.2613
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1220
     Episode_Reward/action_rate_l2: -0.0522
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2654
Metrics/base_velocity/error_vel_xy: 0.3953
Metrics/base_velocity/error_vel_yaw: 0.2826
      Episode_Termination/time_out: 0.4334
  Episode_Termination/base_contact: 0.5666
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 2.53s
                      Time elapsed: 00:22:10
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 521/1400 [0m                      

                       Computation: 9686 steps/s (collection: 2.283s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.4119
                       Mean reward: 7.72
               Mean episode length: 663.95
Episode_Reward/track_lin_vel_xy_exp: 0.4731
Episode_Reward/track_ang_vel_z_exp: 0.2752
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.0511
     Episode_Reward/dof_torques_l2: -0.0559
         Episode_Reward/dof_acc_l2: -0.1287
     Episode_Reward/action_rate_l2: -0.0552
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2810
Metrics/base_velocity/error_vel_xy: 0.4458
Metrics/base_velocity/error_vel_yaw: 0.3097
      Episode_Termination/time_out: 0.4375
  Episode_Termination/base_contact: 0.5625
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 2.54s
                      Time elapsed: 00:22:12
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 522/1400 [0m                      

                       Computation: 9742 steps/s (collection: 2.268s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 6.3982
                       Mean reward: 8.26
               Mean episode length: 682.76
Episode_Reward/track_lin_vel_xy_exp: 0.4632
Episode_Reward/track_ang_vel_z_exp: 0.2638
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0428
     Episode_Reward/dof_torques_l2: -0.0523
         Episode_Reward/dof_acc_l2: -0.1123
     Episode_Reward/action_rate_l2: -0.0516
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2910
Metrics/base_velocity/error_vel_xy: 0.3876
Metrics/base_velocity/error_vel_yaw: 0.2768
      Episode_Termination/time_out: 0.4416
  Episode_Termination/base_contact: 0.5584
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 2.52s
                      Time elapsed: 00:22:15
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 523/1400 [0m                      

                       Computation: 9715 steps/s (collection: 2.274s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.3901
                       Mean reward: 7.95
               Mean episode length: 672.15
Episode_Reward/track_lin_vel_xy_exp: 0.4325
Episode_Reward/track_ang_vel_z_exp: 0.2675
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0478
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1224
     Episode_Reward/action_rate_l2: -0.0532
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2982
Metrics/base_velocity/error_vel_xy: 0.4793
Metrics/base_velocity/error_vel_yaw: 0.2975
      Episode_Termination/time_out: 0.4451
  Episode_Termination/base_contact: 0.5549
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.53s
                      Time elapsed: 00:22:18
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 524/1400 [0m                      

                       Computation: 9752 steps/s (collection: 2.262s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 6.3955
                       Mean reward: 8.27
               Mean episode length: 680.58
Episode_Reward/track_lin_vel_xy_exp: 0.4623
Episode_Reward/track_ang_vel_z_exp: 0.2550
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0440
     Episode_Reward/dof_torques_l2: -0.0505
         Episode_Reward/dof_acc_l2: -0.1126
     Episode_Reward/action_rate_l2: -0.0507
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0139
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3018
Metrics/base_velocity/error_vel_xy: 0.3646
Metrics/base_velocity/error_vel_yaw: 0.2766
      Episode_Termination/time_out: 0.4465
  Episode_Termination/base_contact: 0.5535
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 2.52s
                      Time elapsed: 00:22:20
                               ETA: 00:37:16

################################################################################
                     [1m Learning iteration 525/1400 [0m                      

                       Computation: 9729 steps/s (collection: 2.269s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0176
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 6.3921
                       Mean reward: 7.71
               Mean episode length: 650.92
Episode_Reward/track_lin_vel_xy_exp: 0.4103
Episode_Reward/track_ang_vel_z_exp: 0.2372
       Episode_Reward/lin_vel_z_l2: -0.0367
      Episode_Reward/ang_vel_xy_l2: -0.0435
     Episode_Reward/dof_torques_l2: -0.0512
         Episode_Reward/dof_acc_l2: -0.1186
     Episode_Reward/action_rate_l2: -0.0486
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3057
Metrics/base_velocity/error_vel_xy: 0.4035
Metrics/base_velocity/error_vel_yaw: 0.2912
      Episode_Termination/time_out: 0.4493
  Episode_Termination/base_contact: 0.5507
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 2.53s
                      Time elapsed: 00:22:23
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 526/1400 [0m                      

                       Computation: 9688 steps/s (collection: 2.278s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.3955
                       Mean reward: 7.81
               Mean episode length: 644.54
Episode_Reward/track_lin_vel_xy_exp: 0.4312
Episode_Reward/track_ang_vel_z_exp: 0.2447
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0446
     Episode_Reward/dof_torques_l2: -0.0486
         Episode_Reward/dof_acc_l2: -0.1076
     Episode_Reward/action_rate_l2: -0.0477
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3154
Metrics/base_velocity/error_vel_xy: 0.3685
Metrics/base_velocity/error_vel_yaw: 0.2623
      Episode_Termination/time_out: 0.4529
  Episode_Termination/base_contact: 0.5471
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 2.54s
                      Time elapsed: 00:22:25
                               ETA: 00:37:11

################################################################################
                     [1m Learning iteration 527/1400 [0m                      

                       Computation: 9781 steps/s (collection: 2.257s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.3989
                       Mean reward: 7.53
               Mean episode length: 607.06
Episode_Reward/track_lin_vel_xy_exp: 0.4362
Episode_Reward/track_ang_vel_z_exp: 0.2539
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0448
     Episode_Reward/dof_torques_l2: -0.0507
         Episode_Reward/dof_acc_l2: -0.1028
     Episode_Reward/action_rate_l2: -0.0495
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3171
Metrics/base_velocity/error_vel_xy: 0.3897
Metrics/base_velocity/error_vel_yaw: 0.2716
      Episode_Termination/time_out: 0.4514
  Episode_Termination/base_contact: 0.5486
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.51s
                      Time elapsed: 00:22:28
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 528/1400 [0m                      

                       Computation: 9763 steps/s (collection: 2.261s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.3972
                       Mean reward: 7.54
               Mean episode length: 614.66
Episode_Reward/track_lin_vel_xy_exp: 0.4962
Episode_Reward/track_ang_vel_z_exp: 0.2796
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0535
         Episode_Reward/dof_acc_l2: -0.1247
     Episode_Reward/action_rate_l2: -0.0542
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3151
Metrics/base_velocity/error_vel_xy: 0.3970
Metrics/base_velocity/error_vel_yaw: 0.2839
      Episode_Termination/time_out: 0.4462
  Episode_Termination/base_contact: 0.5538
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 2.52s
                      Time elapsed: 00:22:30
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 529/1400 [0m                      

                       Computation: 9898 steps/s (collection: 2.225s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 6.3876
                       Mean reward: 8.59
               Mean episode length: 682.91
Episode_Reward/track_lin_vel_xy_exp: 0.5110
Episode_Reward/track_ang_vel_z_exp: 0.2928
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0482
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1226
     Episode_Reward/action_rate_l2: -0.0565
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3271
Metrics/base_velocity/error_vel_xy: 0.4208
Metrics/base_velocity/error_vel_yaw: 0.2900
      Episode_Termination/time_out: 0.4503
  Episode_Termination/base_contact: 0.5497
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 2.48s
                      Time elapsed: 00:22:33
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 530/1400 [0m                      

                       Computation: 9765 steps/s (collection: 2.259s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.3740
                       Mean reward: 8.75
               Mean episode length: 696.91
Episode_Reward/track_lin_vel_xy_exp: 0.4357
Episode_Reward/track_ang_vel_z_exp: 0.2473
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0436
     Episode_Reward/dof_torques_l2: -0.0507
         Episode_Reward/dof_acc_l2: -0.1112
     Episode_Reward/action_rate_l2: -0.0497
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3331
Metrics/base_velocity/error_vel_xy: 0.3832
Metrics/base_velocity/error_vel_yaw: 0.2798
      Episode_Termination/time_out: 0.4513
  Episode_Termination/base_contact: 0.5487
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 2.52s
                      Time elapsed: 00:22:35
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 531/1400 [0m                      

                       Computation: 9829 steps/s (collection: 2.242s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 6.3812
                       Mean reward: 9.17
               Mean episode length: 718.20
Episode_Reward/track_lin_vel_xy_exp: 0.4223
Episode_Reward/track_ang_vel_z_exp: 0.2447
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.0453
     Episode_Reward/dof_torques_l2: -0.0483
         Episode_Reward/dof_acc_l2: -0.1065
     Episode_Reward/action_rate_l2: -0.0482
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3250
Metrics/base_velocity/error_vel_xy: 0.3796
Metrics/base_velocity/error_vel_yaw: 0.2638
      Episode_Termination/time_out: 0.4511
  Episode_Termination/base_contact: 0.5489
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.50s
                      Time elapsed: 00:22:38
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 532/1400 [0m                      

                       Computation: 9748 steps/s (collection: 2.266s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.3848
                       Mean reward: 7.47
               Mean episode length: 616.70
Episode_Reward/track_lin_vel_xy_exp: 0.3831
Episode_Reward/track_ang_vel_z_exp: 0.2279
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.0431
     Episode_Reward/dof_torques_l2: -0.0470
         Episode_Reward/dof_acc_l2: -0.1175
     Episode_Reward/action_rate_l2: -0.0459
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3295
Metrics/base_velocity/error_vel_xy: 0.3801
Metrics/base_velocity/error_vel_yaw: 0.2544
      Episode_Termination/time_out: 0.4473
  Episode_Termination/base_contact: 0.5527
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 2.52s
                      Time elapsed: 00:22:40
                               ETA: 00:36:55

################################################################################
                     [1m Learning iteration 533/1400 [0m                      

                       Computation: 9734 steps/s (collection: 2.269s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.3766
                       Mean reward: 7.16
               Mean episode length: 612.18
Episode_Reward/track_lin_vel_xy_exp: 0.4695
Episode_Reward/track_ang_vel_z_exp: 0.2716
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.0480
     Episode_Reward/dof_torques_l2: -0.0555
         Episode_Reward/dof_acc_l2: -0.1334
     Episode_Reward/action_rate_l2: -0.0541
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3330
Metrics/base_velocity/error_vel_xy: 0.4261
Metrics/base_velocity/error_vel_yaw: 0.2892
      Episode_Termination/time_out: 0.4430
  Episode_Termination/base_contact: 0.5570
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 2.52s
                      Time elapsed: 00:22:43
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 534/1400 [0m                      

                       Computation: 9738 steps/s (collection: 2.260s, learning 0.263s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 6.3710
                       Mean reward: 7.26
               Mean episode length: 617.68
Episode_Reward/track_lin_vel_xy_exp: 0.4201
Episode_Reward/track_ang_vel_z_exp: 0.2407
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0435
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.1178
     Episode_Reward/action_rate_l2: -0.0479
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0079
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3337
Metrics/base_velocity/error_vel_xy: 0.3642
Metrics/base_velocity/error_vel_yaw: 0.2536
      Episode_Termination/time_out: 0.4445
  Episode_Termination/base_contact: 0.5555
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 2.52s
                      Time elapsed: 00:22:45
                               ETA: 00:36:50

################################################################################
                     [1m Learning iteration 535/1400 [0m                      

                       Computation: 9803 steps/s (collection: 2.251s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 6.3659
                       Mean reward: 8.08
               Mean episode length: 653.81
Episode_Reward/track_lin_vel_xy_exp: 0.5926
Episode_Reward/track_ang_vel_z_exp: 0.3237
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.0521
     Episode_Reward/dof_torques_l2: -0.0595
         Episode_Reward/dof_acc_l2: -0.1328
     Episode_Reward/action_rate_l2: -0.0621
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3405
Metrics/base_velocity/error_vel_xy: 0.4088
Metrics/base_velocity/error_vel_yaw: 0.3062
      Episode_Termination/time_out: 0.4465
  Episode_Termination/base_contact: 0.5535
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.51s
                      Time elapsed: 00:22:48
                               ETA: 00:36:48

################################################################################
                     [1m Learning iteration 536/1400 [0m                      

                       Computation: 9770 steps/s (collection: 2.258s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.3574
                       Mean reward: 8.62
               Mean episode length: 691.39
Episode_Reward/track_lin_vel_xy_exp: 0.5257
Episode_Reward/track_ang_vel_z_exp: 0.3048
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.0541
     Episode_Reward/dof_torques_l2: -0.0610
         Episode_Reward/dof_acc_l2: -0.1475
     Episode_Reward/action_rate_l2: -0.0605
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3507
Metrics/base_velocity/error_vel_xy: 0.4758
Metrics/base_velocity/error_vel_yaw: 0.3322
      Episode_Termination/time_out: 0.4514
  Episode_Termination/base_contact: 0.5496
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 2.52s
                      Time elapsed: 00:22:50
                               ETA: 00:36:45

################################################################################
                     [1m Learning iteration 537/1400 [0m                      

                       Computation: 9758 steps/s (collection: 2.255s, learning 0.264s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 6.3558
                       Mean reward: 8.65
               Mean episode length: 695.13
Episode_Reward/track_lin_vel_xy_exp: 0.4198
Episode_Reward/track_ang_vel_z_exp: 0.2426
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0420
     Episode_Reward/dof_torques_l2: -0.0496
         Episode_Reward/dof_acc_l2: -0.1123
     Episode_Reward/action_rate_l2: -0.0478
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3542
Metrics/base_velocity/error_vel_xy: 0.3694
Metrics/base_velocity/error_vel_yaw: 0.2514
      Episode_Termination/time_out: 0.4513
  Episode_Termination/base_contact: 0.5496
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 2.52s
                      Time elapsed: 00:22:53
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 538/1400 [0m                      

                       Computation: 9822 steps/s (collection: 2.243s, learning 0.259s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 6.3626
                       Mean reward: 7.66
               Mean episode length: 636.30
Episode_Reward/track_lin_vel_xy_exp: 0.4928
Episode_Reward/track_ang_vel_z_exp: 0.2817
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.0499
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1284
     Episode_Reward/action_rate_l2: -0.0558
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3600
Metrics/base_velocity/error_vel_xy: 0.4400
Metrics/base_velocity/error_vel_yaw: 0.3168
      Episode_Termination/time_out: 0.4468
  Episode_Termination/base_contact: 0.5542
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 2.50s
                      Time elapsed: 00:22:55
                               ETA: 00:36:40

################################################################################
                     [1m Learning iteration 539/1400 [0m                      

                       Computation: 9806 steps/s (collection: 2.252s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.3602
                       Mean reward: 8.22
               Mean episode length: 651.09
Episode_Reward/track_lin_vel_xy_exp: 0.5092
Episode_Reward/track_ang_vel_z_exp: 0.2846
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0515
         Episode_Reward/dof_acc_l2: -0.1185
     Episode_Reward/action_rate_l2: -0.0528
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3666
Metrics/base_velocity/error_vel_xy: 0.3779
Metrics/base_velocity/error_vel_yaw: 0.2742
      Episode_Termination/time_out: 0.4457
  Episode_Termination/base_contact: 0.5553
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.51s
                      Time elapsed: 00:22:58
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 540/1400 [0m                      

                       Computation: 9714 steps/s (collection: 2.277s, learning 0.253s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.3636
                       Mean reward: 8.10
               Mean episode length: 645.89
Episode_Reward/track_lin_vel_xy_exp: 0.4050
Episode_Reward/track_ang_vel_z_exp: 0.2283
       Episode_Reward/lin_vel_z_l2: -0.0327
      Episode_Reward/ang_vel_xy_l2: -0.0405
     Episode_Reward/dof_torques_l2: -0.0448
         Episode_Reward/dof_acc_l2: -0.1021
     Episode_Reward/action_rate_l2: -0.0451
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0110
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3663
Metrics/base_velocity/error_vel_xy: 0.3328
Metrics/base_velocity/error_vel_yaw: 0.2401
      Episode_Termination/time_out: 0.4373
  Episode_Termination/base_contact: 0.5637
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 2.53s
                      Time elapsed: 00:23:00
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 541/1400 [0m                      

                       Computation: 9760 steps/s (collection: 2.262s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.3688
                       Mean reward: 8.21
               Mean episode length: 633.47
Episode_Reward/track_lin_vel_xy_exp: 0.4819
Episode_Reward/track_ang_vel_z_exp: 0.2578
       Episode_Reward/lin_vel_z_l2: -0.0389
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0501
         Episode_Reward/dof_acc_l2: -0.1179
     Episode_Reward/action_rate_l2: -0.0504
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3683
Metrics/base_velocity/error_vel_xy: 0.3404
Metrics/base_velocity/error_vel_yaw: 0.2772
      Episode_Termination/time_out: 0.4334
  Episode_Termination/base_contact: 0.5676
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 2.52s
                      Time elapsed: 00:23:03
                               ETA: 00:36:32

################################################################################
                     [1m Learning iteration 542/1400 [0m                      

                       Computation: 9843 steps/s (collection: 2.240s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.3750
                       Mean reward: 8.47
               Mean episode length: 664.69
Episode_Reward/track_lin_vel_xy_exp: 0.4387
Episode_Reward/track_ang_vel_z_exp: 0.2585
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0468
     Episode_Reward/dof_torques_l2: -0.0541
         Episode_Reward/dof_acc_l2: -0.1180
     Episode_Reward/action_rate_l2: -0.0518
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3761
Metrics/base_velocity/error_vel_xy: 0.4244
Metrics/base_velocity/error_vel_yaw: 0.2904
      Episode_Termination/time_out: 0.4301
  Episode_Termination/base_contact: 0.5709
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 2.50s
                      Time elapsed: 00:23:05
                               ETA: 00:36:29

################################################################################
                     [1m Learning iteration 543/1400 [0m                      

                       Computation: 9760 steps/s (collection: 2.263s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 6.3801
                       Mean reward: 8.30
               Mean episode length: 664.07
Episode_Reward/track_lin_vel_xy_exp: 0.4335
Episode_Reward/track_ang_vel_z_exp: 0.2502
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.0428
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.1093
     Episode_Reward/action_rate_l2: -0.0498
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3862
Metrics/base_velocity/error_vel_xy: 0.4054
Metrics/base_velocity/error_vel_yaw: 0.2856
      Episode_Termination/time_out: 0.4326
  Episode_Termination/base_contact: 0.5684
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.52s
                      Time elapsed: 00:23:08
                               ETA: 00:36:27

################################################################################
                     [1m Learning iteration 544/1400 [0m                      

                       Computation: 9790 steps/s (collection: 2.255s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 6.3730
                       Mean reward: 7.82
               Mean episode length: 628.90
Episode_Reward/track_lin_vel_xy_exp: 0.3968
Episode_Reward/track_ang_vel_z_exp: 0.2312
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.1038
     Episode_Reward/action_rate_l2: -0.0452
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3906
Metrics/base_velocity/error_vel_xy: 0.3657
Metrics/base_velocity/error_vel_yaw: 0.2487
      Episode_Termination/time_out: 0.4329
  Episode_Termination/base_contact: 0.5681
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 2.51s
                      Time elapsed: 00:23:10
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 545/1400 [0m                      

                       Computation: 9746 steps/s (collection: 2.258s, learning 0.263s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.3678
                       Mean reward: 6.51
               Mean episode length: 553.60
Episode_Reward/track_lin_vel_xy_exp: 0.4001
Episode_Reward/track_ang_vel_z_exp: 0.2309
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0413
     Episode_Reward/dof_torques_l2: -0.0476
         Episode_Reward/dof_acc_l2: -0.1065
     Episode_Reward/action_rate_l2: -0.0460
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3874
Metrics/base_velocity/error_vel_xy: 0.3612
Metrics/base_velocity/error_vel_yaw: 0.2539
      Episode_Termination/time_out: 0.4246
  Episode_Termination/base_contact: 0.5763
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 2.52s
                      Time elapsed: 00:23:13
                               ETA: 00:36:21

################################################################################
                     [1m Learning iteration 546/1400 [0m                      

                       Computation: 9876 steps/s (collection: 2.232s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.3625
                       Mean reward: 7.05
               Mean episode length: 579.93
Episode_Reward/track_lin_vel_xy_exp: 0.5531
Episode_Reward/track_ang_vel_z_exp: 0.3042
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.0521
     Episode_Reward/dof_torques_l2: -0.0537
         Episode_Reward/dof_acc_l2: -0.1373
     Episode_Reward/action_rate_l2: -0.0575
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3911
Metrics/base_velocity/error_vel_xy: 0.3968
Metrics/base_velocity/error_vel_yaw: 0.2916
      Episode_Termination/time_out: 0.4255
  Episode_Termination/base_contact: 0.5754
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 2.49s
                      Time elapsed: 00:23:15
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 547/1400 [0m                      

                       Computation: 9800 steps/s (collection: 2.244s, learning 0.263s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.3526
                       Mean reward: 7.95
               Mean episode length: 630.83
Episode_Reward/track_lin_vel_xy_exp: 0.4359
Episode_Reward/track_ang_vel_z_exp: 0.2494
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0427
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.1169
     Episode_Reward/action_rate_l2: -0.0483
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4009
Metrics/base_velocity/error_vel_xy: 0.3793
Metrics/base_velocity/error_vel_yaw: 0.2647
      Episode_Termination/time_out: 0.4247
  Episode_Termination/base_contact: 0.5763
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.51s
                      Time elapsed: 00:23:18
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 548/1400 [0m                      

                       Computation: 9725 steps/s (collection: 2.270s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.3506
                       Mean reward: 8.24
               Mean episode length: 650.45
Episode_Reward/track_lin_vel_xy_exp: 0.4619
Episode_Reward/track_ang_vel_z_exp: 0.2653
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0468
     Episode_Reward/dof_torques_l2: -0.0551
         Episode_Reward/dof_acc_l2: -0.1294
     Episode_Reward/action_rate_l2: -0.0539
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3956
Metrics/base_velocity/error_vel_xy: 0.4325
Metrics/base_velocity/error_vel_yaw: 0.3078
      Episode_Termination/time_out: 0.4229
  Episode_Termination/base_contact: 0.5781
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 2.53s
                      Time elapsed: 00:23:20
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 549/1400 [0m                      

                       Computation: 9728 steps/s (collection: 2.268s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 6.3546
                       Mean reward: 7.54
               Mean episode length: 646.33
Episode_Reward/track_lin_vel_xy_exp: 0.3802
Episode_Reward/track_ang_vel_z_exp: 0.2263
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0438
     Episode_Reward/dof_torques_l2: -0.0506
         Episode_Reward/dof_acc_l2: -0.1193
     Episode_Reward/action_rate_l2: -0.0474
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0078
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4040
Metrics/base_velocity/error_vel_xy: 0.4299
Metrics/base_velocity/error_vel_yaw: 0.2954
      Episode_Termination/time_out: 0.4246
  Episode_Termination/base_contact: 0.5764
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 2.53s
                      Time elapsed: 00:23:23
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 550/1400 [0m                      

                       Computation: 9696 steps/s (collection: 2.278s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.3649
                       Mean reward: 7.54
               Mean episode length: 641.41
Episode_Reward/track_lin_vel_xy_exp: 0.4403
Episode_Reward/track_ang_vel_z_exp: 0.2478
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0449
     Episode_Reward/dof_torques_l2: -0.0489
         Episode_Reward/dof_acc_l2: -0.1151
     Episode_Reward/action_rate_l2: -0.0486
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4078
Metrics/base_velocity/error_vel_xy: 0.3612
Metrics/base_velocity/error_vel_yaw: 0.2593
      Episode_Termination/time_out: 0.4226
  Episode_Termination/base_contact: 0.5783
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 2.53s
                      Time elapsed: 00:23:25
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 551/1400 [0m                      

                       Computation: 9821 steps/s (collection: 2.246s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.3617
                       Mean reward: 8.30
               Mean episode length: 658.16
Episode_Reward/track_lin_vel_xy_exp: 0.5276
Episode_Reward/track_ang_vel_z_exp: 0.2975
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.0515
     Episode_Reward/dof_torques_l2: -0.0596
         Episode_Reward/dof_acc_l2: -0.1286
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4173
Metrics/base_velocity/error_vel_xy: 0.4205
Metrics/base_velocity/error_vel_yaw: 0.3009
      Episode_Termination/time_out: 0.4165
  Episode_Termination/base_contact: 0.5845
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.50s
                      Time elapsed: 00:23:28
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 552/1400 [0m                      

                       Computation: 9769 steps/s (collection: 2.261s, learning 0.254s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 6.3603
                       Mean reward: 8.31
               Mean episode length: 671.82
Episode_Reward/track_lin_vel_xy_exp: 0.4941
Episode_Reward/track_ang_vel_z_exp: 0.2982
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.0535
     Episode_Reward/dof_torques_l2: -0.0587
         Episode_Reward/dof_acc_l2: -0.1322
     Episode_Reward/action_rate_l2: -0.0582
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4261
Metrics/base_velocity/error_vel_xy: 0.5007
Metrics/base_velocity/error_vel_yaw: 0.3243
      Episode_Termination/time_out: 0.4149
  Episode_Termination/base_contact: 0.5861
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 2.52s
                      Time elapsed: 00:23:30
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 553/1400 [0m                      

                       Computation: 9773 steps/s (collection: 2.259s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.3474
                       Mean reward: 8.38
               Mean episode length: 700.75
Episode_Reward/track_lin_vel_xy_exp: 0.4638
Episode_Reward/track_ang_vel_z_exp: 0.2772
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0503
     Episode_Reward/dof_torques_l2: -0.0563
         Episode_Reward/dof_acc_l2: -0.1333
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4348
Metrics/base_velocity/error_vel_xy: 0.4564
Metrics/base_velocity/error_vel_yaw: 0.3008
      Episode_Termination/time_out: 0.4191
  Episode_Termination/base_contact: 0.5815
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 2.51s
                      Time elapsed: 00:23:33
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 554/1400 [0m                      

                       Computation: 9808 steps/s (collection: 2.250s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.3303
                       Mean reward: 7.60
               Mean episode length: 652.45
Episode_Reward/track_lin_vel_xy_exp: 0.3833
Episode_Reward/track_ang_vel_z_exp: 0.2246
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0387
     Episode_Reward/dof_torques_l2: -0.0448
         Episode_Reward/dof_acc_l2: -0.1029
     Episode_Reward/action_rate_l2: -0.0437
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4318
Metrics/base_velocity/error_vel_xy: 0.3498
Metrics/base_velocity/error_vel_yaw: 0.2370
      Episode_Termination/time_out: 0.4157
  Episode_Termination/base_contact: 0.5843
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 2.51s
                      Time elapsed: 00:23:35
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 555/1400 [0m                      

                       Computation: 9783 steps/s (collection: 2.259s, learning 0.253s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.3230
                       Mean reward: 7.52
               Mean episode length: 619.00
Episode_Reward/track_lin_vel_xy_exp: 0.4282
Episode_Reward/track_ang_vel_z_exp: 0.2356
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0402
     Episode_Reward/dof_torques_l2: -0.0452
         Episode_Reward/dof_acc_l2: -0.1127
     Episode_Reward/action_rate_l2: -0.0461
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4395
Metrics/base_velocity/error_vel_xy: 0.3194
Metrics/base_velocity/error_vel_yaw: 0.2435
      Episode_Termination/time_out: 0.4095
  Episode_Termination/base_contact: 0.5905
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.51s
                      Time elapsed: 00:23:38
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 556/1400 [0m                      

                       Computation: 9763 steps/s (collection: 2.261s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 6.3270
                       Mean reward: 7.54
               Mean episode length: 592.05
Episode_Reward/track_lin_vel_xy_exp: 0.3933
Episode_Reward/track_ang_vel_z_exp: 0.2183
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.0402
     Episode_Reward/dof_torques_l2: -0.0439
         Episode_Reward/dof_acc_l2: -0.1007
     Episode_Reward/action_rate_l2: -0.0432
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4441
Metrics/base_velocity/error_vel_xy: 0.3216
Metrics/base_velocity/error_vel_yaw: 0.2411
      Episode_Termination/time_out: 0.4046
  Episode_Termination/base_contact: 0.5954
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 2.52s
                      Time elapsed: 00:23:41
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 557/1400 [0m                      

                       Computation: 9751 steps/s (collection: 2.264s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 6.3336
                       Mean reward: 8.24
               Mean episode length: 621.57
Episode_Reward/track_lin_vel_xy_exp: 0.4586
Episode_Reward/track_ang_vel_z_exp: 0.2517
       Episode_Reward/lin_vel_z_l2: -0.0317
      Episode_Reward/ang_vel_xy_l2: -0.0401
     Episode_Reward/dof_torques_l2: -0.0468
         Episode_Reward/dof_acc_l2: -0.1053
     Episode_Reward/action_rate_l2: -0.0478
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4347
Metrics/base_velocity/error_vel_xy: 0.3300
Metrics/base_velocity/error_vel_yaw: 0.2467
      Episode_Termination/time_out: 0.4001
  Episode_Termination/base_contact: 0.5999
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 2.52s
                      Time elapsed: 00:23:43
                               ETA: 00:35:50

################################################################################
                     [1m Learning iteration 558/1400 [0m                      

                       Computation: 9777 steps/s (collection: 2.258s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.3236
                       Mean reward: 8.09
               Mean episode length: 636.10
Episode_Reward/track_lin_vel_xy_exp: 0.4616
Episode_Reward/track_ang_vel_z_exp: 0.2710
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.0505
     Episode_Reward/dof_torques_l2: -0.0556
         Episode_Reward/dof_acc_l2: -0.1267
     Episode_Reward/action_rate_l2: -0.0531
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4356
Metrics/base_velocity/error_vel_xy: 0.4290
Metrics/base_velocity/error_vel_yaw: 0.2808
      Episode_Termination/time_out: 0.3988
  Episode_Termination/base_contact: 0.6012
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 2.51s
                      Time elapsed: 00:23:46
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 559/1400 [0m                      

                       Computation: 9869 steps/s (collection: 2.234s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 6.3101
                       Mean reward: 8.83
               Mean episode length: 671.82
Episode_Reward/track_lin_vel_xy_exp: 0.6095
Episode_Reward/track_ang_vel_z_exp: 0.3281
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.0512
     Episode_Reward/dof_torques_l2: -0.0619
         Episode_Reward/dof_acc_l2: -0.1344
     Episode_Reward/action_rate_l2: -0.0625
      Episode_Reward/feet_air_time: -0.0099
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4470
Metrics/base_velocity/error_vel_xy: 0.4043
Metrics/base_velocity/error_vel_yaw: 0.3189
      Episode_Termination/time_out: 0.3999
  Episode_Termination/base_contact: 0.6001
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.49s
                      Time elapsed: 00:23:48
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 560/1400 [0m                      

                       Computation: 9640 steps/s (collection: 2.294s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.3098
                       Mean reward: 8.88
               Mean episode length: 686.02
Episode_Reward/track_lin_vel_xy_exp: 0.4620
Episode_Reward/track_ang_vel_z_exp: 0.2645
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0451
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.1188
     Episode_Reward/action_rate_l2: -0.0507
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4472
Metrics/base_velocity/error_vel_xy: 0.3742
Metrics/base_velocity/error_vel_yaw: 0.2592
      Episode_Termination/time_out: 0.3987
  Episode_Termination/base_contact: 0.6013
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 2.55s
                      Time elapsed: 00:23:51
                               ETA: 00:35:42

################################################################################
                     [1m Learning iteration 561/1400 [0m                      

                       Computation: 9318 steps/s (collection: 2.379s, learning 0.259s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 6.3119
                       Mean reward: 9.22
               Mean episode length: 701.20
Episode_Reward/track_lin_vel_xy_exp: 0.5370
Episode_Reward/track_ang_vel_z_exp: 0.2990
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0608
         Episode_Reward/dof_acc_l2: -0.1374
     Episode_Reward/action_rate_l2: -0.0584
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4562
Metrics/base_velocity/error_vel_xy: 0.4417
Metrics/base_velocity/error_vel_yaw: 0.3323
      Episode_Termination/time_out: 0.4021
  Episode_Termination/base_contact: 0.5979
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 2.64s
                      Time elapsed: 00:23:53
                               ETA: 00:35:40

################################################################################
                     [1m Learning iteration 562/1400 [0m                      

                       Computation: 9592 steps/s (collection: 2.307s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.3135
                       Mean reward: 8.59
               Mean episode length: 668.08
Episode_Reward/track_lin_vel_xy_exp: 0.4302
Episode_Reward/track_ang_vel_z_exp: 0.2371
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0400
     Episode_Reward/dof_torques_l2: -0.0464
         Episode_Reward/dof_acc_l2: -0.1083
     Episode_Reward/action_rate_l2: -0.0458
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4621
Metrics/base_velocity/error_vel_xy: 0.3278
Metrics/base_velocity/error_vel_yaw: 0.2470
      Episode_Termination/time_out: 0.4059
  Episode_Termination/base_contact: 0.5941
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 2.56s
                      Time elapsed: 00:23:56
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 563/1400 [0m                      

                       Computation: 9702 steps/s (collection: 2.268s, learning 0.265s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.3217
                       Mean reward: 8.06
               Mean episode length: 641.08
Episode_Reward/track_lin_vel_xy_exp: 0.4791
Episode_Reward/track_ang_vel_z_exp: 0.2776
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0530
         Episode_Reward/dof_acc_l2: -0.1205
     Episode_Reward/action_rate_l2: -0.0528
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4657
Metrics/base_velocity/error_vel_xy: 0.4325
Metrics/base_velocity/error_vel_yaw: 0.2934
      Episode_Termination/time_out: 0.4082
  Episode_Termination/base_contact: 0.5918
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.53s
                      Time elapsed: 00:23:58
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 564/1400 [0m                      

                       Computation: 9824 steps/s (collection: 2.239s, learning 0.263s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.3265
                       Mean reward: 7.73
               Mean episode length: 633.35
Episode_Reward/track_lin_vel_xy_exp: 0.4074
Episode_Reward/track_ang_vel_z_exp: 0.2331
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0388
     Episode_Reward/dof_torques_l2: -0.0458
         Episode_Reward/dof_acc_l2: -0.0987
     Episode_Reward/action_rate_l2: -0.0445
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0066
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4659
Metrics/base_velocity/error_vel_xy: 0.3480
Metrics/base_velocity/error_vel_yaw: 0.2378
      Episode_Termination/time_out: 0.4112
  Episode_Termination/base_contact: 0.5888
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 2.50s
                      Time elapsed: 00:24:01
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 565/1400 [0m                      

                       Computation: 9821 steps/s (collection: 2.243s, learning 0.259s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.3143
                       Mean reward: 8.13
               Mean episode length: 642.64
Episode_Reward/track_lin_vel_xy_exp: 0.4777
Episode_Reward/track_ang_vel_z_exp: 0.2601
       Episode_Reward/lin_vel_z_l2: -0.0355
      Episode_Reward/ang_vel_xy_l2: -0.0414
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.1198
     Episode_Reward/action_rate_l2: -0.0488
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4674
Metrics/base_velocity/error_vel_xy: 0.3251
Metrics/base_velocity/error_vel_yaw: 0.2497
      Episode_Termination/time_out: 0.4124
  Episode_Termination/base_contact: 0.5876
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 2.50s
                      Time elapsed: 00:24:03
                               ETA: 00:35:30

################################################################################
                     [1m Learning iteration 566/1400 [0m                      

                       Computation: 9761 steps/s (collection: 2.253s, learning 0.264s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.3034
                       Mean reward: 9.07
               Mean episode length: 671.64
Episode_Reward/track_lin_vel_xy_exp: 0.4857
Episode_Reward/track_ang_vel_z_exp: 0.2649
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0441
     Episode_Reward/dof_torques_l2: -0.0507
         Episode_Reward/dof_acc_l2: -0.1111
     Episode_Reward/action_rate_l2: -0.0500
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4688
Metrics/base_velocity/error_vel_xy: 0.3488
Metrics/base_velocity/error_vel_yaw: 0.2646
      Episode_Termination/time_out: 0.4165
  Episode_Termination/base_contact: 0.5835
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 2.52s
                      Time elapsed: 00:24:06
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 567/1400 [0m                      

                       Computation: 9750 steps/s (collection: 2.258s, learning 0.262s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.3046
                       Mean reward: 9.21
               Mean episode length: 674.34
Episode_Reward/track_lin_vel_xy_exp: 0.5172
Episode_Reward/track_ang_vel_z_exp: 0.2783
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0472
     Episode_Reward/dof_torques_l2: -0.0549
         Episode_Reward/dof_acc_l2: -0.1234
     Episode_Reward/action_rate_l2: -0.0533
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4759
Metrics/base_velocity/error_vel_xy: 0.3638
Metrics/base_velocity/error_vel_yaw: 0.2881
      Episode_Termination/time_out: 0.4165
  Episode_Termination/base_contact: 0.5835
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.52s
                      Time elapsed: 00:24:08
                               ETA: 00:35:24

################################################################################
                     [1m Learning iteration 568/1400 [0m                      

                       Computation: 9663 steps/s (collection: 2.288s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.2980
                       Mean reward: 8.95
               Mean episode length: 681.72
Episode_Reward/track_lin_vel_xy_exp: 0.4478
Episode_Reward/track_ang_vel_z_exp: 0.2586
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0441
     Episode_Reward/dof_torques_l2: -0.0505
         Episode_Reward/dof_acc_l2: -0.1219
     Episode_Reward/action_rate_l2: -0.0493
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4867
Metrics/base_velocity/error_vel_xy: 0.4004
Metrics/base_velocity/error_vel_yaw: 0.2703
      Episode_Termination/time_out: 0.4175
  Episode_Termination/base_contact: 0.5825
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 2.54s
                      Time elapsed: 00:24:11
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 569/1400 [0m                      

                       Computation: 9644 steps/s (collection: 2.291s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.2905
                       Mean reward: 7.99
               Mean episode length: 648.66
Episode_Reward/track_lin_vel_xy_exp: 0.4195
Episode_Reward/track_ang_vel_z_exp: 0.2420
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.1124
     Episode_Reward/action_rate_l2: -0.0468
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5002
Metrics/base_velocity/error_vel_xy: 0.3832
Metrics/base_velocity/error_vel_yaw: 0.2642
      Episode_Termination/time_out: 0.4210
  Episode_Termination/base_contact: 0.5790
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 2.55s
                      Time elapsed: 00:24:13
                               ETA: 00:35:19

################################################################################
                     [1m Learning iteration 570/1400 [0m                      

                       Computation: 9675 steps/s (collection: 2.287s, learning 0.253s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.2871
                       Mean reward: 7.80
               Mean episode length: 631.56
Episode_Reward/track_lin_vel_xy_exp: 0.4390
Episode_Reward/track_ang_vel_z_exp: 0.2493
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0434
     Episode_Reward/dof_torques_l2: -0.0498
         Episode_Reward/dof_acc_l2: -0.1144
     Episode_Reward/action_rate_l2: -0.0482
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5013
Metrics/base_velocity/error_vel_xy: 0.3837
Metrics/base_velocity/error_vel_yaw: 0.2804
      Episode_Termination/time_out: 0.4166
  Episode_Termination/base_contact: 0.5834
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 2.54s
                      Time elapsed: 00:24:16
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 571/1400 [0m                      

                       Computation: 9623 steps/s (collection: 2.299s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 6.2885
                       Mean reward: 8.35
               Mean episode length: 658.08
Episode_Reward/track_lin_vel_xy_exp: 0.5012
Episode_Reward/track_ang_vel_z_exp: 0.2853
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0472
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.1250
     Episode_Reward/action_rate_l2: -0.0541
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5057
Metrics/base_velocity/error_vel_xy: 0.4003
Metrics/base_velocity/error_vel_yaw: 0.2789
      Episode_Termination/time_out: 0.4211
  Episode_Termination/base_contact: 0.5789
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.55s
                      Time elapsed: 00:24:19
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 572/1400 [0m                      

                       Computation: 9595 steps/s (collection: 2.306s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.2828
                       Mean reward: 9.51
               Mean episode length: 703.10
Episode_Reward/track_lin_vel_xy_exp: 0.4971
Episode_Reward/track_ang_vel_z_exp: 0.2733
       Episode_Reward/lin_vel_z_l2: -0.0359
      Episode_Reward/ang_vel_xy_l2: -0.0444
     Episode_Reward/dof_torques_l2: -0.0521
         Episode_Reward/dof_acc_l2: -0.1108
     Episode_Reward/action_rate_l2: -0.0520
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5176
Metrics/base_velocity/error_vel_xy: 0.3732
Metrics/base_velocity/error_vel_yaw: 0.2744
      Episode_Termination/time_out: 0.4223
  Episode_Termination/base_contact: 0.5777
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 2.56s
                      Time elapsed: 00:24:21
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 573/1400 [0m                      

                       Computation: 9674 steps/s (collection: 2.278s, learning 0.262s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 6.2884
                       Mean reward: 8.66
               Mean episode length: 665.33
Episode_Reward/track_lin_vel_xy_exp: 0.4167
Episode_Reward/track_ang_vel_z_exp: 0.2430
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0422
     Episode_Reward/dof_torques_l2: -0.0502
         Episode_Reward/dof_acc_l2: -0.1095
     Episode_Reward/action_rate_l2: -0.0469
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5091
Metrics/base_velocity/error_vel_xy: 0.4021
Metrics/base_velocity/error_vel_yaw: 0.2806
      Episode_Termination/time_out: 0.4223
  Episode_Termination/base_contact: 0.5777
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 2.54s
                      Time elapsed: 00:24:24
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 574/1400 [0m                      

                       Computation: 9812 steps/s (collection: 2.247s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.2788
                       Mean reward: 8.60
               Mean episode length: 664.83
Episode_Reward/track_lin_vel_xy_exp: 0.5198
Episode_Reward/track_ang_vel_z_exp: 0.2932
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.0518
     Episode_Reward/dof_torques_l2: -0.0597
         Episode_Reward/dof_acc_l2: -0.1396
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5171
Metrics/base_velocity/error_vel_xy: 0.4607
Metrics/base_velocity/error_vel_yaw: 0.3476
      Episode_Termination/time_out: 0.4295
  Episode_Termination/base_contact: 0.5705
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 2.50s
                      Time elapsed: 00:24:26
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 575/1400 [0m                      

                       Computation: 9657 steps/s (collection: 2.284s, learning 0.261s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.2852
                       Mean reward: 8.69
               Mean episode length: 676.19
Episode_Reward/track_lin_vel_xy_exp: 0.4900
Episode_Reward/track_ang_vel_z_exp: 0.2741
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0447
     Episode_Reward/dof_torques_l2: -0.0525
         Episode_Reward/dof_acc_l2: -0.1160
     Episode_Reward/action_rate_l2: -0.0516
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5255
Metrics/base_velocity/error_vel_xy: 0.3757
Metrics/base_velocity/error_vel_yaw: 0.2749
      Episode_Termination/time_out: 0.4306
  Episode_Termination/base_contact: 0.5694
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.54s
                      Time elapsed: 00:24:29
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 576/1400 [0m                      

                       Computation: 9679 steps/s (collection: 2.281s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.2906
                       Mean reward: 9.17
               Mean episode length: 700.48
Episode_Reward/track_lin_vel_xy_exp: 0.4744
Episode_Reward/track_ang_vel_z_exp: 0.2671
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0438
     Episode_Reward/dof_torques_l2: -0.0504
         Episode_Reward/dof_acc_l2: -0.1140
     Episode_Reward/action_rate_l2: -0.0500
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5275
Metrics/base_velocity/error_vel_xy: 0.3768
Metrics/base_velocity/error_vel_yaw: 0.2676
      Episode_Termination/time_out: 0.4303
  Episode_Termination/base_contact: 0.5697
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 2.54s
                      Time elapsed: 00:24:31
                               ETA: 00:35:01

################################################################################
                     [1m Learning iteration 577/1400 [0m                      

                       Computation: 9765 steps/s (collection: 2.262s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.2806
                       Mean reward: 8.89
               Mean episode length: 677.02
Episode_Reward/track_lin_vel_xy_exp: 0.5361
Episode_Reward/track_ang_vel_z_exp: 0.2958
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.0515
     Episode_Reward/dof_torques_l2: -0.0596
         Episode_Reward/dof_acc_l2: -0.1409
     Episode_Reward/action_rate_l2: -0.0570
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5274
Metrics/base_velocity/error_vel_xy: 0.3994
Metrics/base_velocity/error_vel_yaw: 0.2946
      Episode_Termination/time_out: 0.4289
  Episode_Termination/base_contact: 0.5711
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 2.52s
                      Time elapsed: 00:24:34
                               ETA: 00:34:59

################################################################################
                     [1m Learning iteration 578/1400 [0m                      

                       Computation: 9740 steps/s (collection: 2.258s, learning 0.265s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.2704
                       Mean reward: 8.31
               Mean episode length: 659.74
Episode_Reward/track_lin_vel_xy_exp: 0.4136
Episode_Reward/track_ang_vel_z_exp: 0.2543
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0450
     Episode_Reward/dof_torques_l2: -0.0524
         Episode_Reward/dof_acc_l2: -0.1176
     Episode_Reward/action_rate_l2: -0.0491
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5299
Metrics/base_velocity/error_vel_xy: 0.4546
Metrics/base_velocity/error_vel_yaw: 0.2784
      Episode_Termination/time_out: 0.4276
  Episode_Termination/base_contact: 0.5724
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 2.52s
                      Time elapsed: 00:24:36
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 579/1400 [0m                      

                       Computation: 9667 steps/s (collection: 2.289s, learning 0.253s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 6.2637
                       Mean reward: 8.22
               Mean episode length: 669.85
Episode_Reward/track_lin_vel_xy_exp: 0.5190
Episode_Reward/track_ang_vel_z_exp: 0.2866
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0472
     Episode_Reward/dof_torques_l2: -0.0569
         Episode_Reward/dof_acc_l2: -0.1280
     Episode_Reward/action_rate_l2: -0.0547
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5367
Metrics/base_velocity/error_vel_xy: 0.4168
Metrics/base_velocity/error_vel_yaw: 0.3186
      Episode_Termination/time_out: 0.4295
  Episode_Termination/base_contact: 0.5705
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.54s
                      Time elapsed: 00:24:39
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 580/1400 [0m                      

                       Computation: 9702 steps/s (collection: 2.276s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.2615
                       Mean reward: 9.21
               Mean episode length: 732.04
Episode_Reward/track_lin_vel_xy_exp: 0.6304
Episode_Reward/track_ang_vel_z_exp: 0.3607
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.0559
     Episode_Reward/dof_torques_l2: -0.0702
         Episode_Reward/dof_acc_l2: -0.1521
     Episode_Reward/action_rate_l2: -0.0665
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0121
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5494
Metrics/base_velocity/error_vel_xy: 0.5174
Metrics/base_velocity/error_vel_yaw: 0.3429
      Episode_Termination/time_out: 0.4352
  Episode_Termination/base_contact: 0.5648
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 2.53s
                      Time elapsed: 00:24:41
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 581/1400 [0m                      

                       Computation: 9715 steps/s (collection: 2.273s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 6.2515
                       Mean reward: 9.24
               Mean episode length: 728.63
Episode_Reward/track_lin_vel_xy_exp: 0.3920
Episode_Reward/track_ang_vel_z_exp: 0.2291
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0412
     Episode_Reward/dof_torques_l2: -0.0475
         Episode_Reward/dof_acc_l2: -0.1112
     Episode_Reward/action_rate_l2: -0.0440
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5535
Metrics/base_velocity/error_vel_xy: 0.3669
Metrics/base_velocity/error_vel_yaw: 0.2437
      Episode_Termination/time_out: 0.4353
  Episode_Termination/base_contact: 0.5647
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 2.53s
                      Time elapsed: 00:24:44
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 582/1400 [0m                      

                       Computation: 9634 steps/s (collection: 2.292s, learning 0.259s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.2507
                       Mean reward: 9.26
               Mean episode length: 730.21
Episode_Reward/track_lin_vel_xy_exp: 0.5370
Episode_Reward/track_ang_vel_z_exp: 0.3015
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.0511
     Episode_Reward/dof_torques_l2: -0.0584
         Episode_Reward/dof_acc_l2: -0.1372
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5583
Metrics/base_velocity/error_vel_xy: 0.4273
Metrics/base_velocity/error_vel_yaw: 0.3151
      Episode_Termination/time_out: 0.4414
  Episode_Termination/base_contact: 0.5586
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 2.55s
                      Time elapsed: 00:24:46
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 583/1400 [0m                      

                       Computation: 9702 steps/s (collection: 2.276s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.2616
                       Mean reward: 8.81
               Mean episode length: 703.88
Episode_Reward/track_lin_vel_xy_exp: 0.4933
Episode_Reward/track_ang_vel_z_exp: 0.2753
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0531
         Episode_Reward/dof_acc_l2: -0.1316
     Episode_Reward/action_rate_l2: -0.0535
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5592
Metrics/base_velocity/error_vel_xy: 0.4261
Metrics/base_velocity/error_vel_yaw: 0.3081
      Episode_Termination/time_out: 0.4427
  Episode_Termination/base_contact: 0.5573
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.53s
                      Time elapsed: 00:24:49
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 584/1400 [0m                      

                       Computation: 9618 steps/s (collection: 2.291s, learning 0.264s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.2781
                       Mean reward: 9.29
               Mean episode length: 731.07
Episode_Reward/track_lin_vel_xy_exp: 0.4894
Episode_Reward/track_ang_vel_z_exp: 0.2743
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.0489
     Episode_Reward/dof_torques_l2: -0.0538
         Episode_Reward/dof_acc_l2: -0.1224
     Episode_Reward/action_rate_l2: -0.0517
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5685
Metrics/base_velocity/error_vel_xy: 0.4111
Metrics/base_velocity/error_vel_yaw: 0.3016
      Episode_Termination/time_out: 0.4482
  Episode_Termination/base_contact: 0.5518
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 2.56s
                      Time elapsed: 00:24:52
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 585/1400 [0m                      

                       Computation: 9666 steps/s (collection: 2.287s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 6.2583
                       Mean reward: 9.51
               Mean episode length: 741.03
Episode_Reward/track_lin_vel_xy_exp: 0.5709
Episode_Reward/track_ang_vel_z_exp: 0.3115
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0512
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.1366
     Episode_Reward/action_rate_l2: -0.0584
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5754
Metrics/base_velocity/error_vel_xy: 0.4209
Metrics/base_velocity/error_vel_yaw: 0.3232
      Episode_Termination/time_out: 0.4530
  Episode_Termination/base_contact: 0.5470
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 2.54s
                      Time elapsed: 00:24:54
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 586/1400 [0m                      

                       Computation: 9693 steps/s (collection: 2.281s, learning 0.254s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 6.2579
                       Mean reward: 8.47
               Mean episode length: 674.09
Episode_Reward/track_lin_vel_xy_exp: 0.4362
Episode_Reward/track_ang_vel_z_exp: 0.2528
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0511
         Episode_Reward/dof_acc_l2: -0.1090
     Episode_Reward/action_rate_l2: -0.0479
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5807
Metrics/base_velocity/error_vel_xy: 0.3861
Metrics/base_velocity/error_vel_yaw: 0.2653
      Episode_Termination/time_out: 0.4559
  Episode_Termination/base_contact: 0.5441
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 2.54s
                      Time elapsed: 00:24:57
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 587/1400 [0m                      

                       Computation: 9722 steps/s (collection: 2.273s, learning 0.254s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.2552
                       Mean reward: 8.27
               Mean episode length: 654.84
Episode_Reward/track_lin_vel_xy_exp: 0.4434
Episode_Reward/track_ang_vel_z_exp: 0.2576
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0441
     Episode_Reward/dof_torques_l2: -0.0523
         Episode_Reward/dof_acc_l2: -0.1147
     Episode_Reward/action_rate_l2: -0.0494
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5790
Metrics/base_velocity/error_vel_xy: 0.4118
Metrics/base_velocity/error_vel_yaw: 0.2785
      Episode_Termination/time_out: 0.4544
  Episode_Termination/base_contact: 0.5456
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.53s
                      Time elapsed: 00:24:59
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 588/1400 [0m                      

                       Computation: 9724 steps/s (collection: 2.271s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.2469
                       Mean reward: 8.41
               Mean episode length: 662.45
Episode_Reward/track_lin_vel_xy_exp: 0.4965
Episode_Reward/track_ang_vel_z_exp: 0.2850
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.0500
     Episode_Reward/dof_torques_l2: -0.0540
         Episode_Reward/dof_acc_l2: -0.1353
     Episode_Reward/action_rate_l2: -0.0538
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5830
Metrics/base_velocity/error_vel_xy: 0.4339
Metrics/base_velocity/error_vel_yaw: 0.3041
      Episode_Termination/time_out: 0.4495
  Episode_Termination/base_contact: 0.5505
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 2.53s
                      Time elapsed: 00:25:02
                               ETA: 00:34:30

################################################################################
                     [1m Learning iteration 589/1400 [0m                      

                       Computation: 9616 steps/s (collection: 2.300s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.2577
                       Mean reward: 8.34
               Mean episode length: 666.13
Episode_Reward/track_lin_vel_xy_exp: 0.5127
Episode_Reward/track_ang_vel_z_exp: 0.2913
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1265
     Episode_Reward/action_rate_l2: -0.0550
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5809
Metrics/base_velocity/error_vel_xy: 0.4380
Metrics/base_velocity/error_vel_yaw: 0.3078
      Episode_Termination/time_out: 0.4477
  Episode_Termination/base_contact: 0.5523
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 2.56s
                      Time elapsed: 00:25:04
                               ETA: 00:34:28

################################################################################
                     [1m Learning iteration 590/1400 [0m                      

                       Computation: 9661 steps/s (collection: 2.287s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.2611
                       Mean reward: 8.13
               Mean episode length: 652.83
Episode_Reward/track_lin_vel_xy_exp: 0.4533
Episode_Reward/track_ang_vel_z_exp: 0.2649
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0535
         Episode_Reward/dof_acc_l2: -0.1124
     Episode_Reward/action_rate_l2: -0.0511
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5747
Metrics/base_velocity/error_vel_xy: 0.4308
Metrics/base_velocity/error_vel_yaw: 0.2949
      Episode_Termination/time_out: 0.4441
  Episode_Termination/base_contact: 0.5559
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 2.54s
                      Time elapsed: 00:25:07
                               ETA: 00:34:25

################################################################################
                     [1m Learning iteration 591/1400 [0m                      

                       Computation: 9715 steps/s (collection: 2.274s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.2687
                       Mean reward: 8.84
               Mean episode length: 669.25
Episode_Reward/track_lin_vel_xy_exp: 0.5490
Episode_Reward/track_ang_vel_z_exp: 0.2960
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0526
         Episode_Reward/dof_acc_l2: -0.1105
     Episode_Reward/action_rate_l2: -0.0537
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5769
Metrics/base_velocity/error_vel_xy: 0.3644
Metrics/base_velocity/error_vel_yaw: 0.2860
      Episode_Termination/time_out: 0.4434
  Episode_Termination/base_contact: 0.5566
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.53s
                      Time elapsed: 00:25:09
                               ETA: 00:34:23

################################################################################
                     [1m Learning iteration 592/1400 [0m                      

                       Computation: 9625 steps/s (collection: 2.297s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.2874
                       Mean reward: 8.22
               Mean episode length: 639.85
Episode_Reward/track_lin_vel_xy_exp: 0.4168
Episode_Reward/track_ang_vel_z_exp: 0.2373
       Episode_Reward/lin_vel_z_l2: -0.0338
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.0484
         Episode_Reward/dof_acc_l2: -0.1029
     Episode_Reward/action_rate_l2: -0.0455
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0162
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5756
Metrics/base_velocity/error_vel_xy: 0.3843
Metrics/base_velocity/error_vel_yaw: 0.2838
      Episode_Termination/time_out: 0.4420
  Episode_Termination/base_contact: 0.5580
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 2.55s
                      Time elapsed: 00:25:12
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 593/1400 [0m                      

                       Computation: 9719 steps/s (collection: 2.272s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 6.2918
                       Mean reward: 8.11
               Mean episode length: 646.45
Episode_Reward/track_lin_vel_xy_exp: 0.4481
Episode_Reward/track_ang_vel_z_exp: 0.2649
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.0473
     Episode_Reward/dof_torques_l2: -0.0515
         Episode_Reward/dof_acc_l2: -0.1213
     Episode_Reward/action_rate_l2: -0.0507
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5689
Metrics/base_velocity/error_vel_xy: 0.4688
Metrics/base_velocity/error_vel_yaw: 0.3172
      Episode_Termination/time_out: 0.4434
  Episode_Termination/base_contact: 0.5566
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 2.53s
                      Time elapsed: 00:25:14
                               ETA: 00:34:18

################################################################################
                     [1m Learning iteration 594/1400 [0m                      

                       Computation: 9629 steps/s (collection: 2.296s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 6.2971
                       Mean reward: 7.79
               Mean episode length: 640.55
Episode_Reward/track_lin_vel_xy_exp: 0.5097
Episode_Reward/track_ang_vel_z_exp: 0.2854
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0522
         Episode_Reward/dof_acc_l2: -0.1208
     Episode_Reward/action_rate_l2: -0.0530
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5715
Metrics/base_velocity/error_vel_xy: 0.4001
Metrics/base_velocity/error_vel_yaw: 0.2866
      Episode_Termination/time_out: 0.4438
  Episode_Termination/base_contact: 0.5562
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 2.55s
                      Time elapsed: 00:25:17
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 595/1400 [0m                      

                       Computation: 9733 steps/s (collection: 2.263s, learning 0.262s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.2817
                       Mean reward: 7.52
               Mean episode length: 627.25
Episode_Reward/track_lin_vel_xy_exp: 0.4092
Episode_Reward/track_ang_vel_z_exp: 0.2287
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0409
     Episode_Reward/dof_torques_l2: -0.0492
         Episode_Reward/dof_acc_l2: -0.1200
     Episode_Reward/action_rate_l2: -0.0455
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5775
Metrics/base_velocity/error_vel_xy: 0.3648
Metrics/base_velocity/error_vel_yaw: 0.2801
      Episode_Termination/time_out: 0.4397
  Episode_Termination/base_contact: 0.5603
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.52s
                      Time elapsed: 00:25:19
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 596/1400 [0m                      

                       Computation: 9414 steps/s (collection: 2.352s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 6.2743
                       Mean reward: 7.58
               Mean episode length: 623.32
Episode_Reward/track_lin_vel_xy_exp: 0.4055
Episode_Reward/track_ang_vel_z_exp: 0.2321
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.0389
     Episode_Reward/dof_torques_l2: -0.0473
         Episode_Reward/dof_acc_l2: -0.1055
     Episode_Reward/action_rate_l2: -0.0449
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5724
Metrics/base_velocity/error_vel_xy: 0.3613
Metrics/base_velocity/error_vel_yaw: 0.2617
      Episode_Termination/time_out: 0.4387
  Episode_Termination/base_contact: 0.5613
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 2.61s
                      Time elapsed: 00:25:22
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 597/1400 [0m                      

                       Computation: 9590 steps/s (collection: 2.305s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.2810
                       Mean reward: 8.00
               Mean episode length: 646.88
Episode_Reward/track_lin_vel_xy_exp: 0.5311
Episode_Reward/track_ang_vel_z_exp: 0.2991
       Episode_Reward/lin_vel_z_l2: -0.0389
      Episode_Reward/ang_vel_xy_l2: -0.0482
     Episode_Reward/dof_torques_l2: -0.0561
         Episode_Reward/dof_acc_l2: -0.1251
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5649
Metrics/base_velocity/error_vel_xy: 0.4231
Metrics/base_velocity/error_vel_yaw: 0.3071
      Episode_Termination/time_out: 0.4382
  Episode_Termination/base_contact: 0.5618
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 2.56s
                      Time elapsed: 00:25:25
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 598/1400 [0m                      

                       Computation: 9779 steps/s (collection: 2.257s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.2720
                       Mean reward: 8.38
               Mean episode length: 659.88
Episode_Reward/track_lin_vel_xy_exp: 0.5258
Episode_Reward/track_ang_vel_z_exp: 0.2911
       Episode_Reward/lin_vel_z_l2: -0.0334
      Episode_Reward/ang_vel_xy_l2: -0.0443
     Episode_Reward/dof_torques_l2: -0.0574
         Episode_Reward/dof_acc_l2: -0.1136
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0254
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5660
Metrics/base_velocity/error_vel_xy: 0.4167
Metrics/base_velocity/error_vel_yaw: 0.3144
      Episode_Termination/time_out: 0.4409
  Episode_Termination/base_contact: 0.5591
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 2.51s
                      Time elapsed: 00:25:27
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 599/1400 [0m                      

                       Computation: 9652 steps/s (collection: 2.289s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 6.2783
                       Mean reward: 8.53
               Mean episode length: 663.83
Episode_Reward/track_lin_vel_xy_exp: 0.4680
Episode_Reward/track_ang_vel_z_exp: 0.2521
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0411
     Episode_Reward/dof_torques_l2: -0.0488
         Episode_Reward/dof_acc_l2: -0.1154
     Episode_Reward/action_rate_l2: -0.0490
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5719
Metrics/base_velocity/error_vel_xy: 0.3301
Metrics/base_velocity/error_vel_yaw: 0.2608
      Episode_Termination/time_out: 0.4440
  Episode_Termination/base_contact: 0.5560
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.55s
                      Time elapsed: 00:25:30
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 600/1400 [0m                      

                       Computation: 9803 steps/s (collection: 2.243s, learning 0.264s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 6.2816
                       Mean reward: 7.84
               Mean episode length: 622.40
Episode_Reward/track_lin_vel_xy_exp: 0.3946
Episode_Reward/track_ang_vel_z_exp: 0.2208
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0377
     Episode_Reward/dof_torques_l2: -0.0424
         Episode_Reward/dof_acc_l2: -0.0951
     Episode_Reward/action_rate_l2: -0.0419
      Episode_Reward/feet_air_time: -0.0065
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5820
Metrics/base_velocity/error_vel_xy: 0.3245
Metrics/base_velocity/error_vel_yaw: 0.2371
      Episode_Termination/time_out: 0.4398
  Episode_Termination/base_contact: 0.5602
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 2.51s
                      Time elapsed: 00:25:32
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 601/1400 [0m                      

                       Computation: 9823 steps/s (collection: 2.244s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.2844
                       Mean reward: 7.65
               Mean episode length: 603.20
Episode_Reward/track_lin_vel_xy_exp: 0.3910
Episode_Reward/track_ang_vel_z_exp: 0.2227
       Episode_Reward/lin_vel_z_l2: -0.0317
      Episode_Reward/ang_vel_xy_l2: -0.0396
     Episode_Reward/dof_torques_l2: -0.0452
         Episode_Reward/dof_acc_l2: -0.1011
     Episode_Reward/action_rate_l2: -0.0434
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5890
Metrics/base_velocity/error_vel_xy: 0.3764
Metrics/base_velocity/error_vel_yaw: 0.2771
      Episode_Termination/time_out: 0.4340
  Episode_Termination/base_contact: 0.5660
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 2.50s
                      Time elapsed: 00:25:35
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 602/1400 [0m                      

                       Computation: 9709 steps/s (collection: 2.274s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0176
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.2931
                       Mean reward: 7.98
               Mean episode length: 616.78
Episode_Reward/track_lin_vel_xy_exp: 0.4807
Episode_Reward/track_ang_vel_z_exp: 0.2652
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0427
     Episode_Reward/dof_torques_l2: -0.0505
         Episode_Reward/dof_acc_l2: -0.1067
     Episode_Reward/action_rate_l2: -0.0493
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5887
Metrics/base_velocity/error_vel_xy: 0.3621
Metrics/base_velocity/error_vel_yaw: 0.2719
      Episode_Termination/time_out: 0.4334
  Episode_Termination/base_contact: 0.5666
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 2.53s
                      Time elapsed: 00:25:37
                               ETA: 00:33:54

################################################################################
                     [1m Learning iteration 603/1400 [0m                      

                       Computation: 9693 steps/s (collection: 2.280s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 6.2876
                       Mean reward: 9.12
               Mean episode length: 684.27
Episode_Reward/track_lin_vel_xy_exp: 0.5214
Episode_Reward/track_ang_vel_z_exp: 0.2993
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0472
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1280
     Episode_Reward/action_rate_l2: -0.0552
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5783
Metrics/base_velocity/error_vel_xy: 0.4323
Metrics/base_velocity/error_vel_yaw: 0.2991
      Episode_Termination/time_out: 0.4314
  Episode_Termination/base_contact: 0.5686
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.54s
                      Time elapsed: 00:25:40
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 604/1400 [0m                      

                       Computation: 9583 steps/s (collection: 2.309s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.3016
                       Mean reward: 8.87
               Mean episode length: 698.92
Episode_Reward/track_lin_vel_xy_exp: 0.4902
Episode_Reward/track_ang_vel_z_exp: 0.2847
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.0487
     Episode_Reward/dof_torques_l2: -0.0580
         Episode_Reward/dof_acc_l2: -0.1292
     Episode_Reward/action_rate_l2: -0.0554
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5778
Metrics/base_velocity/error_vel_xy: 0.4687
Metrics/base_velocity/error_vel_yaw: 0.3282
      Episode_Termination/time_out: 0.4320
  Episode_Termination/base_contact: 0.5680
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 2.56s
                      Time elapsed: 00:25:42
                               ETA: 00:33:49

################################################################################
                     [1m Learning iteration 605/1400 [0m                      

                       Computation: 9643 steps/s (collection: 2.285s, learning 0.263s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.3000
                       Mean reward: 9.26
               Mean episode length: 721.60
Episode_Reward/track_lin_vel_xy_exp: 0.4968
Episode_Reward/track_ang_vel_z_exp: 0.2795
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0465
     Episode_Reward/dof_torques_l2: -0.0540
         Episode_Reward/dof_acc_l2: -0.1261
     Episode_Reward/action_rate_l2: -0.0528
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5850
Metrics/base_velocity/error_vel_xy: 0.4043
Metrics/base_velocity/error_vel_yaw: 0.2909
      Episode_Termination/time_out: 0.4399
  Episode_Termination/base_contact: 0.5601
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 2.55s
                      Time elapsed: 00:25:45
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 606/1400 [0m                      

                       Computation: 9597 steps/s (collection: 2.295s, learning 0.265s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.2954
                       Mean reward: 9.63
               Mean episode length: 742.20
Episode_Reward/track_lin_vel_xy_exp: 0.5297
Episode_Reward/track_ang_vel_z_exp: 0.2972
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0473
     Episode_Reward/dof_torques_l2: -0.0593
         Episode_Reward/dof_acc_l2: -0.1267
     Episode_Reward/action_rate_l2: -0.0562
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5852
Metrics/base_velocity/error_vel_xy: 0.4184
Metrics/base_velocity/error_vel_yaw: 0.3047
      Episode_Termination/time_out: 0.4441
  Episode_Termination/base_contact: 0.5559
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 2.56s
                      Time elapsed: 00:25:47
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 607/1400 [0m                      

                       Computation: 9553 steps/s (collection: 2.308s, learning 0.264s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 6.2904
                       Mean reward: 9.15
               Mean episode length: 707.48
Episode_Reward/track_lin_vel_xy_exp: 0.4200
Episode_Reward/track_ang_vel_z_exp: 0.2441
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0428
     Episode_Reward/dof_torques_l2: -0.0517
         Episode_Reward/dof_acc_l2: -0.1189
     Episode_Reward/action_rate_l2: -0.0479
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5872
Metrics/base_velocity/error_vel_xy: 0.4061
Metrics/base_velocity/error_vel_yaw: 0.2845
      Episode_Termination/time_out: 0.4425
  Episode_Termination/base_contact: 0.5575
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.57s
                      Time elapsed: 00:25:50
                               ETA: 00:33:42

################################################################################
                     [1m Learning iteration 608/1400 [0m                      

                       Computation: 9620 steps/s (collection: 2.299s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.2972
                       Mean reward: 8.27
               Mean episode length: 656.17
Episode_Reward/track_lin_vel_xy_exp: 0.4014
Episode_Reward/track_ang_vel_z_exp: 0.2330
       Episode_Reward/lin_vel_z_l2: -0.0318
      Episode_Reward/ang_vel_xy_l2: -0.0439
     Episode_Reward/dof_torques_l2: -0.0484
         Episode_Reward/dof_acc_l2: -0.1071
     Episode_Reward/action_rate_l2: -0.0452
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5888
Metrics/base_velocity/error_vel_xy: 0.3763
Metrics/base_velocity/error_vel_yaw: 0.2610
      Episode_Termination/time_out: 0.4442
  Episode_Termination/base_contact: 0.5558
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 2.55s
                      Time elapsed: 00:25:53
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 609/1400 [0m                      

                       Computation: 9384 steps/s (collection: 2.364s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.3004
                       Mean reward: 7.80
               Mean episode length: 608.08
Episode_Reward/track_lin_vel_xy_exp: 0.4115
Episode_Reward/track_ang_vel_z_exp: 0.2257
       Episode_Reward/lin_vel_z_l2: -0.0321
      Episode_Reward/ang_vel_xy_l2: -0.0372
     Episode_Reward/dof_torques_l2: -0.0437
         Episode_Reward/dof_acc_l2: -0.0977
     Episode_Reward/action_rate_l2: -0.0421
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5838
Metrics/base_velocity/error_vel_xy: 0.2891
Metrics/base_velocity/error_vel_yaw: 0.2146
      Episode_Termination/time_out: 0.4434
  Episode_Termination/base_contact: 0.5566
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 2.62s
                      Time elapsed: 00:25:55
                               ETA: 00:33:37

################################################################################
                     [1m Learning iteration 610/1400 [0m                      

                       Computation: 9720 steps/s (collection: 2.272s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.2911
                       Mean reward: 7.39
               Mean episode length: 587.91
Episode_Reward/track_lin_vel_xy_exp: 0.4090
Episode_Reward/track_ang_vel_z_exp: 0.2368
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0441
     Episode_Reward/dof_torques_l2: -0.0494
         Episode_Reward/dof_acc_l2: -0.1088
     Episode_Reward/action_rate_l2: -0.0476
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0226
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5835
Metrics/base_velocity/error_vel_xy: 0.4353
Metrics/base_velocity/error_vel_yaw: 0.3306
      Episode_Termination/time_out: 0.4426
  Episode_Termination/base_contact: 0.5574
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 2.53s
                      Time elapsed: 00:25:58
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 611/1400 [0m                      

                       Computation: 9775 steps/s (collection: 2.256s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.3069
                       Mean reward: 8.25
               Mean episode length: 632.23
Episode_Reward/track_lin_vel_xy_exp: 0.5122
Episode_Reward/track_ang_vel_z_exp: 0.2836
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0468
     Episode_Reward/dof_torques_l2: -0.0528
         Episode_Reward/dof_acc_l2: -0.1235
     Episode_Reward/action_rate_l2: -0.0523
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5921
Metrics/base_velocity/error_vel_xy: 0.3949
Metrics/base_velocity/error_vel_yaw: 0.2980
      Episode_Termination/time_out: 0.4501
  Episode_Termination/base_contact: 0.5499
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.51s
                      Time elapsed: 00:26:00
                               ETA: 00:33:32

################################################################################
                     [1m Learning iteration 612/1400 [0m                      

                       Computation: 9754 steps/s (collection: 2.262s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.3154
                       Mean reward: 8.72
               Mean episode length: 663.38
Episode_Reward/track_lin_vel_xy_exp: 0.3947
Episode_Reward/track_ang_vel_z_exp: 0.2245
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0384
     Episode_Reward/dof_torques_l2: -0.0458
         Episode_Reward/dof_acc_l2: -0.1024
     Episode_Reward/action_rate_l2: -0.0439
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6031
Metrics/base_velocity/error_vel_xy: 0.3615
Metrics/base_velocity/error_vel_yaw: 0.2558
      Episode_Termination/time_out: 0.4487
  Episode_Termination/base_contact: 0.5513
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 2.52s
                      Time elapsed: 00:26:03
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 613/1400 [0m                      

                       Computation: 9855 steps/s (collection: 2.233s, learning 0.261s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.3099
                       Mean reward: 9.28
               Mean episode length: 684.59
Episode_Reward/track_lin_vel_xy_exp: 0.5023
Episode_Reward/track_ang_vel_z_exp: 0.2806
       Episode_Reward/lin_vel_z_l2: -0.0359
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1167
     Episode_Reward/action_rate_l2: -0.0535
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6107
Metrics/base_velocity/error_vel_xy: 0.4139
Metrics/base_velocity/error_vel_yaw: 0.3093
      Episode_Termination/time_out: 0.4493
  Episode_Termination/base_contact: 0.5507
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 2.49s
                      Time elapsed: 00:26:05
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 614/1400 [0m                      

                       Computation: 9724 steps/s (collection: 2.270s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.3007
                       Mean reward: 9.74
               Mean episode length: 732.53
Episode_Reward/track_lin_vel_xy_exp: 0.5087
Episode_Reward/track_ang_vel_z_exp: 0.2876
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1257
     Episode_Reward/action_rate_l2: -0.0547
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6211
Metrics/base_velocity/error_vel_xy: 0.4315
Metrics/base_velocity/error_vel_yaw: 0.3111
      Episode_Termination/time_out: 0.4564
  Episode_Termination/base_contact: 0.5436
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 2.53s
                      Time elapsed: 00:26:08
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 615/1400 [0m                      

                       Computation: 9808 steps/s (collection: 2.250s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.2848
                       Mean reward: 10.01
               Mean episode length: 740.62
Episode_Reward/track_lin_vel_xy_exp: 0.5270
Episode_Reward/track_ang_vel_z_exp: 0.2968
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0449
     Episode_Reward/dof_torques_l2: -0.0530
         Episode_Reward/dof_acc_l2: -0.1165
     Episode_Reward/action_rate_l2: -0.0544
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6236
Metrics/base_velocity/error_vel_xy: 0.4114
Metrics/base_velocity/error_vel_yaw: 0.2913
      Episode_Termination/time_out: 0.4598
  Episode_Termination/base_contact: 0.5402
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.51s
                      Time elapsed: 00:26:10
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 616/1400 [0m                      

                       Computation: 9816 steps/s (collection: 2.245s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 6.2765
                       Mean reward: 8.95
               Mean episode length: 702.68
Episode_Reward/track_lin_vel_xy_exp: 0.4089
Episode_Reward/track_ang_vel_z_exp: 0.2305
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.0389
     Episode_Reward/dof_torques_l2: -0.0463
         Episode_Reward/dof_acc_l2: -0.1034
     Episode_Reward/action_rate_l2: -0.0445
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0075
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6252
Metrics/base_velocity/error_vel_xy: 0.3538
Metrics/base_velocity/error_vel_yaw: 0.2610
      Episode_Termination/time_out: 0.4613
  Episode_Termination/base_contact: 0.5387
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 2.50s
                      Time elapsed: 00:26:13
                               ETA: 00:33:19

################################################################################
                     [1m Learning iteration 617/1400 [0m                      

                       Computation: 9751 steps/s (collection: 2.265s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 6.2912
                       Mean reward: 8.93
               Mean episode length: 679.29
Episode_Reward/track_lin_vel_xy_exp: 0.5051
Episode_Reward/track_ang_vel_z_exp: 0.2789
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0451
     Episode_Reward/dof_torques_l2: -0.0503
         Episode_Reward/dof_acc_l2: -0.1214
     Episode_Reward/action_rate_l2: -0.0519
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6255
Metrics/base_velocity/error_vel_xy: 0.3792
Metrics/base_velocity/error_vel_yaw: 0.2886
      Episode_Termination/time_out: 0.4592
  Episode_Termination/base_contact: 0.5408
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 2.52s
                      Time elapsed: 00:26:15
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 618/1400 [0m                      

                       Computation: 9777 steps/s (collection: 2.258s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 6.2918
                       Mean reward: 9.00
               Mean episode length: 708.43
Episode_Reward/track_lin_vel_xy_exp: 0.4908
Episode_Reward/track_ang_vel_z_exp: 0.2908
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0505
     Episode_Reward/dof_torques_l2: -0.0572
         Episode_Reward/dof_acc_l2: -0.1273
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6310
Metrics/base_velocity/error_vel_xy: 0.4871
Metrics/base_velocity/error_vel_yaw: 0.3229
      Episode_Termination/time_out: 0.4596
  Episode_Termination/base_contact: 0.5404
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 2.51s
                      Time elapsed: 00:26:18
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 619/1400 [0m                      

                       Computation: 9726 steps/s (collection: 2.269s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 6.2849
                       Mean reward: 9.01
               Mean episode length: 694.10
Episode_Reward/track_lin_vel_xy_exp: 0.4369
Episode_Reward/track_ang_vel_z_exp: 0.2488
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0413
     Episode_Reward/dof_torques_l2: -0.0462
         Episode_Reward/dof_acc_l2: -0.1023
     Episode_Reward/action_rate_l2: -0.0461
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6342
Metrics/base_velocity/error_vel_xy: 0.3692
Metrics/base_velocity/error_vel_yaw: 0.2583
      Episode_Termination/time_out: 0.4595
  Episode_Termination/base_contact: 0.5405
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.53s
                      Time elapsed: 00:26:20
                               ETA: 00:33:11

################################################################################
                     [1m Learning iteration 620/1400 [0m                      

                       Computation: 9750 steps/s (collection: 2.264s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.2722
                       Mean reward: 8.93
               Mean episode length: 690.24
Episode_Reward/track_lin_vel_xy_exp: 0.4694
Episode_Reward/track_ang_vel_z_exp: 0.2682
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.0502
         Episode_Reward/dof_acc_l2: -0.1147
     Episode_Reward/action_rate_l2: -0.0509
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6348
Metrics/base_velocity/error_vel_xy: 0.4097
Metrics/base_velocity/error_vel_yaw: 0.2862
      Episode_Termination/time_out: 0.4554
  Episode_Termination/base_contact: 0.5446
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 2.52s
                      Time elapsed: 00:26:23
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 621/1400 [0m                      

                       Computation: 9794 steps/s (collection: 2.246s, learning 0.264s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 6.2636
                       Mean reward: 8.64
               Mean episode length: 674.07
Episode_Reward/track_lin_vel_xy_exp: 0.5506
Episode_Reward/track_ang_vel_z_exp: 0.3124
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0520
     Episode_Reward/dof_torques_l2: -0.0585
         Episode_Reward/dof_acc_l2: -0.1330
     Episode_Reward/action_rate_l2: -0.0584
      Episode_Reward/feet_air_time: -0.0099
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6348
Metrics/base_velocity/error_vel_xy: 0.4856
Metrics/base_velocity/error_vel_yaw: 0.3491
      Episode_Termination/time_out: 0.4543
  Episode_Termination/base_contact: 0.5457
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 2.51s
                      Time elapsed: 00:26:25
                               ETA: 00:33:06

################################################################################
                     [1m Learning iteration 622/1400 [0m                      

                       Computation: 9748 steps/s (collection: 2.265s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.2545
                       Mean reward: 9.15
               Mean episode length: 713.21
Episode_Reward/track_lin_vel_xy_exp: 0.4967
Episode_Reward/track_ang_vel_z_exp: 0.2842
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1201
     Episode_Reward/action_rate_l2: -0.0544
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6324
Metrics/base_velocity/error_vel_xy: 0.4355
Metrics/base_velocity/error_vel_yaw: 0.3153
      Episode_Termination/time_out: 0.4542
  Episode_Termination/base_contact: 0.5458
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 2.52s
                      Time elapsed: 00:26:28
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 623/1400 [0m                      

                       Computation: 9631 steps/s (collection: 2.297s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.2498
                       Mean reward: 8.93
               Mean episode length: 695.86
Episode_Reward/track_lin_vel_xy_exp: 0.4925
Episode_Reward/track_ang_vel_z_exp: 0.2705
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0447
     Episode_Reward/dof_torques_l2: -0.0524
         Episode_Reward/dof_acc_l2: -0.1159
     Episode_Reward/action_rate_l2: -0.0519
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6298
Metrics/base_velocity/error_vel_xy: 0.3885
Metrics/base_velocity/error_vel_yaw: 0.3032
      Episode_Termination/time_out: 0.4514
  Episode_Termination/base_contact: 0.5486
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.55s
                      Time elapsed: 00:26:30
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 624/1400 [0m                      

                       Computation: 9765 steps/s (collection: 2.261s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.2368
                       Mean reward: 8.17
               Mean episode length: 644.11
Episode_Reward/track_lin_vel_xy_exp: 0.3014
Episode_Reward/track_ang_vel_z_exp: 0.1810
       Episode_Reward/lin_vel_z_l2: -0.0295
      Episode_Reward/ang_vel_xy_l2: -0.0329
     Episode_Reward/dof_torques_l2: -0.0387
         Episode_Reward/dof_acc_l2: -0.0867
     Episode_Reward/action_rate_l2: -0.0370
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6244
Metrics/base_velocity/error_vel_xy: 0.3580
Metrics/base_velocity/error_vel_yaw: 0.2358
      Episode_Termination/time_out: 0.4494
  Episode_Termination/base_contact: 0.5506
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 2.52s
                      Time elapsed: 00:26:33
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 625/1400 [0m                      

                       Computation: 9764 steps/s (collection: 2.263s, learning 0.254s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 6.2383
                       Mean reward: 7.71
               Mean episode length: 644.25
Episode_Reward/track_lin_vel_xy_exp: 0.4422
Episode_Reward/track_ang_vel_z_exp: 0.2552
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0454
     Episode_Reward/dof_torques_l2: -0.0504
         Episode_Reward/dof_acc_l2: -0.1186
     Episode_Reward/action_rate_l2: -0.0489
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6318
Metrics/base_velocity/error_vel_xy: 0.4022
Metrics/base_velocity/error_vel_yaw: 0.2740
      Episode_Termination/time_out: 0.4450
  Episode_Termination/base_contact: 0.5550
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 2.52s
                      Time elapsed: 00:26:35
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 626/1400 [0m                      

                       Computation: 9844 steps/s (collection: 2.240s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 6.2393
                       Mean reward: 7.91
               Mean episode length: 676.47
Episode_Reward/track_lin_vel_xy_exp: 0.5301
Episode_Reward/track_ang_vel_z_exp: 0.3076
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.0535
     Episode_Reward/dof_torques_l2: -0.0629
         Episode_Reward/dof_acc_l2: -0.1516
     Episode_Reward/action_rate_l2: -0.0597
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6294
Metrics/base_velocity/error_vel_xy: 0.4890
Metrics/base_velocity/error_vel_yaw: 0.3397
      Episode_Termination/time_out: 0.4457
  Episode_Termination/base_contact: 0.5543
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 2.50s
                      Time elapsed: 00:26:38
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 627/1400 [0m                      

                       Computation: 9707 steps/s (collection: 2.280s, learning 0.251s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.2337
                       Mean reward: 7.61
               Mean episode length: 651.13
Episode_Reward/track_lin_vel_xy_exp: 0.3627
Episode_Reward/track_ang_vel_z_exp: 0.2151
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.0361
     Episode_Reward/dof_torques_l2: -0.0443
         Episode_Reward/dof_acc_l2: -0.0971
     Episode_Reward/action_rate_l2: -0.0408
      Episode_Reward/feet_air_time: -0.0065
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6259
Metrics/base_velocity/error_vel_xy: 0.3474
Metrics/base_velocity/error_vel_yaw: 0.2241
      Episode_Termination/time_out: 0.4364
  Episode_Termination/base_contact: 0.5636
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.53s
                      Time elapsed: 00:26:40
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 628/1400 [0m                      

                       Computation: 9808 steps/s (collection: 2.246s, learning 0.259s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.2267
                       Mean reward: 8.14
               Mean episode length: 650.50
Episode_Reward/track_lin_vel_xy_exp: 0.5166
Episode_Reward/track_ang_vel_z_exp: 0.2918
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0451
     Episode_Reward/dof_torques_l2: -0.0531
         Episode_Reward/dof_acc_l2: -0.1166
     Episode_Reward/action_rate_l2: -0.0541
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6302
Metrics/base_velocity/error_vel_xy: 0.4149
Metrics/base_velocity/error_vel_yaw: 0.2820
      Episode_Termination/time_out: 0.4333
  Episode_Termination/base_contact: 0.5667
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 2.51s
                      Time elapsed: 00:26:43
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 629/1400 [0m                      

                       Computation: 9774 steps/s (collection: 2.259s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.2267
                       Mean reward: 8.79
               Mean episode length: 685.29
Episode_Reward/track_lin_vel_xy_exp: 0.4911
Episode_Reward/track_ang_vel_z_exp: 0.2851
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1263
     Episode_Reward/action_rate_l2: -0.0547
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6359
Metrics/base_velocity/error_vel_xy: 0.4579
Metrics/base_velocity/error_vel_yaw: 0.3197
      Episode_Termination/time_out: 0.4356
  Episode_Termination/base_contact: 0.5644
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 2.51s
                      Time elapsed: 00:26:45
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 630/1400 [0m                      

                       Computation: 9817 steps/s (collection: 2.248s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 6.2178
                       Mean reward: 9.59
               Mean episode length: 724.76
Episode_Reward/track_lin_vel_xy_exp: 0.5700
Episode_Reward/track_ang_vel_z_exp: 0.3156
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0585
         Episode_Reward/dof_acc_l2: -0.1263
     Episode_Reward/action_rate_l2: -0.0598
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6402
Metrics/base_velocity/error_vel_xy: 0.4302
Metrics/base_velocity/error_vel_yaw: 0.3222
      Episode_Termination/time_out: 0.4339
  Episode_Termination/base_contact: 0.5661
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 2.50s
                      Time elapsed: 00:26:48
                               ETA: 00:32:42

################################################################################
                     [1m Learning iteration 631/1400 [0m                      

                       Computation: 9659 steps/s (collection: 2.282s, learning 0.262s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.2266
                       Mean reward: 9.02
               Mean episode length: 699.46
Episode_Reward/track_lin_vel_xy_exp: 0.4733
Episode_Reward/track_ang_vel_z_exp: 0.2696
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0435
     Episode_Reward/dof_torques_l2: -0.0531
         Episode_Reward/dof_acc_l2: -0.1167
     Episode_Reward/action_rate_l2: -0.0526
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6394
Metrics/base_velocity/error_vel_xy: 0.4312
Metrics/base_velocity/error_vel_yaw: 0.3192
      Episode_Termination/time_out: 0.4365
  Episode_Termination/base_contact: 0.5635
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.54s
                      Time elapsed: 00:26:51
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 632/1400 [0m                      

                       Computation: 9836 steps/s (collection: 2.243s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.2323
                       Mean reward: 9.54
               Mean episode length: 718.43
Episode_Reward/track_lin_vel_xy_exp: 0.4905
Episode_Reward/track_ang_vel_z_exp: 0.2772
       Episode_Reward/lin_vel_z_l2: -0.0378
      Episode_Reward/ang_vel_xy_l2: -0.0452
     Episode_Reward/dof_torques_l2: -0.0555
         Episode_Reward/dof_acc_l2: -0.1186
     Episode_Reward/action_rate_l2: -0.0539
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0073
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6440
Metrics/base_velocity/error_vel_xy: 0.4077
Metrics/base_velocity/error_vel_yaw: 0.3009
      Episode_Termination/time_out: 0.4395
  Episode_Termination/base_contact: 0.5605
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 2.50s
                      Time elapsed: 00:26:53
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 633/1400 [0m                      

                       Computation: 9804 steps/s (collection: 2.249s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.2335
                       Mean reward: 9.01
               Mean episode length: 700.20
Episode_Reward/track_lin_vel_xy_exp: 0.5283
Episode_Reward/track_ang_vel_z_exp: 0.3037
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0590
         Episode_Reward/dof_acc_l2: -0.1225
     Episode_Reward/action_rate_l2: -0.0563
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6414
Metrics/base_velocity/error_vel_xy: 0.4264
Metrics/base_velocity/error_vel_yaw: 0.2913
      Episode_Termination/time_out: 0.4400
  Episode_Termination/base_contact: 0.5600
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 2.51s
                      Time elapsed: 00:26:56
                               ETA: 00:32:35

################################################################################
                     [1m Learning iteration 634/1400 [0m                      

                       Computation: 9789 steps/s (collection: 2.254s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.2387
                       Mean reward: 8.35
               Mean episode length: 670.65
Episode_Reward/track_lin_vel_xy_exp: 0.3883
Episode_Reward/track_ang_vel_z_exp: 0.2286
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0390
     Episode_Reward/dof_torques_l2: -0.0468
         Episode_Reward/dof_acc_l2: -0.1073
     Episode_Reward/action_rate_l2: -0.0454
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6463
Metrics/base_velocity/error_vel_xy: 0.3951
Metrics/base_velocity/error_vel_yaw: 0.2695
      Episode_Termination/time_out: 0.4420
  Episode_Termination/base_contact: 0.5580
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 2.51s
                      Time elapsed: 00:26:58
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 635/1400 [0m                      

                       Computation: 9715 steps/s (collection: 2.274s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.2367
                       Mean reward: 8.15
               Mean episode length: 656.38
Episode_Reward/track_lin_vel_xy_exp: 0.4209
Episode_Reward/track_ang_vel_z_exp: 0.2485
       Episode_Reward/lin_vel_z_l2: -0.0308
      Episode_Reward/ang_vel_xy_l2: -0.0393
     Episode_Reward/dof_torques_l2: -0.0497
         Episode_Reward/dof_acc_l2: -0.1093
     Episode_Reward/action_rate_l2: -0.0482
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6467
Metrics/base_velocity/error_vel_xy: 0.4288
Metrics/base_velocity/error_vel_yaw: 0.2866
      Episode_Termination/time_out: 0.4419
  Episode_Termination/base_contact: 0.5581
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.53s
                      Time elapsed: 00:27:01
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 636/1400 [0m                      

                       Computation: 9690 steps/s (collection: 2.281s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 6.2361
                       Mean reward: 7.86
               Mean episode length: 650.13
Episode_Reward/track_lin_vel_xy_exp: 0.5009
Episode_Reward/track_ang_vel_z_exp: 0.2990
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0592
         Episode_Reward/dof_acc_l2: -0.1433
     Episode_Reward/action_rate_l2: -0.0560
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0105
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6480
Metrics/base_velocity/error_vel_xy: 0.4611
Metrics/base_velocity/error_vel_yaw: 0.2950
      Episode_Termination/time_out: 0.4459
  Episode_Termination/base_contact: 0.5541
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 2.54s
                      Time elapsed: 00:27:03
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 637/1400 [0m                      

                       Computation: 9679 steps/s (collection: 2.287s, learning 0.252s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.2387
                       Mean reward: 8.39
               Mean episode length: 660.76
Episode_Reward/track_lin_vel_xy_exp: 0.4591
Episode_Reward/track_ang_vel_z_exp: 0.2609
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0413
     Episode_Reward/dof_torques_l2: -0.0530
         Episode_Reward/dof_acc_l2: -0.1179
     Episode_Reward/action_rate_l2: -0.0505
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6518
Metrics/base_velocity/error_vel_xy: 0.4054
Metrics/base_velocity/error_vel_yaw: 0.2834
      Episode_Termination/time_out: 0.4467
  Episode_Termination/base_contact: 0.5533
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 2.54s
                      Time elapsed: 00:27:06
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 638/1400 [0m                      

                       Computation: 9819 steps/s (collection: 2.250s, learning 0.252s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.2381
                       Mean reward: 7.46
               Mean episode length: 605.86
Episode_Reward/track_lin_vel_xy_exp: 0.4246
Episode_Reward/track_ang_vel_z_exp: 0.2448
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0408
     Episode_Reward/dof_torques_l2: -0.0477
         Episode_Reward/dof_acc_l2: -0.1040
     Episode_Reward/action_rate_l2: -0.0463
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6499
Metrics/base_velocity/error_vel_xy: 0.3658
Metrics/base_velocity/error_vel_yaw: 0.2494
      Episode_Termination/time_out: 0.4426
  Episode_Termination/base_contact: 0.5574
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 2.50s
                      Time elapsed: 00:27:08
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 639/1400 [0m                      

                       Computation: 9729 steps/s (collection: 2.270s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 6.2312
                       Mean reward: 8.20
               Mean episode length: 645.71
Episode_Reward/track_lin_vel_xy_exp: 0.4616
Episode_Reward/track_ang_vel_z_exp: 0.2691
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0457
     Episode_Reward/dof_torques_l2: -0.0520
         Episode_Reward/dof_acc_l2: -0.1145
     Episode_Reward/action_rate_l2: -0.0515
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6508
Metrics/base_velocity/error_vel_xy: 0.4286
Metrics/base_velocity/error_vel_yaw: 0.2859
      Episode_Termination/time_out: 0.4430
  Episode_Termination/base_contact: 0.5570
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.53s
                      Time elapsed: 00:27:11
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 640/1400 [0m                      

                       Computation: 9766 steps/s (collection: 2.254s, learning 0.262s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 6.2220
                       Mean reward: 9.22
               Mean episode length: 692.66
Episode_Reward/track_lin_vel_xy_exp: 0.4898
Episode_Reward/track_ang_vel_z_exp: 0.2716
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0435
     Episode_Reward/dof_torques_l2: -0.0515
         Episode_Reward/dof_acc_l2: -0.1153
     Episode_Reward/action_rate_l2: -0.0510
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6597
Metrics/base_velocity/error_vel_xy: 0.3721
Metrics/base_velocity/error_vel_yaw: 0.2749
      Episode_Termination/time_out: 0.4415
  Episode_Termination/base_contact: 0.5585
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 2.52s
                      Time elapsed: 00:27:13
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 641/1400 [0m                      

                       Computation: 9849 steps/s (collection: 2.231s, learning 0.264s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.2244
                       Mean reward: 8.53
               Mean episode length: 659.07
Episode_Reward/track_lin_vel_xy_exp: 0.4875
Episode_Reward/track_ang_vel_z_exp: 0.2787
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1317
     Episode_Reward/action_rate_l2: -0.0527
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6618
Metrics/base_velocity/error_vel_xy: 0.4210
Metrics/base_velocity/error_vel_yaw: 0.2931
      Episode_Termination/time_out: 0.4447
  Episode_Termination/base_contact: 0.5553
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 2.50s
                      Time elapsed: 00:27:16
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 642/1400 [0m                      

                       Computation: 9806 steps/s (collection: 2.250s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0127
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.2312
                       Mean reward: 7.95
               Mean episode length: 631.14
Episode_Reward/track_lin_vel_xy_exp: 0.3655
Episode_Reward/track_ang_vel_z_exp: 0.2198
       Episode_Reward/lin_vel_z_l2: -0.0314
      Episode_Reward/ang_vel_xy_l2: -0.0379
     Episode_Reward/dof_torques_l2: -0.0456
         Episode_Reward/dof_acc_l2: -0.1049
     Episode_Reward/action_rate_l2: -0.0430
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6614
Metrics/base_velocity/error_vel_xy: 0.3797
Metrics/base_velocity/error_vel_yaw: 0.2578
      Episode_Termination/time_out: 0.4468
  Episode_Termination/base_contact: 0.5532
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 2.51s
                      Time elapsed: 00:27:18
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 643/1400 [0m                      

                       Computation: 9779 steps/s (collection: 2.258s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.2261
                       Mean reward: 8.34
               Mean episode length: 646.76
Episode_Reward/track_lin_vel_xy_exp: 0.5171
Episode_Reward/track_ang_vel_z_exp: 0.2886
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0426
     Episode_Reward/dof_torques_l2: -0.0550
         Episode_Reward/dof_acc_l2: -0.1150
     Episode_Reward/action_rate_l2: -0.0540
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6643
Metrics/base_velocity/error_vel_xy: 0.4044
Metrics/base_velocity/error_vel_yaw: 0.2902
      Episode_Termination/time_out: 0.4460
  Episode_Termination/base_contact: 0.5540
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.51s
                      Time elapsed: 00:27:21
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 644/1400 [0m                      

                       Computation: 9836 steps/s (collection: 2.243s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 6.2125
                       Mean reward: 8.50
               Mean episode length: 652.90
Episode_Reward/track_lin_vel_xy_exp: 0.4734
Episode_Reward/track_ang_vel_z_exp: 0.2729
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0518
         Episode_Reward/dof_acc_l2: -0.1166
     Episode_Reward/action_rate_l2: -0.0514
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6694
Metrics/base_velocity/error_vel_xy: 0.4216
Metrics/base_velocity/error_vel_yaw: 0.2983
      Episode_Termination/time_out: 0.4469
  Episode_Termination/base_contact: 0.5531
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 2.50s
                      Time elapsed: 00:27:23
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 645/1400 [0m                      

                       Computation: 9768 steps/s (collection: 2.253s, learning 0.262s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.2054
                       Mean reward: 9.82
               Mean episode length: 720.11
Episode_Reward/track_lin_vel_xy_exp: 0.6543
Episode_Reward/track_ang_vel_z_exp: 0.3543
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0529
     Episode_Reward/dof_torques_l2: -0.0660
         Episode_Reward/dof_acc_l2: -0.1397
     Episode_Reward/action_rate_l2: -0.0665
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6770
Metrics/base_velocity/error_vel_xy: 0.4234
Metrics/base_velocity/error_vel_yaw: 0.3216
      Episode_Termination/time_out: 0.4515
  Episode_Termination/base_contact: 0.5485
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 2.52s
                      Time elapsed: 00:27:26
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 646/1400 [0m                      

                       Computation: 9665 steps/s (collection: 2.290s, learning 0.253s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 6.2046
                       Mean reward: 10.24
               Mean episode length: 753.34
Episode_Reward/track_lin_vel_xy_exp: 0.5453
Episode_Reward/track_ang_vel_z_exp: 0.3033
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0561
         Episode_Reward/dof_acc_l2: -0.1308
     Episode_Reward/action_rate_l2: -0.0577
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6932
Metrics/base_velocity/error_vel_xy: 0.4258
Metrics/base_velocity/error_vel_yaw: 0.3169
      Episode_Termination/time_out: 0.4513
  Episode_Termination/base_contact: 0.5487
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 2.54s
                      Time elapsed: 00:27:28
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 647/1400 [0m                      

                       Computation: 9806 steps/s (collection: 2.250s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.2122
                       Mean reward: 10.04
               Mean episode length: 734.22
Episode_Reward/track_lin_vel_xy_exp: 0.5418
Episode_Reward/track_ang_vel_z_exp: 0.2954
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0530
         Episode_Reward/dof_acc_l2: -0.1128
     Episode_Reward/action_rate_l2: -0.0550
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7106
Metrics/base_velocity/error_vel_xy: 0.3713
Metrics/base_velocity/error_vel_yaw: 0.2812
      Episode_Termination/time_out: 0.4547
  Episode_Termination/base_contact: 0.5453
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.51s
                      Time elapsed: 00:27:31
                               ETA: 00:31:58

################################################################################
                     [1m Learning iteration 648/1400 [0m                      

                       Computation: 9891 steps/s (collection: 2.229s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 6.2128
                       Mean reward: 9.25
               Mean episode length: 713.41
Episode_Reward/track_lin_vel_xy_exp: 0.4453
Episode_Reward/track_ang_vel_z_exp: 0.2642
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0538
         Episode_Reward/dof_acc_l2: -0.1191
     Episode_Reward/action_rate_l2: -0.0523
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6996
Metrics/base_velocity/error_vel_xy: 0.4600
Metrics/base_velocity/error_vel_yaw: 0.2994
      Episode_Termination/time_out: 0.4554
  Episode_Termination/base_contact: 0.5446
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 2.48s
                      Time elapsed: 00:27:33
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 649/1400 [0m                      

                       Computation: 9831 steps/s (collection: 2.243s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.2092
                       Mean reward: 9.26
               Mean episode length: 698.59
Episode_Reward/track_lin_vel_xy_exp: 0.5013
Episode_Reward/track_ang_vel_z_exp: 0.2784
       Episode_Reward/lin_vel_z_l2: -0.0331
      Episode_Reward/ang_vel_xy_l2: -0.0429
     Episode_Reward/dof_torques_l2: -0.0500
         Episode_Reward/dof_acc_l2: -0.1110
     Episode_Reward/action_rate_l2: -0.0520
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6990
Metrics/base_velocity/error_vel_xy: 0.3949
Metrics/base_velocity/error_vel_yaw: 0.2805
      Episode_Termination/time_out: 0.4546
  Episode_Termination/base_contact: 0.5454
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 2.50s
                      Time elapsed: 00:27:36
                               ETA: 00:31:53

################################################################################
                     [1m Learning iteration 650/1400 [0m                      

                       Computation: 9750 steps/s (collection: 2.268s, learning 0.253s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.1952
                       Mean reward: 9.07
               Mean episode length: 693.59
Episode_Reward/track_lin_vel_xy_exp: 0.5430
Episode_Reward/track_ang_vel_z_exp: 0.2982
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0539
         Episode_Reward/dof_acc_l2: -0.1263
     Episode_Reward/action_rate_l2: -0.0557
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7013
Metrics/base_velocity/error_vel_xy: 0.3803
Metrics/base_velocity/error_vel_yaw: 0.2820
      Episode_Termination/time_out: 0.4530
  Episode_Termination/base_contact: 0.5470
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 2.52s
                      Time elapsed: 00:27:38
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 651/1400 [0m                      

                       Computation: 9868 steps/s (collection: 2.235s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.1873
                       Mean reward: 9.55
               Mean episode length: 686.89
Episode_Reward/track_lin_vel_xy_exp: 0.5173
Episode_Reward/track_ang_vel_z_exp: 0.2836
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0507
         Episode_Reward/dof_acc_l2: -0.1083
     Episode_Reward/action_rate_l2: -0.0524
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7045
Metrics/base_velocity/error_vel_xy: 0.3497
Metrics/base_velocity/error_vel_yaw: 0.2602
      Episode_Termination/time_out: 0.4584
  Episode_Termination/base_contact: 0.5416
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.49s
                      Time elapsed: 00:27:41
                               ETA: 00:31:48

################################################################################
                     [1m Learning iteration 652/1400 [0m                      

                       Computation: 9788 steps/s (collection: 2.257s, learning 0.253s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.1855
                       Mean reward: 9.05
               Mean episode length: 674.15
Episode_Reward/track_lin_vel_xy_exp: 0.4361
Episode_Reward/track_ang_vel_z_exp: 0.2586
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0452
     Episode_Reward/dof_torques_l2: -0.0551
         Episode_Reward/dof_acc_l2: -0.1248
     Episode_Reward/action_rate_l2: -0.0508
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7038
Metrics/base_velocity/error_vel_xy: 0.4428
Metrics/base_velocity/error_vel_yaw: 0.2937
      Episode_Termination/time_out: 0.4641
  Episode_Termination/base_contact: 0.5359
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 2.51s
                      Time elapsed: 00:27:43
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 653/1400 [0m                      

                       Computation: 9789 steps/s (collection: 2.255s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 6.1742
                       Mean reward: 8.32
               Mean episode length: 650.22
Episode_Reward/track_lin_vel_xy_exp: 0.4699
Episode_Reward/track_ang_vel_z_exp: 0.2627
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0429
     Episode_Reward/dof_torques_l2: -0.0521
         Episode_Reward/dof_acc_l2: -0.1186
     Episode_Reward/action_rate_l2: -0.0509
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0104
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6994
Metrics/base_velocity/error_vel_xy: 0.3797
Metrics/base_velocity/error_vel_yaw: 0.2833
      Episode_Termination/time_out: 0.4651
  Episode_Termination/base_contact: 0.5349
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 2.51s
                      Time elapsed: 00:27:46
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 654/1400 [0m                      

                       Computation: 9912 steps/s (collection: 2.222s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.1676
                       Mean reward: 8.09
               Mean episode length: 657.87
Episode_Reward/track_lin_vel_xy_exp: 0.5284
Episode_Reward/track_ang_vel_z_exp: 0.2975
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0463
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1231
     Episode_Reward/action_rate_l2: -0.0566
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6890
Metrics/base_velocity/error_vel_xy: 0.4011
Metrics/base_velocity/error_vel_yaw: 0.2818
      Episode_Termination/time_out: 0.4671
  Episode_Termination/base_contact: 0.5329
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 2.48s
                      Time elapsed: 00:27:48
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 655/1400 [0m                      

                       Computation: 9784 steps/s (collection: 2.255s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.1632
                       Mean reward: 8.88
               Mean episode length: 691.13
Episode_Reward/track_lin_vel_xy_exp: 0.5506
Episode_Reward/track_ang_vel_z_exp: 0.3031
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1215
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0066
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7008
Metrics/base_velocity/error_vel_xy: 0.3958
Metrics/base_velocity/error_vel_yaw: 0.3013
      Episode_Termination/time_out: 0.4663
  Episode_Termination/base_contact: 0.5337
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.51s
                      Time elapsed: 00:27:51
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 656/1400 [0m                      

                       Computation: 9811 steps/s (collection: 2.249s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 6.1675
                       Mean reward: 10.37
               Mean episode length: 747.97
Episode_Reward/track_lin_vel_xy_exp: 0.6317
Episode_Reward/track_ang_vel_z_exp: 0.3448
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0609
         Episode_Reward/dof_acc_l2: -0.1320
     Episode_Reward/action_rate_l2: -0.0637
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7043
Metrics/base_velocity/error_vel_xy: 0.4324
Metrics/base_velocity/error_vel_yaw: 0.3262
      Episode_Termination/time_out: 0.4696
  Episode_Termination/base_contact: 0.5304
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 2.50s
                      Time elapsed: 00:27:53
                               ETA: 00:31:35

################################################################################
                     [1m Learning iteration 657/1400 [0m                      

                       Computation: 9825 steps/s (collection: 2.245s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.1655
                       Mean reward: 10.51
               Mean episode length: 760.38
Episode_Reward/track_lin_vel_xy_exp: 0.5211
Episode_Reward/track_ang_vel_z_exp: 0.2948
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0447
     Episode_Reward/dof_torques_l2: -0.0553
         Episode_Reward/dof_acc_l2: -0.1206
     Episode_Reward/action_rate_l2: -0.0558
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6982
Metrics/base_velocity/error_vel_xy: 0.3957
Metrics/base_velocity/error_vel_yaw: 0.2785
      Episode_Termination/time_out: 0.4731
  Episode_Termination/base_contact: 0.5269
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 2.50s
                      Time elapsed: 00:27:56
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 658/1400 [0m                      

                       Computation: 9780 steps/s (collection: 2.259s, learning 0.254s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 6.1648
                       Mean reward: 10.52
               Mean episode length: 770.82
Episode_Reward/track_lin_vel_xy_exp: 0.5914
Episode_Reward/track_ang_vel_z_exp: 0.3312
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0508
     Episode_Reward/dof_torques_l2: -0.0629
         Episode_Reward/dof_acc_l2: -0.1417
     Episode_Reward/action_rate_l2: -0.0637
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7012
Metrics/base_velocity/error_vel_xy: 0.4545
Metrics/base_velocity/error_vel_yaw: 0.3288
      Episode_Termination/time_out: 0.4763
  Episode_Termination/base_contact: 0.5237
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 2.51s
                      Time elapsed: 00:27:58
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 659/1400 [0m                      

                       Computation: 9736 steps/s (collection: 2.273s, learning 0.252s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 6.1823
                       Mean reward: 9.08
               Mean episode length: 708.65
Episode_Reward/track_lin_vel_xy_exp: 0.4848
Episode_Reward/track_ang_vel_z_exp: 0.2697
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0528
         Episode_Reward/dof_acc_l2: -0.1239
     Episode_Reward/action_rate_l2: -0.0517
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7100
Metrics/base_velocity/error_vel_xy: 0.3773
Metrics/base_velocity/error_vel_yaw: 0.2752
      Episode_Termination/time_out: 0.4776
  Episode_Termination/base_contact: 0.5225
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.52s
                      Time elapsed: 00:28:01
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 660/1400 [0m                      

                       Computation: 9698 steps/s (collection: 2.277s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 6.1943
                       Mean reward: 8.85
               Mean episode length: 714.03
Episode_Reward/track_lin_vel_xy_exp: 0.5599
Episode_Reward/track_ang_vel_z_exp: 0.3289
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.0518
     Episode_Reward/dof_torques_l2: -0.0612
         Episode_Reward/dof_acc_l2: -0.1324
     Episode_Reward/action_rate_l2: -0.0628
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7032
Metrics/base_velocity/error_vel_xy: 0.5086
Metrics/base_velocity/error_vel_yaw: 0.3421
      Episode_Termination/time_out: 0.4798
  Episode_Termination/base_contact: 0.5212
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 2.53s
                      Time elapsed: 00:28:03
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 661/1400 [0m                      

                       Computation: 9689 steps/s (collection: 2.278s, learning 0.259s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.2019
                       Mean reward: 8.07
               Mean episode length: 663.03
Episode_Reward/track_lin_vel_xy_exp: 0.4170
Episode_Reward/track_ang_vel_z_exp: 0.2347
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0394
     Episode_Reward/dof_torques_l2: -0.0441
         Episode_Reward/dof_acc_l2: -0.0999
     Episode_Reward/action_rate_l2: -0.0460
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7010
Metrics/base_velocity/error_vel_xy: 0.3768
Metrics/base_velocity/error_vel_yaw: 0.2727
      Episode_Termination/time_out: 0.4823
  Episode_Termination/base_contact: 0.5187
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 2.54s
                      Time elapsed: 00:28:06
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 662/1400 [0m                      

                       Computation: 9779 steps/s (collection: 2.257s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 6.2070
                       Mean reward: 8.42
               Mean episode length: 680.61
Episode_Reward/track_lin_vel_xy_exp: 0.5022
Episode_Reward/track_ang_vel_z_exp: 0.2837
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0454
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1207
     Episode_Reward/action_rate_l2: -0.0554
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6981
Metrics/base_velocity/error_vel_xy: 0.4419
Metrics/base_velocity/error_vel_yaw: 0.2985
      Episode_Termination/time_out: 0.4833
  Episode_Termination/base_contact: 0.5177
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 2.51s
                      Time elapsed: 00:28:08
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 663/1400 [0m                      

                       Computation: 9812 steps/s (collection: 2.246s, learning 0.259s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 6.2133
                       Mean reward: 8.68
               Mean episode length: 678.10
Episode_Reward/track_lin_vel_xy_exp: 0.4792
Episode_Reward/track_ang_vel_z_exp: 0.2696
       Episode_Reward/lin_vel_z_l2: -0.0330
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.0528
         Episode_Reward/dof_acc_l2: -0.1111
     Episode_Reward/action_rate_l2: -0.0511
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0097
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7088
Metrics/base_velocity/error_vel_xy: 0.3964
Metrics/base_velocity/error_vel_yaw: 0.2753
      Episode_Termination/time_out: 0.4859
  Episode_Termination/base_contact: 0.5151
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.50s
                      Time elapsed: 00:28:11
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 664/1400 [0m                      

                       Computation: 9747 steps/s (collection: 2.265s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.2041
                       Mean reward: 9.71
               Mean episode length: 733.75
Episode_Reward/track_lin_vel_xy_exp: 0.6055
Episode_Reward/track_ang_vel_z_exp: 0.3337
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0516
     Episode_Reward/dof_torques_l2: -0.0623
         Episode_Reward/dof_acc_l2: -0.1392
     Episode_Reward/action_rate_l2: -0.0640
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0078
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7104
Metrics/base_velocity/error_vel_xy: 0.4392
Metrics/base_velocity/error_vel_yaw: 0.3292
      Episode_Termination/time_out: 0.4876
  Episode_Termination/base_contact: 0.5134
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 2.52s
                      Time elapsed: 00:28:13
                               ETA: 00:31:14

################################################################################
                     [1m Learning iteration 665/1400 [0m                      

                       Computation: 9782 steps/s (collection: 2.255s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.1952
                       Mean reward: 9.77
               Mean episode length: 732.05
Episode_Reward/track_lin_vel_xy_exp: 0.5499
Episode_Reward/track_ang_vel_z_exp: 0.3060
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0487
     Episode_Reward/dof_torques_l2: -0.0569
         Episode_Reward/dof_acc_l2: -0.1290
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7070
Metrics/base_velocity/error_vel_xy: 0.4085
Metrics/base_velocity/error_vel_yaw: 0.2985
      Episode_Termination/time_out: 0.4901
  Episode_Termination/base_contact: 0.5109
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 2.51s
                      Time elapsed: 00:28:16
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 666/1400 [0m                      

                       Computation: 9752 steps/s (collection: 2.263s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 6.1894
                       Mean reward: 9.70
               Mean episode length: 719.72
Episode_Reward/track_lin_vel_xy_exp: 0.5211
Episode_Reward/track_ang_vel_z_exp: 0.2978
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.0563
         Episode_Reward/dof_acc_l2: -0.1222
     Episode_Reward/action_rate_l2: -0.0572
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7093
Metrics/base_velocity/error_vel_xy: 0.4363
Metrics/base_velocity/error_vel_yaw: 0.3059
      Episode_Termination/time_out: 0.4931
  Episode_Termination/base_contact: 0.5079
--------------------------------------------------------------------------------
                   Total timesteps: 16392192
                    Iteration time: 2.52s
                      Time elapsed: 00:28:18
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 667/1400 [0m                      

                       Computation: 9804 steps/s (collection: 2.250s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.1856
                       Mean reward: 9.91
               Mean episode length: 718.47
Episode_Reward/track_lin_vel_xy_exp: 0.6278
Episode_Reward/track_ang_vel_z_exp: 0.3497
       Episode_Reward/lin_vel_z_l2: -0.0372
      Episode_Reward/ang_vel_xy_l2: -0.0524
     Episode_Reward/dof_torques_l2: -0.0656
         Episode_Reward/dof_acc_l2: -0.1438
     Episode_Reward/action_rate_l2: -0.0666
      Episode_Reward/feet_air_time: -0.0105
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7157
Metrics/base_velocity/error_vel_xy: 0.4896
Metrics/base_velocity/error_vel_yaw: 0.3547
      Episode_Termination/time_out: 0.4974
  Episode_Termination/base_contact: 0.5036
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.51s
                      Time elapsed: 00:28:21
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 668/1400 [0m                      

                       Computation: 9715 steps/s (collection: 2.278s, learning 0.251s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.1980
                       Mean reward: 9.47
               Mean episode length: 703.52
Episode_Reward/track_lin_vel_xy_exp: 0.3931
Episode_Reward/track_ang_vel_z_exp: 0.2457
       Episode_Reward/lin_vel_z_l2: -0.0317
      Episode_Reward/ang_vel_xy_l2: -0.0399
     Episode_Reward/dof_torques_l2: -0.0488
         Episode_Reward/dof_acc_l2: -0.1079
     Episode_Reward/action_rate_l2: -0.0487
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7291
Metrics/base_velocity/error_vel_xy: 0.4618
Metrics/base_velocity/error_vel_yaw: 0.2772
      Episode_Termination/time_out: 0.4955
  Episode_Termination/base_contact: 0.5055
--------------------------------------------------------------------------------
                   Total timesteps: 16441344
                    Iteration time: 2.53s
                      Time elapsed: 00:28:24
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 669/1400 [0m                      

                       Computation: 9750 steps/s (collection: 2.265s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.2019
                       Mean reward: 10.59
               Mean episode length: 781.33
Episode_Reward/track_lin_vel_xy_exp: 0.6200
Episode_Reward/track_ang_vel_z_exp: 0.3428
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.0517
     Episode_Reward/dof_torques_l2: -0.0640
         Episode_Reward/dof_acc_l2: -0.1429
     Episode_Reward/action_rate_l2: -0.0649
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7389
Metrics/base_velocity/error_vel_xy: 0.4451
Metrics/base_velocity/error_vel_yaw: 0.3256
      Episode_Termination/time_out: 0.5035
  Episode_Termination/base_contact: 0.4975
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 2.52s
                      Time elapsed: 00:28:26
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 670/1400 [0m                      

                       Computation: 9791 steps/s (collection: 2.253s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.1870
                       Mean reward: 10.06
               Mean episode length: 757.67
Episode_Reward/track_lin_vel_xy_exp: 0.5413
Episode_Reward/track_ang_vel_z_exp: 0.3080
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.0581
         Episode_Reward/dof_acc_l2: -0.1363
     Episode_Reward/action_rate_l2: -0.0594
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7399
Metrics/base_velocity/error_vel_xy: 0.4523
Metrics/base_velocity/error_vel_yaw: 0.3226
      Episode_Termination/time_out: 0.5104
  Episode_Termination/base_contact: 0.4906
--------------------------------------------------------------------------------
                   Total timesteps: 16490496
                    Iteration time: 2.51s
                      Time elapsed: 00:28:29
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 671/1400 [0m                      

                       Computation: 9834 steps/s (collection: 2.244s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.1706
                       Mean reward: 9.83
               Mean episode length: 756.75
Episode_Reward/track_lin_vel_xy_exp: 0.4494
Episode_Reward/track_ang_vel_z_exp: 0.2646
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0441
     Episode_Reward/dof_torques_l2: -0.0511
         Episode_Reward/dof_acc_l2: -0.1266
     Episode_Reward/action_rate_l2: -0.0512
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7350
Metrics/base_velocity/error_vel_xy: 0.4280
Metrics/base_velocity/error_vel_yaw: 0.2933
      Episode_Termination/time_out: 0.5120
  Episode_Termination/base_contact: 0.4890
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.50s
                      Time elapsed: 00:28:31
                               ETA: 00:30:56

################################################################################
                     [1m Learning iteration 672/1400 [0m                      

                       Computation: 9694 steps/s (collection: 2.280s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.1642
                       Mean reward: 8.97
               Mean episode length: 719.04
Episode_Reward/track_lin_vel_xy_exp: 0.5069
Episode_Reward/track_ang_vel_z_exp: 0.2879
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.0544
         Episode_Reward/dof_acc_l2: -0.1310
     Episode_Reward/action_rate_l2: -0.0558
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7393
Metrics/base_velocity/error_vel_xy: 0.4250
Metrics/base_velocity/error_vel_yaw: 0.3010
      Episode_Termination/time_out: 0.5122
  Episode_Termination/base_contact: 0.4888
--------------------------------------------------------------------------------
                   Total timesteps: 16539648
                    Iteration time: 2.54s
                      Time elapsed: 00:28:34
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 673/1400 [0m                      

                       Computation: 9833 steps/s (collection: 2.241s, learning 0.259s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.1457
                       Mean reward: 8.17
               Mean episode length: 683.34
Episode_Reward/track_lin_vel_xy_exp: 0.4399
Episode_Reward/track_ang_vel_z_exp: 0.2550
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0422
     Episode_Reward/dof_torques_l2: -0.0522
         Episode_Reward/dof_acc_l2: -0.1229
     Episode_Reward/action_rate_l2: -0.0504
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7440
Metrics/base_velocity/error_vel_xy: 0.4045
Metrics/base_velocity/error_vel_yaw: 0.2797
      Episode_Termination/time_out: 0.5117
  Episode_Termination/base_contact: 0.4893
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 2.50s
                      Time elapsed: 00:28:36
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 674/1400 [0m                      

                       Computation: 9939 steps/s (collection: 2.217s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.1432
                       Mean reward: 8.31
               Mean episode length: 696.84
Episode_Reward/track_lin_vel_xy_exp: 0.5094
Episode_Reward/track_ang_vel_z_exp: 0.3014
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.0620
         Episode_Reward/dof_acc_l2: -0.1562
     Episode_Reward/action_rate_l2: -0.0589
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7501
Metrics/base_velocity/error_vel_xy: 0.4613
Metrics/base_velocity/error_vel_yaw: 0.3078
      Episode_Termination/time_out: 0.5114
  Episode_Termination/base_contact: 0.4895
--------------------------------------------------------------------------------
                   Total timesteps: 16588800
                    Iteration time: 2.47s
                      Time elapsed: 00:28:39
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 675/1400 [0m                      

                       Computation: 9906 steps/s (collection: 2.219s, learning 0.262s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.1445
                       Mean reward: 8.04
               Mean episode length: 668.77
Episode_Reward/track_lin_vel_xy_exp: 0.4033
Episode_Reward/track_ang_vel_z_exp: 0.2351
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.0383
     Episode_Reward/dof_torques_l2: -0.0486
         Episode_Reward/dof_acc_l2: -0.1113
     Episode_Reward/action_rate_l2: -0.0468
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7563
Metrics/base_velocity/error_vel_xy: 0.3840
Metrics/base_velocity/error_vel_yaw: 0.2627
      Episode_Termination/time_out: 0.5130
  Episode_Termination/base_contact: 0.4880
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.48s
                      Time elapsed: 00:28:41
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 676/1400 [0m                      

                       Computation: 9701 steps/s (collection: 2.276s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.1516
                       Mean reward: 8.04
               Mean episode length: 680.64
Episode_Reward/track_lin_vel_xy_exp: 0.4894
Episode_Reward/track_ang_vel_z_exp: 0.2787
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.0480
     Episode_Reward/dof_torques_l2: -0.0562
         Episode_Reward/dof_acc_l2: -0.1336
     Episode_Reward/action_rate_l2: -0.0558
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7594
Metrics/base_velocity/error_vel_xy: 0.4425
Metrics/base_velocity/error_vel_yaw: 0.3230
      Episode_Termination/time_out: 0.5129
  Episode_Termination/base_contact: 0.4880
--------------------------------------------------------------------------------
                   Total timesteps: 16637952
                    Iteration time: 2.53s
                      Time elapsed: 00:28:44
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 677/1400 [0m                      

                       Computation: 9766 steps/s (collection: 2.259s, learning 0.258s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.1359
                       Mean reward: 8.67
               Mean episode length: 724.97
Episode_Reward/track_lin_vel_xy_exp: 0.6067
Episode_Reward/track_ang_vel_z_exp: 0.3463
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.0553
     Episode_Reward/dof_torques_l2: -0.0680
         Episode_Reward/dof_acc_l2: -0.1563
     Episode_Reward/action_rate_l2: -0.0675
      Episode_Reward/feet_air_time: -0.0100
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7659
Metrics/base_velocity/error_vel_xy: 0.5038
Metrics/base_velocity/error_vel_yaw: 0.3509
      Episode_Termination/time_out: 0.5177
  Episode_Termination/base_contact: 0.4833
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 2.52s
                      Time elapsed: 00:28:46
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 678/1400 [0m                      

                       Computation: 9884 steps/s (collection: 2.231s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.1313
                       Mean reward: 8.93
               Mean episode length: 729.56
Episode_Reward/track_lin_vel_xy_exp: 0.4895
Episode_Reward/track_ang_vel_z_exp: 0.2729
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0461
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1260
     Episode_Reward/action_rate_l2: -0.0533
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7801
Metrics/base_velocity/error_vel_xy: 0.3905
Metrics/base_velocity/error_vel_yaw: 0.2976
      Episode_Termination/time_out: 0.5191
  Episode_Termination/base_contact: 0.4819
--------------------------------------------------------------------------------
                   Total timesteps: 16687104
                    Iteration time: 2.49s
                      Time elapsed: 00:28:49
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 679/1400 [0m                      

                       Computation: 9780 steps/s (collection: 2.258s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 6.1402
                       Mean reward: 9.76
               Mean episode length: 749.88
Episode_Reward/track_lin_vel_xy_exp: 0.4905
Episode_Reward/track_ang_vel_z_exp: 0.2776
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0448
     Episode_Reward/dof_torques_l2: -0.0509
         Episode_Reward/dof_acc_l2: -0.1170
     Episode_Reward/action_rate_l2: -0.0536
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7802
Metrics/base_velocity/error_vel_xy: 0.4193
Metrics/base_velocity/error_vel_yaw: 0.2981
      Episode_Termination/time_out: 0.5193
  Episode_Termination/base_contact: 0.4817
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.51s
                      Time elapsed: 00:28:51
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 680/1400 [0m                      

                       Computation: 9780 steps/s (collection: 2.258s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.1381
                       Mean reward: 9.41
               Mean episode length: 725.53
Episode_Reward/track_lin_vel_xy_exp: 0.5477
Episode_Reward/track_ang_vel_z_exp: 0.3175
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.0550
     Episode_Reward/dof_torques_l2: -0.0632
         Episode_Reward/dof_acc_l2: -0.1504
     Episode_Reward/action_rate_l2: -0.0618
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7826
Metrics/base_velocity/error_vel_xy: 0.5213
Metrics/base_velocity/error_vel_yaw: 0.3452
      Episode_Termination/time_out: 0.5221
  Episode_Termination/base_contact: 0.4789
--------------------------------------------------------------------------------
                   Total timesteps: 16736256
                    Iteration time: 2.51s
                      Time elapsed: 00:28:54
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 681/1400 [0m                      

                       Computation: 9813 steps/s (collection: 2.248s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.1368
                       Mean reward: 9.61
               Mean episode length: 718.75
Episode_Reward/track_lin_vel_xy_exp: 0.5376
Episode_Reward/track_ang_vel_z_exp: 0.2913
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.0450
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1215
     Episode_Reward/action_rate_l2: -0.0557
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7877
Metrics/base_velocity/error_vel_xy: 0.3807
Metrics/base_velocity/error_vel_yaw: 0.2966
      Episode_Termination/time_out: 0.5205
  Episode_Termination/base_contact: 0.4805
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 2.50s
                      Time elapsed: 00:28:56
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 682/1400 [0m                      

                       Computation: 9860 steps/s (collection: 2.237s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 6.1381
                       Mean reward: 9.04
               Mean episode length: 712.53
Episode_Reward/track_lin_vel_xy_exp: 0.4761
Episode_Reward/track_ang_vel_z_exp: 0.2808
       Episode_Reward/lin_vel_z_l2: -0.0330
      Episode_Reward/ang_vel_xy_l2: -0.0440
     Episode_Reward/dof_torques_l2: -0.0547
         Episode_Reward/dof_acc_l2: -0.1160
     Episode_Reward/action_rate_l2: -0.0542
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0076
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7897
Metrics/base_velocity/error_vel_xy: 0.4222
Metrics/base_velocity/error_vel_yaw: 0.2692
      Episode_Termination/time_out: 0.5196
  Episode_Termination/base_contact: 0.4814
--------------------------------------------------------------------------------
                   Total timesteps: 16785408
                    Iteration time: 2.49s
                      Time elapsed: 00:28:59
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 683/1400 [0m                      

                       Computation: 9971 steps/s (collection: 2.208s, learning 0.257s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.1382
                       Mean reward: 9.46
               Mean episode length: 719.46
Episode_Reward/track_lin_vel_xy_exp: 0.6129
Episode_Reward/track_ang_vel_z_exp: 0.3253
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0550
         Episode_Reward/dof_acc_l2: -0.1375
     Episode_Reward/action_rate_l2: -0.0615
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7922
Metrics/base_velocity/error_vel_xy: 0.3678
Metrics/base_velocity/error_vel_yaw: 0.2923
      Episode_Termination/time_out: 0.5194
  Episode_Termination/base_contact: 0.4816
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.46s
                      Time elapsed: 00:29:01
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 684/1400 [0m                      

                       Computation: 9825 steps/s (collection: 2.246s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.1469
                       Mean reward: 9.49
               Mean episode length: 735.14
Episode_Reward/track_lin_vel_xy_exp: 0.5351
Episode_Reward/track_ang_vel_z_exp: 0.2936
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0558
         Episode_Reward/dof_acc_l2: -0.1318
     Episode_Reward/action_rate_l2: -0.0566
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8041
Metrics/base_velocity/error_vel_xy: 0.3906
Metrics/base_velocity/error_vel_yaw: 0.2982
      Episode_Termination/time_out: 0.5197
  Episode_Termination/base_contact: 0.4812
--------------------------------------------------------------------------------
                   Total timesteps: 16834560
                    Iteration time: 2.50s
                      Time elapsed: 00:29:04
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 685/1400 [0m                      

                       Computation: 9781 steps/s (collection: 2.258s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.1413
                       Mean reward: 10.17
               Mean episode length: 777.63
Episode_Reward/track_lin_vel_xy_exp: 0.5490
Episode_Reward/track_ang_vel_z_exp: 0.3165
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.0508
     Episode_Reward/dof_torques_l2: -0.0605
         Episode_Reward/dof_acc_l2: -0.1397
     Episode_Reward/action_rate_l2: -0.0610
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8176
Metrics/base_velocity/error_vel_xy: 0.4683
Metrics/base_velocity/error_vel_yaw: 0.3213
      Episode_Termination/time_out: 0.5210
  Episode_Termination/base_contact: 0.4799
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 2.51s
                      Time elapsed: 00:29:06
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 686/1400 [0m                      

                       Computation: 9820 steps/s (collection: 2.246s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 6.1317
                       Mean reward: 10.18
               Mean episode length: 768.83
Episode_Reward/track_lin_vel_xy_exp: 0.5891
Episode_Reward/track_ang_vel_z_exp: 0.3301
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0511
     Episode_Reward/dof_torques_l2: -0.0582
         Episode_Reward/dof_acc_l2: -0.1342
     Episode_Reward/action_rate_l2: -0.0613
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8285
Metrics/base_velocity/error_vel_xy: 0.4166
Metrics/base_velocity/error_vel_yaw: 0.2921
      Episode_Termination/time_out: 0.5237
  Episode_Termination/base_contact: 0.4773
--------------------------------------------------------------------------------
                   Total timesteps: 16883712
                    Iteration time: 2.50s
                      Time elapsed: 00:29:09
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 687/1400 [0m                      

                       Computation: 9822 steps/s (collection: 2.250s, learning 0.252s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.1241
                       Mean reward: 9.60
               Mean episode length: 748.75
Episode_Reward/track_lin_vel_xy_exp: 0.4620
Episode_Reward/track_ang_vel_z_exp: 0.2707
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0461
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1320
     Episode_Reward/action_rate_l2: -0.0547
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8277
Metrics/base_velocity/error_vel_xy: 0.4416
Metrics/base_velocity/error_vel_yaw: 0.2971
      Episode_Termination/time_out: 0.5184
  Episode_Termination/base_contact: 0.4825
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.50s
                      Time elapsed: 00:29:11
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 688/1400 [0m                      

                       Computation: 9723 steps/s (collection: 2.272s, learning 0.255s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 6.1221
                       Mean reward: 9.23
               Mean episode length: 740.57
Episode_Reward/track_lin_vel_xy_exp: 0.5835
Episode_Reward/track_ang_vel_z_exp: 0.3284
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.0542
     Episode_Reward/dof_torques_l2: -0.0621
         Episode_Reward/dof_acc_l2: -0.1495
     Episode_Reward/action_rate_l2: -0.0639
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8174
Metrics/base_velocity/error_vel_xy: 0.4620
Metrics/base_velocity/error_vel_yaw: 0.3271
      Episode_Termination/time_out: 0.5205
  Episode_Termination/base_contact: 0.4804
--------------------------------------------------------------------------------
                   Total timesteps: 16932864
                    Iteration time: 2.53s
                      Time elapsed: 00:29:14
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 689/1400 [0m                      

                       Computation: 9858 steps/s (collection: 2.237s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.1242
                       Mean reward: 9.38
               Mean episode length: 737.68
Episode_Reward/track_lin_vel_xy_exp: 0.5389
Episode_Reward/track_ang_vel_z_exp: 0.3024
       Episode_Reward/lin_vel_z_l2: -0.0367
      Episode_Reward/ang_vel_xy_l2: -0.0446
     Episode_Reward/dof_torques_l2: -0.0533
         Episode_Reward/dof_acc_l2: -0.1211
     Episode_Reward/action_rate_l2: -0.0568
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0072
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8251
Metrics/base_velocity/error_vel_xy: 0.4191
Metrics/base_velocity/error_vel_yaw: 0.2966
      Episode_Termination/time_out: 0.5265
  Episode_Termination/base_contact: 0.4745
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 2.49s
                      Time elapsed: 00:29:16
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 690/1400 [0m                      

                       Computation: 9862 steps/s (collection: 2.236s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.1097
                       Mean reward: 9.44
               Mean episode length: 724.96
Episode_Reward/track_lin_vel_xy_exp: 0.4970
Episode_Reward/track_ang_vel_z_exp: 0.2902
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0457
     Episode_Reward/dof_torques_l2: -0.0551
         Episode_Reward/dof_acc_l2: -0.1262
     Episode_Reward/action_rate_l2: -0.0563
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8274
Metrics/base_velocity/error_vel_xy: 0.4412
Metrics/base_velocity/error_vel_yaw: 0.2924
      Episode_Termination/time_out: 0.5244
  Episode_Termination/base_contact: 0.4766
--------------------------------------------------------------------------------
                   Total timesteps: 16982016
                    Iteration time: 2.49s
                      Time elapsed: 00:29:19
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 691/1400 [0m                      

                       Computation: 9793 steps/s (collection: 2.248s, learning 0.261s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.1010
                       Mean reward: 9.94
               Mean episode length: 751.32
Episode_Reward/track_lin_vel_xy_exp: 0.5563
Episode_Reward/track_ang_vel_z_exp: 0.3165
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0495
     Episode_Reward/dof_torques_l2: -0.0594
         Episode_Reward/dof_acc_l2: -0.1408
     Episode_Reward/action_rate_l2: -0.0610
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8384
Metrics/base_velocity/error_vel_xy: 0.4444
Metrics/base_velocity/error_vel_yaw: 0.3111
      Episode_Termination/time_out: 0.5288
  Episode_Termination/base_contact: 0.4722
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.51s
                      Time elapsed: 00:29:21
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 692/1400 [0m                      

                       Computation: 9769 steps/s (collection: 2.254s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.1028
                       Mean reward: 9.61
               Mean episode length: 732.40
Episode_Reward/track_lin_vel_xy_exp: 0.5327
Episode_Reward/track_ang_vel_z_exp: 0.2943
       Episode_Reward/lin_vel_z_l2: -0.0344
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0540
         Episode_Reward/dof_acc_l2: -0.1243
     Episode_Reward/action_rate_l2: -0.0564
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8499
Metrics/base_velocity/error_vel_xy: 0.3842
Metrics/base_velocity/error_vel_yaw: 0.2857
      Episode_Termination/time_out: 0.5319
  Episode_Termination/base_contact: 0.4691
--------------------------------------------------------------------------------
                   Total timesteps: 17031168
                    Iteration time: 2.52s
                      Time elapsed: 00:29:24
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 693/1400 [0m                      

                       Computation: 9728 steps/s (collection: 2.271s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 6.1026
                       Mean reward: 9.89
               Mean episode length: 745.02
Episode_Reward/track_lin_vel_xy_exp: 0.5679
Episode_Reward/track_ang_vel_z_exp: 0.3156
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0602
         Episode_Reward/dof_acc_l2: -0.1412
     Episode_Reward/action_rate_l2: -0.0614
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8563
Metrics/base_velocity/error_vel_xy: 0.4339
Metrics/base_velocity/error_vel_yaw: 0.3182
      Episode_Termination/time_out: 0.5318
  Episode_Termination/base_contact: 0.4692
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 2.53s
                      Time elapsed: 00:29:26
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 694/1400 [0m                      

                       Computation: 9773 steps/s (collection: 2.258s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 6.0868
                       Mean reward: 9.11
               Mean episode length: 715.49
Episode_Reward/track_lin_vel_xy_exp: 0.4701
Episode_Reward/track_ang_vel_z_exp: 0.2713
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0460
     Episode_Reward/dof_torques_l2: -0.0523
         Episode_Reward/dof_acc_l2: -0.1249
     Episode_Reward/action_rate_l2: -0.0529
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8593
Metrics/base_velocity/error_vel_xy: 0.4157
Metrics/base_velocity/error_vel_yaw: 0.2941
      Episode_Termination/time_out: 0.5334
  Episode_Termination/base_contact: 0.4675
--------------------------------------------------------------------------------
                   Total timesteps: 17080320
                    Iteration time: 2.51s
                      Time elapsed: 00:29:29
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 695/1400 [0m                      

                       Computation: 9844 steps/s (collection: 2.234s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.0863
                       Mean reward: 9.73
               Mean episode length: 750.57
Episode_Reward/track_lin_vel_xy_exp: 0.4868
Episode_Reward/track_ang_vel_z_exp: 0.2741
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0453
     Episode_Reward/dof_torques_l2: -0.0526
         Episode_Reward/dof_acc_l2: -0.1182
     Episode_Reward/action_rate_l2: -0.0532
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8685
Metrics/base_velocity/error_vel_xy: 0.3949
Metrics/base_velocity/error_vel_yaw: 0.2786
      Episode_Termination/time_out: 0.5373
  Episode_Termination/base_contact: 0.4637
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.50s
                      Time elapsed: 00:29:31
                               ETA: 00:29:54

################################################################################
                     [1m Learning iteration 696/1400 [0m                      

                       Computation: 9881 steps/s (collection: 2.222s, learning 0.266s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.0979
                       Mean reward: 9.98
               Mean episode length: 776.88
Episode_Reward/track_lin_vel_xy_exp: 0.5725
Episode_Reward/track_ang_vel_z_exp: 0.3275
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.0535
     Episode_Reward/dof_torques_l2: -0.0649
         Episode_Reward/dof_acc_l2: -0.1441
     Episode_Reward/action_rate_l2: -0.0644
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8736
Metrics/base_velocity/error_vel_xy: 0.5251
Metrics/base_velocity/error_vel_yaw: 0.3523
      Episode_Termination/time_out: 0.5353
  Episode_Termination/base_contact: 0.4657
--------------------------------------------------------------------------------
                   Total timesteps: 17129472
                    Iteration time: 2.49s
                      Time elapsed: 00:29:34
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 697/1400 [0m                      

                       Computation: 9824 steps/s (collection: 2.247s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 6.1015
                       Mean reward: 10.09
               Mean episode length: 791.34
Episode_Reward/track_lin_vel_xy_exp: 0.5318
Episode_Reward/track_ang_vel_z_exp: 0.3029
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0504
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.1361
     Episode_Reward/action_rate_l2: -0.0592
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0103
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8808
Metrics/base_velocity/error_vel_xy: 0.4511
Metrics/base_velocity/error_vel_yaw: 0.3281
      Episode_Termination/time_out: 0.5354
  Episode_Termination/base_contact: 0.4656
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 2.50s
                      Time elapsed: 00:29:36
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 698/1400 [0m                      

                       Computation: 9805 steps/s (collection: 2.244s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.1041
                       Mean reward: 9.41
               Mean episode length: 741.75
Episode_Reward/track_lin_vel_xy_exp: 0.5505
Episode_Reward/track_ang_vel_z_exp: 0.3037
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0565
         Episode_Reward/dof_acc_l2: -0.1333
     Episode_Reward/action_rate_l2: -0.0586
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8912
Metrics/base_velocity/error_vel_xy: 0.3962
Metrics/base_velocity/error_vel_yaw: 0.2927
      Episode_Termination/time_out: 0.5341
  Episode_Termination/base_contact: 0.4669
--------------------------------------------------------------------------------
                   Total timesteps: 17178624
                    Iteration time: 2.51s
                      Time elapsed: 00:29:39
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 699/1400 [0m                      

                       Computation: 9737 steps/s (collection: 2.272s, learning 0.252s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 6.0998
                       Mean reward: 9.45
               Mean episode length: 726.87
Episode_Reward/track_lin_vel_xy_exp: 0.5657
Episode_Reward/track_ang_vel_z_exp: 0.3170
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0542
         Episode_Reward/dof_acc_l2: -0.1263
     Episode_Reward/action_rate_l2: -0.0594
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8868
Metrics/base_velocity/error_vel_xy: 0.4148
Metrics/base_velocity/error_vel_yaw: 0.2999
      Episode_Termination/time_out: 0.5312
  Episode_Termination/base_contact: 0.4697
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.52s
                      Time elapsed: 00:29:41
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 700/1400 [0m                      

                       Computation: 9771 steps/s (collection: 2.259s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.0923
                       Mean reward: 9.42
               Mean episode length: 700.47
Episode_Reward/track_lin_vel_xy_exp: 0.4828
Episode_Reward/track_ang_vel_z_exp: 0.2759
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0439
     Episode_Reward/dof_torques_l2: -0.0497
         Episode_Reward/dof_acc_l2: -0.1164
     Episode_Reward/action_rate_l2: -0.0524
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8865
Metrics/base_velocity/error_vel_xy: 0.3912
Metrics/base_velocity/error_vel_yaw: 0.2617
      Episode_Termination/time_out: 0.5304
  Episode_Termination/base_contact: 0.4706
--------------------------------------------------------------------------------
                   Total timesteps: 17227776
                    Iteration time: 2.52s
                      Time elapsed: 00:29:44
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 701/1400 [0m                      

                       Computation: 9817 steps/s (collection: 2.248s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.0923
                       Mean reward: 9.54
               Mean episode length: 714.94
Episode_Reward/track_lin_vel_xy_exp: 0.5437
Episode_Reward/track_ang_vel_z_exp: 0.3185
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.0522
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1424
     Episode_Reward/action_rate_l2: -0.0606
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8873
Metrics/base_velocity/error_vel_xy: 0.4694
Metrics/base_velocity/error_vel_yaw: 0.3042
      Episode_Termination/time_out: 0.5314
  Episode_Termination/base_contact: 0.4692
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 2.50s
                      Time elapsed: 00:29:46
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 702/1400 [0m                      

                       Computation: 9806 steps/s (collection: 2.243s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 6.0893
                       Mean reward: 9.36
               Mean episode length: 712.15
Episode_Reward/track_lin_vel_xy_exp: 0.5999
Episode_Reward/track_ang_vel_z_exp: 0.3307
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0520
     Episode_Reward/dof_torques_l2: -0.0603
         Episode_Reward/dof_acc_l2: -0.1403
     Episode_Reward/action_rate_l2: -0.0626
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8903
Metrics/base_velocity/error_vel_xy: 0.4448
Metrics/base_velocity/error_vel_yaw: 0.3232
      Episode_Termination/time_out: 0.5316
  Episode_Termination/base_contact: 0.4684
--------------------------------------------------------------------------------
                   Total timesteps: 17276928
                    Iteration time: 2.51s
                      Time elapsed: 00:29:49
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 703/1400 [0m                      

                       Computation: 9845 steps/s (collection: 2.240s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.0873
                       Mean reward: 10.08
               Mean episode length: 745.98
Episode_Reward/track_lin_vel_xy_exp: 0.6482
Episode_Reward/track_ang_vel_z_exp: 0.3621
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0618
         Episode_Reward/dof_acc_l2: -0.1417
     Episode_Reward/action_rate_l2: -0.0672
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8986
Metrics/base_velocity/error_vel_xy: 0.4866
Metrics/base_velocity/error_vel_yaw: 0.3311
      Episode_Termination/time_out: 0.5337
  Episode_Termination/base_contact: 0.4663
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.50s
                      Time elapsed: 00:29:51
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 704/1400 [0m                      

                       Computation: 9698 steps/s (collection: 2.278s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.1030
                       Mean reward: 10.69
               Mean episode length: 787.99
Episode_Reward/track_lin_vel_xy_exp: 0.5345
Episode_Reward/track_ang_vel_z_exp: 0.3026
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0567
         Episode_Reward/dof_acc_l2: -0.1317
     Episode_Reward/action_rate_l2: -0.0586
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9066
Metrics/base_velocity/error_vel_xy: 0.4232
Metrics/base_velocity/error_vel_yaw: 0.2964
      Episode_Termination/time_out: 0.5389
  Episode_Termination/base_contact: 0.4613
--------------------------------------------------------------------------------
                   Total timesteps: 17326080
                    Iteration time: 2.53s
                      Time elapsed: 00:29:54
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 705/1400 [0m                      

                       Computation: 9895 steps/s (collection: 2.226s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 6.1040
                       Mean reward: 10.70
               Mean episode length: 775.65
Episode_Reward/track_lin_vel_xy_exp: 0.5616
Episode_Reward/track_ang_vel_z_exp: 0.3206
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0499
     Episode_Reward/dof_torques_l2: -0.0612
         Episode_Reward/dof_acc_l2: -0.1259
     Episode_Reward/action_rate_l2: -0.0622
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0147
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9104
Metrics/base_velocity/error_vel_xy: 0.4919
Metrics/base_velocity/error_vel_yaw: 0.3505
      Episode_Termination/time_out: 0.5423
  Episode_Termination/base_contact: 0.4587
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 2.48s
                      Time elapsed: 00:29:56
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 706/1400 [0m                      

                       Computation: 9805 steps/s (collection: 2.253s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.1112
                       Mean reward: 10.06
               Mean episode length: 767.76
Episode_Reward/track_lin_vel_xy_exp: 0.5000
Episode_Reward/track_ang_vel_z_exp: 0.3087
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0591
         Episode_Reward/dof_acc_l2: -0.1635
     Episode_Reward/action_rate_l2: -0.0607
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9246
Metrics/base_velocity/error_vel_xy: 0.5345
Metrics/base_velocity/error_vel_yaw: 0.3283
      Episode_Termination/time_out: 0.5412
  Episode_Termination/base_contact: 0.4598
--------------------------------------------------------------------------------
                   Total timesteps: 17375232
                    Iteration time: 2.51s
                      Time elapsed: 00:29:59
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 707/1400 [0m                      

                       Computation: 9850 steps/s (collection: 2.232s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.1108
                       Mean reward: 9.63
               Mean episode length: 760.26
Episode_Reward/track_lin_vel_xy_exp: 0.4468
Episode_Reward/track_ang_vel_z_exp: 0.2617
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0433
     Episode_Reward/dof_torques_l2: -0.0522
         Episode_Reward/dof_acc_l2: -0.1185
     Episode_Reward/action_rate_l2: -0.0500
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9321
Metrics/base_velocity/error_vel_xy: 0.3974
Metrics/base_velocity/error_vel_yaw: 0.2586
      Episode_Termination/time_out: 0.5389
  Episode_Termination/base_contact: 0.4621
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.49s
                      Time elapsed: 00:30:01
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 708/1400 [0m                      

                       Computation: 9836 steps/s (collection: 2.234s, learning 0.265s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.1153
                       Mean reward: 9.85
               Mean episode length: 779.04
Episode_Reward/track_lin_vel_xy_exp: 0.6211
Episode_Reward/track_ang_vel_z_exp: 0.3646
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.0602
     Episode_Reward/dof_torques_l2: -0.0703
         Episode_Reward/dof_acc_l2: -0.1743
     Episode_Reward/action_rate_l2: -0.0708
      Episode_Reward/feet_air_time: -0.0106
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9432
Metrics/base_velocity/error_vel_xy: 0.5641
Metrics/base_velocity/error_vel_yaw: 0.3643
      Episode_Termination/time_out: 0.5402
  Episode_Termination/base_contact: 0.4608
--------------------------------------------------------------------------------
                   Total timesteps: 17424384
                    Iteration time: 2.50s
                      Time elapsed: 00:30:04
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 709/1400 [0m                      

                       Computation: 9601 steps/s (collection: 2.304s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.1209
                       Mean reward: 9.31
               Mean episode length: 754.39
Episode_Reward/track_lin_vel_xy_exp: 0.5281
Episode_Reward/track_ang_vel_z_exp: 0.3076
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0627
         Episode_Reward/dof_acc_l2: -0.1430
     Episode_Reward/action_rate_l2: -0.0614
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9455
Metrics/base_velocity/error_vel_xy: 0.4962
Metrics/base_velocity/error_vel_yaw: 0.3385
      Episode_Termination/time_out: 0.5422
  Episode_Termination/base_contact: 0.4587
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 2.56s
                      Time elapsed: 00:30:06
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 710/1400 [0m                      

                       Computation: 9684 steps/s (collection: 2.282s, learning 0.256s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.1320
                       Mean reward: 8.78
               Mean episode length: 737.64
Episode_Reward/track_lin_vel_xy_exp: 0.4298
Episode_Reward/track_ang_vel_z_exp: 0.2546
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0445
     Episode_Reward/dof_torques_l2: -0.0510
         Episode_Reward/dof_acc_l2: -0.1191
     Episode_Reward/action_rate_l2: -0.0497
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9494
Metrics/base_velocity/error_vel_xy: 0.4091
Metrics/base_velocity/error_vel_yaw: 0.2663
      Episode_Termination/time_out: 0.5406
  Episode_Termination/base_contact: 0.4604
--------------------------------------------------------------------------------
                   Total timesteps: 17473536
                    Iteration time: 2.54s
                      Time elapsed: 00:30:09
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 711/1400 [0m                      

                       Computation: 9731 steps/s (collection: 2.273s, learning 0.253s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.1274
                       Mean reward: 8.85
               Mean episode length: 717.19
Episode_Reward/track_lin_vel_xy_exp: 0.5146
Episode_Reward/track_ang_vel_z_exp: 0.2956
       Episode_Reward/lin_vel_z_l2: -0.0372
      Episode_Reward/ang_vel_xy_l2: -0.0480
     Episode_Reward/dof_torques_l2: -0.0602
         Episode_Reward/dof_acc_l2: -0.1368
     Episode_Reward/action_rate_l2: -0.0583
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0107
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9435
Metrics/base_velocity/error_vel_xy: 0.4458
Metrics/base_velocity/error_vel_yaw: 0.2978
      Episode_Termination/time_out: 0.5372
  Episode_Termination/base_contact: 0.4638
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.53s
                      Time elapsed: 00:30:11
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 712/1400 [0m                      

                       Computation: 9758 steps/s (collection: 2.267s, learning 0.252s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.1231
                       Mean reward: 8.89
               Mean episode length: 726.07
Episode_Reward/track_lin_vel_xy_exp: 0.5229
Episode_Reward/track_ang_vel_z_exp: 0.3003
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0572
         Episode_Reward/dof_acc_l2: -0.1401
     Episode_Reward/action_rate_l2: -0.0595
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9406
Metrics/base_velocity/error_vel_xy: 0.4546
Metrics/base_velocity/error_vel_yaw: 0.3166
      Episode_Termination/time_out: 0.5338
  Episode_Termination/base_contact: 0.4672
--------------------------------------------------------------------------------
                   Total timesteps: 17522688
                    Iteration time: 2.52s
                      Time elapsed: 00:30:14
                               ETA: 00:29:10

################################################################################
                     [1m Learning iteration 713/1400 [0m                      

                       Computation: 9918 steps/s (collection: 2.221s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.1134
                       Mean reward: 8.95
               Mean episode length: 734.50
Episode_Reward/track_lin_vel_xy_exp: 0.5326
Episode_Reward/track_ang_vel_z_exp: 0.3204
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.0563
     Episode_Reward/dof_torques_l2: -0.0609
         Episode_Reward/dof_acc_l2: -0.1577
     Episode_Reward/action_rate_l2: -0.0627
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9376
Metrics/base_velocity/error_vel_xy: 0.5287
Metrics/base_velocity/error_vel_yaw: 0.3306
      Episode_Termination/time_out: 0.5321
  Episode_Termination/base_contact: 0.4689
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 2.48s
                      Time elapsed: 00:30:16
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 714/1400 [0m                      

                       Computation: 9879 steps/s (collection: 2.226s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 6.0989
                       Mean reward: 9.19
               Mean episode length: 741.00
Episode_Reward/track_lin_vel_xy_exp: 0.5256
Episode_Reward/track_ang_vel_z_exp: 0.2994
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.0508
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1393
     Episode_Reward/action_rate_l2: -0.0586
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9371
Metrics/base_velocity/error_vel_xy: 0.4179
Metrics/base_velocity/error_vel_yaw: 0.2903
      Episode_Termination/time_out: 0.5310
  Episode_Termination/base_contact: 0.4700
--------------------------------------------------------------------------------
                   Total timesteps: 17571840
                    Iteration time: 2.49s
                      Time elapsed: 00:30:19
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 715/1400 [0m                      

                       Computation: 9869 steps/s (collection: 2.234s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 6.0894
                       Mean reward: 8.96
               Mean episode length: 723.76
Episode_Reward/track_lin_vel_xy_exp: 0.5259
Episode_Reward/track_ang_vel_z_exp: 0.2943
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0574
         Episode_Reward/dof_acc_l2: -0.1325
     Episode_Reward/action_rate_l2: -0.0573
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9424
Metrics/base_velocity/error_vel_xy: 0.4118
Metrics/base_velocity/error_vel_yaw: 0.2985
      Episode_Termination/time_out: 0.5286
  Episode_Termination/base_contact: 0.4724
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.49s
                      Time elapsed: 00:30:21
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 716/1400 [0m                      

                       Computation: 9913 steps/s (collection: 2.229s, learning 0.250s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.0788
                       Mean reward: 9.49
               Mean episode length: 749.10
Episode_Reward/track_lin_vel_xy_exp: 0.6083
Episode_Reward/track_ang_vel_z_exp: 0.3297
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0511
     Episode_Reward/dof_torques_l2: -0.0590
         Episode_Reward/dof_acc_l2: -0.1487
     Episode_Reward/action_rate_l2: -0.0639
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9354
Metrics/base_velocity/error_vel_xy: 0.4283
Metrics/base_velocity/error_vel_yaw: 0.3327
      Episode_Termination/time_out: 0.5298
  Episode_Termination/base_contact: 0.4712
--------------------------------------------------------------------------------
                   Total timesteps: 17620992
                    Iteration time: 2.48s
                      Time elapsed: 00:30:24
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 717/1400 [0m                      

                       Computation: 10024 steps/s (collection: 2.196s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.0766
                       Mean reward: 9.54
               Mean episode length: 737.36
Episode_Reward/track_lin_vel_xy_exp: 0.6372
Episode_Reward/track_ang_vel_z_exp: 0.3483
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.0539
     Episode_Reward/dof_torques_l2: -0.0637
         Episode_Reward/dof_acc_l2: -0.1397
     Episode_Reward/action_rate_l2: -0.0660
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9375
Metrics/base_velocity/error_vel_xy: 0.4088
Metrics/base_velocity/error_vel_yaw: 0.2964
      Episode_Termination/time_out: 0.5329
  Episode_Termination/base_contact: 0.4681
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 2.45s
                      Time elapsed: 00:30:26
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 718/1400 [0m                      

                       Computation: 9707 steps/s (collection: 2.275s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 6.0646
                       Mean reward: 10.89
               Mean episode length: 797.06
Episode_Reward/track_lin_vel_xy_exp: 0.6126
Episode_Reward/track_ang_vel_z_exp: 0.3395
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0521
     Episode_Reward/dof_torques_l2: -0.0633
         Episode_Reward/dof_acc_l2: -0.1470
     Episode_Reward/action_rate_l2: -0.0653
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9521
Metrics/base_velocity/error_vel_xy: 0.4490
Metrics/base_velocity/error_vel_yaw: 0.3143
      Episode_Termination/time_out: 0.5412
  Episode_Termination/base_contact: 0.4598
--------------------------------------------------------------------------------
                   Total timesteps: 17670144
                    Iteration time: 2.53s
                      Time elapsed: 00:30:29
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 719/1400 [0m                      

                       Computation: 9815 steps/s (collection: 2.248s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 6.0731
                       Mean reward: 10.98
               Mean episode length: 795.07
Episode_Reward/track_lin_vel_xy_exp: 0.6005
Episode_Reward/track_ang_vel_z_exp: 0.3351
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0499
     Episode_Reward/dof_torques_l2: -0.0605
         Episode_Reward/dof_acc_l2: -0.1417
     Episode_Reward/action_rate_l2: -0.0639
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9612
Metrics/base_velocity/error_vel_xy: 0.4434
Metrics/base_velocity/error_vel_yaw: 0.3235
      Episode_Termination/time_out: 0.5455
  Episode_Termination/base_contact: 0.4555
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.50s
                      Time elapsed: 00:30:31
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 720/1400 [0m                      

                       Computation: 9818 steps/s (collection: 2.248s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 6.0759
                       Mean reward: 10.56
               Mean episode length: 792.39
Episode_Reward/track_lin_vel_xy_exp: 0.5327
Episode_Reward/track_ang_vel_z_exp: 0.3096
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0504
     Episode_Reward/dof_torques_l2: -0.0598
         Episode_Reward/dof_acc_l2: -0.1442
     Episode_Reward/action_rate_l2: -0.0621
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9648
Metrics/base_velocity/error_vel_xy: 0.4967
Metrics/base_velocity/error_vel_yaw: 0.3445
      Episode_Termination/time_out: 0.5453
  Episode_Termination/base_contact: 0.4557
--------------------------------------------------------------------------------
                   Total timesteps: 17719296
                    Iteration time: 2.50s
                      Time elapsed: 00:30:34
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 721/1400 [0m                      

                       Computation: 9786 steps/s (collection: 2.247s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.0713
                       Mean reward: 9.52
               Mean episode length: 758.89
Episode_Reward/track_lin_vel_xy_exp: 0.4670
Episode_Reward/track_ang_vel_z_exp: 0.2852
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0575
         Episode_Reward/dof_acc_l2: -0.1298
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0175
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9638
Metrics/base_velocity/error_vel_xy: 0.5188
Metrics/base_velocity/error_vel_yaw: 0.3319
      Episode_Termination/time_out: 0.5469
  Episode_Termination/base_contact: 0.4541
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 2.51s
                      Time elapsed: 00:30:36
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 722/1400 [0m                      

                       Computation: 9860 steps/s (collection: 2.227s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.0703
                       Mean reward: 8.51
               Mean episode length: 699.09
Episode_Reward/track_lin_vel_xy_exp: 0.4249
Episode_Reward/track_ang_vel_z_exp: 0.2441
       Episode_Reward/lin_vel_z_l2: -0.0321
      Episode_Reward/ang_vel_xy_l2: -0.0408
     Episode_Reward/dof_torques_l2: -0.0465
         Episode_Reward/dof_acc_l2: -0.1130
     Episode_Reward/action_rate_l2: -0.0471
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9694
Metrics/base_velocity/error_vel_xy: 0.3505
Metrics/base_velocity/error_vel_yaw: 0.2408
      Episode_Termination/time_out: 0.5474
  Episode_Termination/base_contact: 0.4536
--------------------------------------------------------------------------------
                   Total timesteps: 17768448
                    Iteration time: 2.49s
                      Time elapsed: 00:30:39
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 723/1400 [0m                      

                       Computation: 10039 steps/s (collection: 2.193s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 6.0740
                       Mean reward: 8.87
               Mean episode length: 730.05
Episode_Reward/track_lin_vel_xy_exp: 0.5558
Episode_Reward/track_ang_vel_z_exp: 0.3273
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.0550
     Episode_Reward/dof_torques_l2: -0.0648
         Episode_Reward/dof_acc_l2: -0.1489
     Episode_Reward/action_rate_l2: -0.0648
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9681
Metrics/base_velocity/error_vel_xy: 0.5265
Metrics/base_velocity/error_vel_yaw: 0.3593
      Episode_Termination/time_out: 0.5503
  Episode_Termination/base_contact: 0.4507
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.45s
                      Time elapsed: 00:30:41
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 724/1400 [0m                      

                       Computation: 9837 steps/s (collection: 2.241s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 6.0686
                       Mean reward: 9.44
               Mean episode length: 739.88
Episode_Reward/track_lin_vel_xy_exp: 0.5452
Episode_Reward/track_ang_vel_z_exp: 0.3192
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0602
         Episode_Reward/dof_acc_l2: -0.1417
     Episode_Reward/action_rate_l2: -0.0621
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9654
Metrics/base_velocity/error_vel_xy: 0.4867
Metrics/base_velocity/error_vel_yaw: 0.3135
      Episode_Termination/time_out: 0.5515
  Episode_Termination/base_contact: 0.4495
--------------------------------------------------------------------------------
                   Total timesteps: 17817600
                    Iteration time: 2.50s
                      Time elapsed: 00:30:44
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 725/1400 [0m                      

                       Computation: 9986 steps/s (collection: 2.199s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.0654
                       Mean reward: 9.23
               Mean episode length: 751.38
Episode_Reward/track_lin_vel_xy_exp: 0.4861
Episode_Reward/track_ang_vel_z_exp: 0.2892
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1364
     Episode_Reward/action_rate_l2: -0.0568
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9612
Metrics/base_velocity/error_vel_xy: 0.4721
Metrics/base_velocity/error_vel_yaw: 0.3031
      Episode_Termination/time_out: 0.5498
  Episode_Termination/base_contact: 0.4512
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 2.46s
                      Time elapsed: 00:30:46
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 726/1400 [0m                      

                       Computation: 9900 steps/s (collection: 2.228s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.0680
                       Mean reward: 9.75
               Mean episode length: 756.28
Episode_Reward/track_lin_vel_xy_exp: 0.5116
Episode_Reward/track_ang_vel_z_exp: 0.3002
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0584
         Episode_Reward/dof_acc_l2: -0.1280
     Episode_Reward/action_rate_l2: -0.0583
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9614
Metrics/base_velocity/error_vel_xy: 0.4686
Metrics/base_velocity/error_vel_yaw: 0.3088
      Episode_Termination/time_out: 0.5517
  Episode_Termination/base_contact: 0.4493
--------------------------------------------------------------------------------
                   Total timesteps: 17866752
                    Iteration time: 2.48s
                      Time elapsed: 00:30:49
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 727/1400 [0m                      

                       Computation: 9713 steps/s (collection: 2.275s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.0793
                       Mean reward: 9.68
               Mean episode length: 760.46
Episode_Reward/track_lin_vel_xy_exp: 0.5314
Episode_Reward/track_ang_vel_z_exp: 0.3037
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1308
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9635
Metrics/base_velocity/error_vel_xy: 0.4271
Metrics/base_velocity/error_vel_yaw: 0.2971
      Episode_Termination/time_out: 0.5566
  Episode_Termination/base_contact: 0.4444
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.53s
                      Time elapsed: 00:30:51
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 728/1400 [0m                      

                       Computation: 9904 steps/s (collection: 2.226s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.0914
                       Mean reward: 9.81
               Mean episode length: 744.31
Episode_Reward/track_lin_vel_xy_exp: 0.4934
Episode_Reward/track_ang_vel_z_exp: 0.2864
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.0457
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.1222
     Episode_Reward/action_rate_l2: -0.0564
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9574
Metrics/base_velocity/error_vel_xy: 0.4387
Metrics/base_velocity/error_vel_yaw: 0.2977
      Episode_Termination/time_out: 0.5566
  Episode_Termination/base_contact: 0.4443
--------------------------------------------------------------------------------
                   Total timesteps: 17915904
                    Iteration time: 2.48s
                      Time elapsed: 00:30:54
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 729/1400 [0m                      

                       Computation: 9751 steps/s (collection: 2.266s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.0999
                       Mean reward: 9.49
               Mean episode length: 742.32
Episode_Reward/track_lin_vel_xy_exp: 0.5578
Episode_Reward/track_ang_vel_z_exp: 0.3247
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0519
     Episode_Reward/dof_torques_l2: -0.0588
         Episode_Reward/dof_acc_l2: -0.1364
     Episode_Reward/action_rate_l2: -0.0624
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9601
Metrics/base_velocity/error_vel_xy: 0.4571
Metrics/base_velocity/error_vel_yaw: 0.3076
      Episode_Termination/time_out: 0.5596
  Episode_Termination/base_contact: 0.4414
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 2.52s
                      Time elapsed: 00:30:56
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 730/1400 [0m                      

                       Computation: 9708 steps/s (collection: 2.269s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 6.0907
                       Mean reward: 10.44
               Mean episode length: 787.45
Episode_Reward/track_lin_vel_xy_exp: 0.5771
Episode_Reward/track_ang_vel_z_exp: 0.3292
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0615
         Episode_Reward/dof_acc_l2: -0.1456
     Episode_Reward/action_rate_l2: -0.0635
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9762
Metrics/base_velocity/error_vel_xy: 0.4652
Metrics/base_velocity/error_vel_yaw: 0.3178
      Episode_Termination/time_out: 0.5618
  Episode_Termination/base_contact: 0.4392
--------------------------------------------------------------------------------
                   Total timesteps: 17965056
                    Iteration time: 2.53s
                      Time elapsed: 00:30:59
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 731/1400 [0m                      

                       Computation: 9881 steps/s (collection: 2.226s, learning 0.261s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.0775
                       Mean reward: 10.82
               Mean episode length: 817.09
Episode_Reward/track_lin_vel_xy_exp: 0.5951
Episode_Reward/track_ang_vel_z_exp: 0.3438
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0505
     Episode_Reward/dof_torques_l2: -0.0617
         Episode_Reward/dof_acc_l2: -0.1369
     Episode_Reward/action_rate_l2: -0.0645
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9794
Metrics/base_velocity/error_vel_xy: 0.4781
Metrics/base_velocity/error_vel_yaw: 0.3125
      Episode_Termination/time_out: 0.5642
  Episode_Termination/base_contact: 0.4368
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.49s
                      Time elapsed: 00:31:01
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 732/1400 [0m                      

                       Computation: 9931 steps/s (collection: 2.218s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.0753
                       Mean reward: 10.12
               Mean episode length: 791.45
Episode_Reward/track_lin_vel_xy_exp: 0.4701
Episode_Reward/track_ang_vel_z_exp: 0.2857
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.1252
     Episode_Reward/action_rate_l2: -0.0560
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9770
Metrics/base_velocity/error_vel_xy: 0.4731
Metrics/base_velocity/error_vel_yaw: 0.2973
      Episode_Termination/time_out: 0.5642
  Episode_Termination/base_contact: 0.4368
--------------------------------------------------------------------------------
                   Total timesteps: 18014208
                    Iteration time: 2.47s
                      Time elapsed: 00:31:04
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 733/1400 [0m                      

                       Computation: 9772 steps/s (collection: 2.260s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.0831
                       Mean reward: 9.96
               Mean episode length: 750.64
Episode_Reward/track_lin_vel_xy_exp: 0.5314
Episode_Reward/track_ang_vel_z_exp: 0.2986
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1291
     Episode_Reward/action_rate_l2: -0.0566
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9815
Metrics/base_velocity/error_vel_xy: 0.3850
Metrics/base_velocity/error_vel_yaw: 0.2667
      Episode_Termination/time_out: 0.5683
  Episode_Termination/base_contact: 0.4327
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 2.51s
                      Time elapsed: 00:31:06
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 734/1400 [0m                      

                       Computation: 9989 steps/s (collection: 2.198s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 6.0757
                       Mean reward: 9.89
               Mean episode length: 733.46
Episode_Reward/track_lin_vel_xy_exp: 0.5535
Episode_Reward/track_ang_vel_z_exp: 0.3093
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.0518
     Episode_Reward/dof_torques_l2: -0.0563
         Episode_Reward/dof_acc_l2: -0.1258
     Episode_Reward/action_rate_l2: -0.0591
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9809
Metrics/base_velocity/error_vel_xy: 0.4077
Metrics/base_velocity/error_vel_yaw: 0.2914
      Episode_Termination/time_out: 0.5725
  Episode_Termination/base_contact: 0.4285
--------------------------------------------------------------------------------
                   Total timesteps: 18063360
                    Iteration time: 2.46s
                      Time elapsed: 00:31:09
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 735/1400 [0m                      

                       Computation: 9865 steps/s (collection: 2.234s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 6.0602
                       Mean reward: 10.19
               Mean episode length: 752.31
Episode_Reward/track_lin_vel_xy_exp: 0.4823
Episode_Reward/track_ang_vel_z_exp: 0.2771
       Episode_Reward/lin_vel_z_l2: -0.0330
      Episode_Reward/ang_vel_xy_l2: -0.0426
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.1139
     Episode_Reward/action_rate_l2: -0.0527
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9710
Metrics/base_velocity/error_vel_xy: 0.4003
Metrics/base_velocity/error_vel_yaw: 0.2752
      Episode_Termination/time_out: 0.5754
  Episode_Termination/base_contact: 0.4256
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.49s
                      Time elapsed: 00:31:11
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 736/1400 [0m                      

                       Computation: 9933 steps/s (collection: 2.212s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.0570
                       Mean reward: 9.97
               Mean episode length: 755.98
Episode_Reward/track_lin_vel_xy_exp: 0.5499
Episode_Reward/track_ang_vel_z_exp: 0.3060
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0466
     Episode_Reward/dof_torques_l2: -0.0549
         Episode_Reward/dof_acc_l2: -0.1342
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9758
Metrics/base_velocity/error_vel_xy: 0.3929
Metrics/base_velocity/error_vel_yaw: 0.2843
      Episode_Termination/time_out: 0.5765
  Episode_Termination/base_contact: 0.4244
--------------------------------------------------------------------------------
                   Total timesteps: 18112512
                    Iteration time: 2.47s
                      Time elapsed: 00:31:14
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 737/1400 [0m                      

                       Computation: 9783 steps/s (collection: 2.257s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.0515
                       Mean reward: 9.25
               Mean episode length: 723.89
Episode_Reward/track_lin_vel_xy_exp: 0.4776
Episode_Reward/track_ang_vel_z_exp: 0.2791
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0545
         Episode_Reward/dof_acc_l2: -0.1268
     Episode_Reward/action_rate_l2: -0.0543
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9840
Metrics/base_velocity/error_vel_xy: 0.4153
Metrics/base_velocity/error_vel_yaw: 0.2780
      Episode_Termination/time_out: 0.5754
  Episode_Termination/base_contact: 0.4255
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 2.51s
                      Time elapsed: 00:31:16
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 738/1400 [0m                      

                       Computation: 9911 steps/s (collection: 2.223s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.0552
                       Mean reward: 9.51
               Mean episode length: 737.99
Episode_Reward/track_lin_vel_xy_exp: 0.4754
Episode_Reward/track_ang_vel_z_exp: 0.2811
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0450
     Episode_Reward/dof_torques_l2: -0.0485
         Episode_Reward/dof_acc_l2: -0.1066
     Episode_Reward/action_rate_l2: -0.0523
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9884
Metrics/base_velocity/error_vel_xy: 0.4357
Metrics/base_velocity/error_vel_yaw: 0.2634
      Episode_Termination/time_out: 0.5726
  Episode_Termination/base_contact: 0.4284
--------------------------------------------------------------------------------
                   Total timesteps: 18161664
                    Iteration time: 2.48s
                      Time elapsed: 00:31:19
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 739/1400 [0m                      

                       Computation: 9773 steps/s (collection: 2.261s, learning 0.253s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.0605
                       Mean reward: 9.49
               Mean episode length: 724.89
Episode_Reward/track_lin_vel_xy_exp: 0.5798
Episode_Reward/track_ang_vel_z_exp: 0.3215
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0582
         Episode_Reward/dof_acc_l2: -0.1328
     Episode_Reward/action_rate_l2: -0.0631
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9873
Metrics/base_velocity/error_vel_xy: 0.4249
Metrics/base_velocity/error_vel_yaw: 0.3098
      Episode_Termination/time_out: 0.5710
  Episode_Termination/base_contact: 0.4300
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.51s
                      Time elapsed: 00:31:21
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 740/1400 [0m                      

                       Computation: 9813 steps/s (collection: 2.246s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 6.0595
                       Mean reward: 9.44
               Mean episode length: 739.01
Episode_Reward/track_lin_vel_xy_exp: 0.5203
Episode_Reward/track_ang_vel_z_exp: 0.3109
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0500
     Episode_Reward/dof_torques_l2: -0.0574
         Episode_Reward/dof_acc_l2: -0.1381
     Episode_Reward/action_rate_l2: -0.0589
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9874
Metrics/base_velocity/error_vel_xy: 0.4608
Metrics/base_velocity/error_vel_yaw: 0.2875
      Episode_Termination/time_out: 0.5691
  Episode_Termination/base_contact: 0.4318
--------------------------------------------------------------------------------
                   Total timesteps: 18210816
                    Iteration time: 2.50s
                      Time elapsed: 00:31:24
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 741/1400 [0m                      

                       Computation: 9856 steps/s (collection: 2.238s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.0574
                       Mean reward: 9.88
               Mean episode length: 749.78
Episode_Reward/track_lin_vel_xy_exp: 0.6429
Episode_Reward/track_ang_vel_z_exp: 0.3554
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0529
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1340
     Episode_Reward/action_rate_l2: -0.0663
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9893
Metrics/base_velocity/error_vel_xy: 0.4369
Metrics/base_velocity/error_vel_yaw: 0.3149
      Episode_Termination/time_out: 0.5675
  Episode_Termination/base_contact: 0.4334
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 2.49s
                      Time elapsed: 00:31:26
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 742/1400 [0m                      

                       Computation: 9878 steps/s (collection: 2.231s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.0511
                       Mean reward: 10.18
               Mean episode length: 764.32
Episode_Reward/track_lin_vel_xy_exp: 0.5136
Episode_Reward/track_ang_vel_z_exp: 0.3003
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.0480
     Episode_Reward/dof_torques_l2: -0.0563
         Episode_Reward/dof_acc_l2: -0.1335
     Episode_Reward/action_rate_l2: -0.0577
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0026
Metrics/base_velocity/error_vel_xy: 0.4273
Metrics/base_velocity/error_vel_yaw: 0.2775
      Episode_Termination/time_out: 0.5726
  Episode_Termination/base_contact: 0.4284
--------------------------------------------------------------------------------
                   Total timesteps: 18259968
                    Iteration time: 2.49s
                      Time elapsed: 00:31:29
                               ETA: 00:27:52

################################################################################
                     [1m Learning iteration 743/1400 [0m                      

                       Computation: 9846 steps/s (collection: 2.239s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.0394
                       Mean reward: 9.65
               Mean episode length: 749.82
Episode_Reward/track_lin_vel_xy_exp: 0.5485
Episode_Reward/track_ang_vel_z_exp: 0.3335
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0525
     Episode_Reward/dof_torques_l2: -0.0623
         Episode_Reward/dof_acc_l2: -0.1388
     Episode_Reward/action_rate_l2: -0.0636
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0061
Metrics/base_velocity/error_vel_xy: 0.5193
Metrics/base_velocity/error_vel_yaw: 0.3144
      Episode_Termination/time_out: 0.5726
  Episode_Termination/base_contact: 0.4283
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.50s
                      Time elapsed: 00:31:31
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 744/1400 [0m                      

                       Computation: 9754 steps/s (collection: 2.264s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.0272
                       Mean reward: 9.02
               Mean episode length: 729.36
Episode_Reward/track_lin_vel_xy_exp: 0.5060
Episode_Reward/track_ang_vel_z_exp: 0.2886
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.0517
         Episode_Reward/dof_acc_l2: -0.1239
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0062
Metrics/base_velocity/error_vel_xy: 0.3998
Metrics/base_velocity/error_vel_yaw: 0.2693
      Episode_Termination/time_out: 0.5659
  Episode_Termination/base_contact: 0.4351
--------------------------------------------------------------------------------
                   Total timesteps: 18309120
                    Iteration time: 2.52s
                      Time elapsed: 00:31:34
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 745/1400 [0m                      

                       Computation: 9864 steps/s (collection: 2.231s, learning 0.260s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0115
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 6.0212
                       Mean reward: 8.89
               Mean episode length: 724.34
Episode_Reward/track_lin_vel_xy_exp: 0.5390
Episode_Reward/track_ang_vel_z_exp: 0.3153
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.0519
     Episode_Reward/dof_torques_l2: -0.0595
         Episode_Reward/dof_acc_l2: -0.1440
     Episode_Reward/action_rate_l2: -0.0617
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0053
Metrics/base_velocity/error_vel_xy: 0.4805
Metrics/base_velocity/error_vel_yaw: 0.3100
      Episode_Termination/time_out: 0.5618
  Episode_Termination/base_contact: 0.4392
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 2.49s
                      Time elapsed: 00:31:36
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 746/1400 [0m                      

                       Computation: 9935 steps/s (collection: 2.218s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 6.0115
                       Mean reward: 9.14
               Mean episode length: 729.41
Episode_Reward/track_lin_vel_xy_exp: 0.6042
Episode_Reward/track_ang_vel_z_exp: 0.3425
       Episode_Reward/lin_vel_z_l2: -0.0412
      Episode_Reward/ang_vel_xy_l2: -0.0524
     Episode_Reward/dof_torques_l2: -0.0627
         Episode_Reward/dof_acc_l2: -0.1498
     Episode_Reward/action_rate_l2: -0.0667
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0024
Metrics/base_velocity/error_vel_xy: 0.4825
Metrics/base_velocity/error_vel_yaw: 0.3374
      Episode_Termination/time_out: 0.5615
  Episode_Termination/base_contact: 0.4389
--------------------------------------------------------------------------------
                   Total timesteps: 18358272
                    Iteration time: 2.47s
                      Time elapsed: 00:31:39
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 747/1400 [0m                      

                       Computation: 9873 steps/s (collection: 2.232s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 6.0128
                       Mean reward: 10.20
               Mean episode length: 748.16
Episode_Reward/track_lin_vel_xy_exp: 0.5209
Episode_Reward/track_ang_vel_z_exp: 0.2931
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0436
     Episode_Reward/dof_torques_l2: -0.0541
         Episode_Reward/dof_acc_l2: -0.1169
     Episode_Reward/action_rate_l2: -0.0552
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0084
Metrics/base_velocity/error_vel_xy: 0.3716
Metrics/base_velocity/error_vel_yaw: 0.2547
      Episode_Termination/time_out: 0.5591
  Episode_Termination/base_contact: 0.4409
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.49s
                      Time elapsed: 00:31:41
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 748/1400 [0m                      

                       Computation: 9862 steps/s (collection: 2.236s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.0148
                       Mean reward: 9.71
               Mean episode length: 729.49
Episode_Reward/track_lin_vel_xy_exp: 0.4373
Episode_Reward/track_ang_vel_z_exp: 0.2581
       Episode_Reward/lin_vel_z_l2: -0.0342
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0491
         Episode_Reward/dof_acc_l2: -0.1162
     Episode_Reward/action_rate_l2: -0.0506
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0070
Metrics/base_velocity/error_vel_xy: 0.4109
Metrics/base_velocity/error_vel_yaw: 0.2736
      Episode_Termination/time_out: 0.5594
  Episode_Termination/base_contact: 0.4406
--------------------------------------------------------------------------------
                   Total timesteps: 18407424
                    Iteration time: 2.49s
                      Time elapsed: 00:31:44
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 749/1400 [0m                      

                       Computation: 9869 steps/s (collection: 2.233s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.0073
                       Mean reward: 8.73
               Mean episode length: 688.80
Episode_Reward/track_lin_vel_xy_exp: 0.4111
Episode_Reward/track_ang_vel_z_exp: 0.2527
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0437
     Episode_Reward/dof_torques_l2: -0.0526
         Episode_Reward/dof_acc_l2: -0.1145
     Episode_Reward/action_rate_l2: -0.0494
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0196
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0048
Metrics/base_velocity/error_vel_xy: 0.4198
Metrics/base_velocity/error_vel_yaw: 0.2594
      Episode_Termination/time_out: 0.5584
  Episode_Termination/base_contact: 0.4416
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 2.49s
                      Time elapsed: 00:31:46
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 750/1400 [0m                      

                       Computation: 9907 steps/s (collection: 2.225s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.0000
                       Mean reward: 8.87
               Mean episode length: 713.90
Episode_Reward/track_lin_vel_xy_exp: 0.5531
Episode_Reward/track_ang_vel_z_exp: 0.3222
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.0555
     Episode_Reward/dof_torques_l2: -0.0661
         Episode_Reward/dof_acc_l2: -0.1544
     Episode_Reward/action_rate_l2: -0.0665
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0026
Metrics/base_velocity/error_vel_xy: 0.5435
Metrics/base_velocity/error_vel_yaw: 0.3784
      Episode_Termination/time_out: 0.5539
  Episode_Termination/base_contact: 0.4461
--------------------------------------------------------------------------------
                   Total timesteps: 18456576
                    Iteration time: 2.48s
                      Time elapsed: 00:31:49
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 751/1400 [0m                      

                       Computation: 9934 steps/s (collection: 2.217s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.9932
                       Mean reward: 9.22
               Mean episode length: 736.96
Episode_Reward/track_lin_vel_xy_exp: 0.5872
Episode_Reward/track_ang_vel_z_exp: 0.3381
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.0530
     Episode_Reward/dof_torques_l2: -0.0631
         Episode_Reward/dof_acc_l2: -0.1494
     Episode_Reward/action_rate_l2: -0.0649
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0157
Metrics/base_velocity/error_vel_xy: 0.4897
Metrics/base_velocity/error_vel_yaw: 0.3280
      Episode_Termination/time_out: 0.5604
  Episode_Termination/base_contact: 0.4396
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.47s
                      Time elapsed: 00:31:51
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 752/1400 [0m                      

                       Computation: 9821 steps/s (collection: 2.247s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 6.0017
                       Mean reward: 9.87
               Mean episode length: 772.76
Episode_Reward/track_lin_vel_xy_exp: 0.6104
Episode_Reward/track_ang_vel_z_exp: 0.3496
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0522
     Episode_Reward/dof_torques_l2: -0.0637
         Episode_Reward/dof_acc_l2: -0.1452
     Episode_Reward/action_rate_l2: -0.0656
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0204
Metrics/base_velocity/error_vel_xy: 0.4617
Metrics/base_velocity/error_vel_yaw: 0.3117
      Episode_Termination/time_out: 0.5668
  Episode_Termination/base_contact: 0.4332
--------------------------------------------------------------------------------
                   Total timesteps: 18505728
                    Iteration time: 2.50s
                      Time elapsed: 00:31:54
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 753/1400 [0m                      

                       Computation: 9786 steps/s (collection: 2.255s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 5.9955
                       Mean reward: 10.83
               Mean episode length: 810.43
Episode_Reward/track_lin_vel_xy_exp: 0.6405
Episode_Reward/track_ang_vel_z_exp: 0.3581
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.0558
     Episode_Reward/dof_torques_l2: -0.0635
         Episode_Reward/dof_acc_l2: -0.1523
     Episode_Reward/action_rate_l2: -0.0671
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0340
Metrics/base_velocity/error_vel_xy: 0.4498
Metrics/base_velocity/error_vel_yaw: 0.3112
      Episode_Termination/time_out: 0.5728
  Episode_Termination/base_contact: 0.4272
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 2.51s
                      Time elapsed: 00:31:56
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 754/1400 [0m                      

                       Computation: 9742 steps/s (collection: 2.267s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 5.9886
                       Mean reward: 11.12
               Mean episode length: 823.37
Episode_Reward/track_lin_vel_xy_exp: 0.5846
Episode_Reward/track_ang_vel_z_exp: 0.3413
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0532
     Episode_Reward/dof_torques_l2: -0.0638
         Episode_Reward/dof_acc_l2: -0.1401
     Episode_Reward/action_rate_l2: -0.0651
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0088
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0488
Metrics/base_velocity/error_vel_xy: 0.4993
Metrics/base_velocity/error_vel_yaw: 0.3254
      Episode_Termination/time_out: 0.5741
  Episode_Termination/base_contact: 0.4259
--------------------------------------------------------------------------------
                   Total timesteps: 18554880
                    Iteration time: 2.52s
                      Time elapsed: 00:31:59
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 755/1400 [0m                      

                       Computation: 9738 steps/s (collection: 2.268s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 5.9871
                       Mean reward: 10.59
               Mean episode length: 784.82
Episode_Reward/track_lin_vel_xy_exp: 0.5430
Episode_Reward/track_ang_vel_z_exp: 0.3107
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0573
         Episode_Reward/dof_acc_l2: -0.1268
     Episode_Reward/action_rate_l2: -0.0596
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0557
Metrics/base_velocity/error_vel_xy: 0.4367
Metrics/base_velocity/error_vel_yaw: 0.3003
      Episode_Termination/time_out: 0.5758
  Episode_Termination/base_contact: 0.4242
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.52s
                      Time elapsed: 00:32:01
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 756/1400 [0m                      

                       Computation: 9713 steps/s (collection: 2.274s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.9931
                       Mean reward: 10.25
               Mean episode length: 765.19
Episode_Reward/track_lin_vel_xy_exp: 0.5241
Episode_Reward/track_ang_vel_z_exp: 0.2997
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.0604
         Episode_Reward/dof_acc_l2: -0.1390
     Episode_Reward/action_rate_l2: -0.0582
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0636
Metrics/base_velocity/error_vel_xy: 0.4451
Metrics/base_velocity/error_vel_yaw: 0.3046
      Episode_Termination/time_out: 0.5780
  Episode_Termination/base_contact: 0.4220
--------------------------------------------------------------------------------
                   Total timesteps: 18604032
                    Iteration time: 2.53s
                      Time elapsed: 00:32:04
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 757/1400 [0m                      

                       Computation: 9836 steps/s (collection: 2.241s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.9934
                       Mean reward: 9.77
               Mean episode length: 754.68
Episode_Reward/track_lin_vel_xy_exp: 0.5451
Episode_Reward/track_ang_vel_z_exp: 0.3167
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.0513
     Episode_Reward/dof_torques_l2: -0.0634
         Episode_Reward/dof_acc_l2: -0.1422
     Episode_Reward/action_rate_l2: -0.0621
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0076
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0628
Metrics/base_velocity/error_vel_xy: 0.4759
Metrics/base_velocity/error_vel_yaw: 0.3227
      Episode_Termination/time_out: 0.5807
  Episode_Termination/base_contact: 0.4193
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 2.50s
                      Time elapsed: 00:32:06
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 758/1400 [0m                      

                       Computation: 9852 steps/s (collection: 2.232s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 6.0029
                       Mean reward: 9.56
               Mean episode length: 736.11
Episode_Reward/track_lin_vel_xy_exp: 0.5028
Episode_Reward/track_ang_vel_z_exp: 0.2945
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0466
     Episode_Reward/dof_torques_l2: -0.0558
         Episode_Reward/dof_acc_l2: -0.1301
     Episode_Reward/action_rate_l2: -0.0556
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0588
Metrics/base_velocity/error_vel_xy: 0.4395
Metrics/base_velocity/error_vel_yaw: 0.2754
      Episode_Termination/time_out: 0.5797
  Episode_Termination/base_contact: 0.4203
--------------------------------------------------------------------------------
                   Total timesteps: 18653184
                    Iteration time: 2.49s
                      Time elapsed: 00:32:09
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 759/1400 [0m                      

                       Computation: 9791 steps/s (collection: 2.253s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 6.0097
                       Mean reward: 9.64
               Mean episode length: 738.00
Episode_Reward/track_lin_vel_xy_exp: 0.5044
Episode_Reward/track_ang_vel_z_exp: 0.2902
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0457
     Episode_Reward/dof_torques_l2: -0.0534
         Episode_Reward/dof_acc_l2: -0.1305
     Episode_Reward/action_rate_l2: -0.0545
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0577
Metrics/base_velocity/error_vel_xy: 0.3871
Metrics/base_velocity/error_vel_yaw: 0.2546
      Episode_Termination/time_out: 0.5767
  Episode_Termination/base_contact: 0.4233
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.51s
                      Time elapsed: 00:32:11
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 760/1400 [0m                      

                       Computation: 9726 steps/s (collection: 2.273s, learning 0.253s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.0078
                       Mean reward: 9.12
               Mean episode length: 711.12
Episode_Reward/track_lin_vel_xy_exp: 0.5165
Episode_Reward/track_ang_vel_z_exp: 0.3041
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0507
     Episode_Reward/dof_torques_l2: -0.0593
         Episode_Reward/dof_acc_l2: -0.1414
     Episode_Reward/action_rate_l2: -0.0596
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0565
Metrics/base_velocity/error_vel_xy: 0.4727
Metrics/base_velocity/error_vel_yaw: 0.3147
      Episode_Termination/time_out: 0.5733
  Episode_Termination/base_contact: 0.4267
--------------------------------------------------------------------------------
                   Total timesteps: 18702336
                    Iteration time: 2.53s
                      Time elapsed: 00:32:14
                               ETA: 00:27:06

################################################################################
                     [1m Learning iteration 761/1400 [0m                      

                       Computation: 9878 steps/s (collection: 2.225s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.0043
                       Mean reward: 9.23
               Mean episode length: 723.58
Episode_Reward/track_lin_vel_xy_exp: 0.5969
Episode_Reward/track_ang_vel_z_exp: 0.3367
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.1450
     Episode_Reward/action_rate_l2: -0.0646
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0626
Metrics/base_velocity/error_vel_xy: 0.4542
Metrics/base_velocity/error_vel_yaw: 0.3160
      Episode_Termination/time_out: 0.5705
  Episode_Termination/base_contact: 0.4295
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 2.49s
                      Time elapsed: 00:32:16
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 762/1400 [0m                      

                       Computation: 9771 steps/s (collection: 2.257s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.9933
                       Mean reward: 9.20
               Mean episode length: 741.55
Episode_Reward/track_lin_vel_xy_exp: 0.5699
Episode_Reward/track_ang_vel_z_exp: 0.3317
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.0552
     Episode_Reward/dof_torques_l2: -0.0677
         Episode_Reward/dof_acc_l2: -0.1465
     Episode_Reward/action_rate_l2: -0.0649
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0697
Metrics/base_velocity/error_vel_xy: 0.5113
Metrics/base_velocity/error_vel_yaw: 0.3416
      Episode_Termination/time_out: 0.5739
  Episode_Termination/base_contact: 0.4261
--------------------------------------------------------------------------------
                   Total timesteps: 18751488
                    Iteration time: 2.51s
                      Time elapsed: 00:32:19
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 763/1400 [0m                      

                       Computation: 9776 steps/s (collection: 2.251s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.9877
                       Mean reward: 9.38
               Mean episode length: 743.51
Episode_Reward/track_lin_vel_xy_exp: 0.5358
Episode_Reward/track_ang_vel_z_exp: 0.3058
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0475
     Episode_Reward/dof_torques_l2: -0.0580
         Episode_Reward/dof_acc_l2: -0.1336
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0779
Metrics/base_velocity/error_vel_xy: 0.4218
Metrics/base_velocity/error_vel_yaw: 0.2854
      Episode_Termination/time_out: 0.5758
  Episode_Termination/base_contact: 0.4242
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.51s
                      Time elapsed: 00:32:21
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 764/1400 [0m                      

                       Computation: 9749 steps/s (collection: 2.263s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 5.9849
                       Mean reward: 9.09
               Mean episode length: 726.17
Episode_Reward/track_lin_vel_xy_exp: 0.5079
Episode_Reward/track_ang_vel_z_exp: 0.2985
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.0513
     Episode_Reward/dof_torques_l2: -0.0570
         Episode_Reward/dof_acc_l2: -0.1392
     Episode_Reward/action_rate_l2: -0.0568
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0766
Metrics/base_velocity/error_vel_xy: 0.4533
Metrics/base_velocity/error_vel_yaw: 0.2985
      Episode_Termination/time_out: 0.5729
  Episode_Termination/base_contact: 0.4271
--------------------------------------------------------------------------------
                   Total timesteps: 18800640
                    Iteration time: 2.52s
                      Time elapsed: 00:32:24
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 765/1400 [0m                      

                       Computation: 9869 steps/s (collection: 2.233s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 5.9842
                       Mean reward: 9.13
               Mean episode length: 726.61
Episode_Reward/track_lin_vel_xy_exp: 0.6002
Episode_Reward/track_ang_vel_z_exp: 0.3439
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0522
     Episode_Reward/dof_torques_l2: -0.0633
         Episode_Reward/dof_acc_l2: -0.1408
     Episode_Reward/action_rate_l2: -0.0647
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0850
Metrics/base_velocity/error_vel_xy: 0.4829
Metrics/base_velocity/error_vel_yaw: 0.3250
      Episode_Termination/time_out: 0.5708
  Episode_Termination/base_contact: 0.4292
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 2.49s
                      Time elapsed: 00:32:26
                               ETA: 00:26:53

################################################################################
                     [1m Learning iteration 766/1400 [0m                      

                       Computation: 9799 steps/s (collection: 2.247s, learning 0.261s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.9892
                       Mean reward: 9.58
               Mean episode length: 744.34
Episode_Reward/track_lin_vel_xy_exp: 0.5804
Episode_Reward/track_ang_vel_z_exp: 0.3313
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1293
     Episode_Reward/action_rate_l2: -0.0617
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0839
Metrics/base_velocity/error_vel_xy: 0.4405
Metrics/base_velocity/error_vel_yaw: 0.2939
      Episode_Termination/time_out: 0.5718
  Episode_Termination/base_contact: 0.4282
--------------------------------------------------------------------------------
                   Total timesteps: 18849792
                    Iteration time: 2.51s
                      Time elapsed: 00:32:29
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 767/1400 [0m                      

                       Computation: 9825 steps/s (collection: 2.243s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.9864
                       Mean reward: 9.76
               Mean episode length: 770.00
Episode_Reward/track_lin_vel_xy_exp: 0.5036
Episode_Reward/track_ang_vel_z_exp: 0.3107
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.0544
     Episode_Reward/dof_torques_l2: -0.0668
         Episode_Reward/dof_acc_l2: -0.1516
     Episode_Reward/action_rate_l2: -0.0617
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0820
Metrics/base_velocity/error_vel_xy: 0.5337
Metrics/base_velocity/error_vel_yaw: 0.3342
      Episode_Termination/time_out: 0.5728
  Episode_Termination/base_contact: 0.4272
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.50s
                      Time elapsed: 00:32:31
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 768/1400 [0m                      

                       Computation: 9774 steps/s (collection: 2.259s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 5.9777
                       Mean reward: 9.29
               Mean episode length: 738.87
Episode_Reward/track_lin_vel_xy_exp: 0.5159
Episode_Reward/track_ang_vel_z_exp: 0.2966
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1276
     Episode_Reward/action_rate_l2: -0.0566
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0857
Metrics/base_velocity/error_vel_xy: 0.4207
Metrics/base_velocity/error_vel_yaw: 0.2865
      Episode_Termination/time_out: 0.5715
  Episode_Termination/base_contact: 0.4285
--------------------------------------------------------------------------------
                   Total timesteps: 18898944
                    Iteration time: 2.51s
                      Time elapsed: 00:32:34
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 769/1400 [0m                      

                       Computation: 9748 steps/s (collection: 2.265s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 5.9830
                       Mean reward: 9.29
               Mean episode length: 731.40
Episode_Reward/track_lin_vel_xy_exp: 0.5658
Episode_Reward/track_ang_vel_z_exp: 0.3232
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0502
     Episode_Reward/dof_torques_l2: -0.0610
         Episode_Reward/dof_acc_l2: -0.1348
     Episode_Reward/action_rate_l2: -0.0605
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0109
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0891
Metrics/base_velocity/error_vel_xy: 0.4419
Metrics/base_velocity/error_vel_yaw: 0.2948
      Episode_Termination/time_out: 0.5632
  Episode_Termination/base_contact: 0.4374
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 2.52s
                      Time elapsed: 00:32:36
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 770/1400 [0m                      

                       Computation: 9750 steps/s (collection: 2.265s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 5.9783
                       Mean reward: 10.02
               Mean episode length: 736.84
Episode_Reward/track_lin_vel_xy_exp: 0.5717
Episode_Reward/track_ang_vel_z_exp: 0.3205
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0601
         Episode_Reward/dof_acc_l2: -0.1326
     Episode_Reward/action_rate_l2: -0.0606
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0897
Metrics/base_velocity/error_vel_xy: 0.4250
Metrics/base_velocity/error_vel_yaw: 0.2957
      Episode_Termination/time_out: 0.5660
  Episode_Termination/base_contact: 0.4350
--------------------------------------------------------------------------------
                   Total timesteps: 18948096
                    Iteration time: 2.52s
                      Time elapsed: 00:32:39
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 771/1400 [0m                      

                       Computation: 9730 steps/s (collection: 2.261s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0187
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.9815
                       Mean reward: 10.59
               Mean episode length: 758.72
Episode_Reward/track_lin_vel_xy_exp: 0.5431
Episode_Reward/track_ang_vel_z_exp: 0.3151
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0585
         Episode_Reward/dof_acc_l2: -0.1337
     Episode_Reward/action_rate_l2: -0.0610
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0910
Metrics/base_velocity/error_vel_xy: 0.4753
Metrics/base_velocity/error_vel_yaw: 0.3147
      Episode_Termination/time_out: 0.5619
  Episode_Termination/base_contact: 0.4390
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.53s
                      Time elapsed: 00:32:41
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 772/1400 [0m                      

                       Computation: 9736 steps/s (collection: 2.269s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.9938
                       Mean reward: 9.42
               Mean episode length: 716.40
Episode_Reward/track_lin_vel_xy_exp: 0.4780
Episode_Reward/track_ang_vel_z_exp: 0.2850
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0540
         Episode_Reward/dof_acc_l2: -0.1281
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0106
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0936
Metrics/base_velocity/error_vel_xy: 0.4532
Metrics/base_velocity/error_vel_yaw: 0.2871
      Episode_Termination/time_out: 0.5619
  Episode_Termination/base_contact: 0.4390
--------------------------------------------------------------------------------
                   Total timesteps: 18997248
                    Iteration time: 2.52s
                      Time elapsed: 00:32:44
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 773/1400 [0m                      

                       Computation: 9691 steps/s (collection: 2.280s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 5.9885
                       Mean reward: 9.19
               Mean episode length: 726.19
Episode_Reward/track_lin_vel_xy_exp: 0.4832
Episode_Reward/track_ang_vel_z_exp: 0.2838
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0591
         Episode_Reward/dof_acc_l2: -0.1341
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0868
Metrics/base_velocity/error_vel_xy: 0.4474
Metrics/base_velocity/error_vel_yaw: 0.3021
      Episode_Termination/time_out: 0.5575
  Episode_Termination/base_contact: 0.4435
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 2.54s
                      Time elapsed: 00:32:46
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 774/1400 [0m                      

                       Computation: 9776 steps/s (collection: 2.260s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.9912
                       Mean reward: 9.32
               Mean episode length: 719.83
Episode_Reward/track_lin_vel_xy_exp: 0.5452
Episode_Reward/track_ang_vel_z_exp: 0.3103
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0472
     Episode_Reward/dof_torques_l2: -0.0586
         Episode_Reward/dof_acc_l2: -0.1342
     Episode_Reward/action_rate_l2: -0.0588
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0953
Metrics/base_velocity/error_vel_xy: 0.4311
Metrics/base_velocity/error_vel_yaw: 0.2951
      Episode_Termination/time_out: 0.5615
  Episode_Termination/base_contact: 0.4395
--------------------------------------------------------------------------------
                   Total timesteps: 19046400
                    Iteration time: 2.51s
                      Time elapsed: 00:32:49
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 775/1400 [0m                      

                       Computation: 9716 steps/s (collection: 2.273s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.9892
                       Mean reward: 10.04
               Mean episode length: 761.71
Episode_Reward/track_lin_vel_xy_exp: 0.5955
Episode_Reward/track_ang_vel_z_exp: 0.3350
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.0536
     Episode_Reward/dof_torques_l2: -0.0629
         Episode_Reward/dof_acc_l2: -0.1467
     Episode_Reward/action_rate_l2: -0.0642
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1118
Metrics/base_velocity/error_vel_xy: 0.4659
Metrics/base_velocity/error_vel_yaw: 0.3233
      Episode_Termination/time_out: 0.5632
  Episode_Termination/base_contact: 0.4377
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.53s
                      Time elapsed: 00:32:51
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 776/1400 [0m                      

                       Computation: 9781 steps/s (collection: 2.256s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 5.9835
                       Mean reward: 11.40
               Mean episode length: 827.33
Episode_Reward/track_lin_vel_xy_exp: 0.6241
Episode_Reward/track_ang_vel_z_exp: 0.3650
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.0546
     Episode_Reward/dof_torques_l2: -0.0680
         Episode_Reward/dof_acc_l2: -0.1584
     Episode_Reward/action_rate_l2: -0.0699
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1310
Metrics/base_velocity/error_vel_xy: 0.5485
Metrics/base_velocity/error_vel_yaw: 0.3575
      Episode_Termination/time_out: 0.5637
  Episode_Termination/base_contact: 0.4373
--------------------------------------------------------------------------------
                   Total timesteps: 19095552
                    Iteration time: 2.51s
                      Time elapsed: 00:32:54
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 777/1400 [0m                      

                       Computation: 9767 steps/s (collection: 2.261s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 5.9955
                       Mean reward: 10.87
               Mean episode length: 790.48
Episode_Reward/track_lin_vel_xy_exp: 0.4671
Episode_Reward/track_ang_vel_z_exp: 0.2555
       Episode_Reward/lin_vel_z_l2: -0.0318
      Episode_Reward/ang_vel_xy_l2: -0.0372
     Episode_Reward/dof_torques_l2: -0.0465
         Episode_Reward/dof_acc_l2: -0.1040
     Episode_Reward/action_rate_l2: -0.0484
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1396
Metrics/base_velocity/error_vel_xy: 0.3322
Metrics/base_velocity/error_vel_yaw: 0.2469
      Episode_Termination/time_out: 0.5626
  Episode_Termination/base_contact: 0.4384
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 2.52s
                      Time elapsed: 00:32:56
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 778/1400 [0m                      

                       Computation: 9612 steps/s (collection: 2.299s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.0070
                       Mean reward: 10.00
               Mean episode length: 778.12
Episode_Reward/track_lin_vel_xy_exp: 0.4648
Episode_Reward/track_ang_vel_z_exp: 0.2739
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0466
     Episode_Reward/dof_torques_l2: -0.0565
         Episode_Reward/dof_acc_l2: -0.1279
     Episode_Reward/action_rate_l2: -0.0540
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0107
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1327
Metrics/base_velocity/error_vel_xy: 0.4725
Metrics/base_velocity/error_vel_yaw: 0.3099
      Episode_Termination/time_out: 0.5616
  Episode_Termination/base_contact: 0.4384
--------------------------------------------------------------------------------
                   Total timesteps: 19144704
                    Iteration time: 2.56s
                      Time elapsed: 00:32:59
                               ETA: 00:26:20

################################################################################
                     [1m Learning iteration 779/1400 [0m                      

                       Computation: 9597 steps/s (collection: 2.303s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.0149
                       Mean reward: 9.22
               Mean episode length: 725.02
Episode_Reward/track_lin_vel_xy_exp: 0.5203
Episode_Reward/track_ang_vel_z_exp: 0.2937
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0438
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1153
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1278
Metrics/base_velocity/error_vel_xy: 0.4118
Metrics/base_velocity/error_vel_yaw: 0.2854
      Episode_Termination/time_out: 0.5599
  Episode_Termination/base_contact: 0.4401
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.56s
                      Time elapsed: 00:33:02
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 780/1400 [0m                      

                       Computation: 9549 steps/s (collection: 2.311s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.0125
                       Mean reward: 8.36
               Mean episode length: 704.48
Episode_Reward/track_lin_vel_xy_exp: 0.4375
Episode_Reward/track_ang_vel_z_exp: 0.2583
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0503
         Episode_Reward/dof_acc_l2: -0.1175
     Episode_Reward/action_rate_l2: -0.0510
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1285
Metrics/base_velocity/error_vel_xy: 0.4429
Metrics/base_velocity/error_vel_yaw: 0.3045
      Episode_Termination/time_out: 0.5553
  Episode_Termination/base_contact: 0.4447
--------------------------------------------------------------------------------
                   Total timesteps: 19193856
                    Iteration time: 2.57s
                      Time elapsed: 00:33:04
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 781/1400 [0m                      

                       Computation: 9769 steps/s (collection: 2.259s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.0147
                       Mean reward: 8.83
               Mean episode length: 717.13
Episode_Reward/track_lin_vel_xy_exp: 0.5344
Episode_Reward/track_ang_vel_z_exp: 0.3061
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.0502
     Episode_Reward/dof_torques_l2: -0.0618
         Episode_Reward/dof_acc_l2: -0.1439
     Episode_Reward/action_rate_l2: -0.0594
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1351
Metrics/base_velocity/error_vel_xy: 0.4644
Metrics/base_velocity/error_vel_yaw: 0.3181
      Episode_Termination/time_out: 0.5531
  Episode_Termination/base_contact: 0.4469
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 2.52s
                      Time elapsed: 00:33:07
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 782/1400 [0m                      

                       Computation: 9770 steps/s (collection: 2.253s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 6.0204
                       Mean reward: 9.07
               Mean episode length: 726.33
Episode_Reward/track_lin_vel_xy_exp: 0.5103
Episode_Reward/track_ang_vel_z_exp: 0.2895
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0563
         Episode_Reward/dof_acc_l2: -0.1253
     Episode_Reward/action_rate_l2: -0.0557
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1425
Metrics/base_velocity/error_vel_xy: 0.4269
Metrics/base_velocity/error_vel_yaw: 0.3028
      Episode_Termination/time_out: 0.5532
  Episode_Termination/base_contact: 0.4468
--------------------------------------------------------------------------------
                   Total timesteps: 19243008
                    Iteration time: 2.52s
                      Time elapsed: 00:33:09
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 783/1400 [0m                      

                       Computation: 9949 steps/s (collection: 2.208s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.0239
                       Mean reward: 9.40
               Mean episode length: 735.36
Episode_Reward/track_lin_vel_xy_exp: 0.4778
Episode_Reward/track_ang_vel_z_exp: 0.2876
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0449
     Episode_Reward/dof_torques_l2: -0.0562
         Episode_Reward/dof_acc_l2: -0.1198
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0203
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1325
Metrics/base_velocity/error_vel_xy: 0.4491
Metrics/base_velocity/error_vel_yaw: 0.2820
      Episode_Termination/time_out: 0.5475
  Episode_Termination/base_contact: 0.4525
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.47s
                      Time elapsed: 00:33:12
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 784/1400 [0m                      

                       Computation: 9869 steps/s (collection: 2.234s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 6.0212
                       Mean reward: 9.29
               Mean episode length: 700.91
Episode_Reward/track_lin_vel_xy_exp: 0.5588
Episode_Reward/track_ang_vel_z_exp: 0.3115
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0544
         Episode_Reward/dof_acc_l2: -0.1272
     Episode_Reward/action_rate_l2: -0.0584
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1335
Metrics/base_velocity/error_vel_xy: 0.4027
Metrics/base_velocity/error_vel_yaw: 0.2828
      Episode_Termination/time_out: 0.5480
  Episode_Termination/base_contact: 0.4520
--------------------------------------------------------------------------------
                   Total timesteps: 19292160
                    Iteration time: 2.49s
                      Time elapsed: 00:33:14
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 785/1400 [0m                      

                       Computation: 9873 steps/s (collection: 2.234s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 6.0228
                       Mean reward: 9.99
               Mean episode length: 752.93
Episode_Reward/track_lin_vel_xy_exp: 0.5433
Episode_Reward/track_ang_vel_z_exp: 0.3106
       Episode_Reward/lin_vel_z_l2: -0.0389
      Episode_Reward/ang_vel_xy_l2: -0.0484
     Episode_Reward/dof_torques_l2: -0.0604
         Episode_Reward/dof_acc_l2: -0.1371
     Episode_Reward/action_rate_l2: -0.0588
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1442
Metrics/base_velocity/error_vel_xy: 0.4288
Metrics/base_velocity/error_vel_yaw: 0.2965
      Episode_Termination/time_out: 0.5561
  Episode_Termination/base_contact: 0.4439
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 2.49s
                      Time elapsed: 00:33:17
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 786/1400 [0m                      

                       Computation: 9842 steps/s (collection: 2.234s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.0257
                       Mean reward: 9.66
               Mean episode length: 731.73
Episode_Reward/track_lin_vel_xy_exp: 0.4243
Episode_Reward/track_ang_vel_z_exp: 0.2487
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0509
         Episode_Reward/dof_acc_l2: -0.1206
     Episode_Reward/action_rate_l2: -0.0491
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1458
Metrics/base_velocity/error_vel_xy: 0.4022
Metrics/base_velocity/error_vel_yaw: 0.2717
      Episode_Termination/time_out: 0.5592
  Episode_Termination/base_contact: 0.4408
--------------------------------------------------------------------------------
                   Total timesteps: 19341312
                    Iteration time: 2.50s
                      Time elapsed: 00:33:19
                               ETA: 00:26:00

################################################################################
                     [1m Learning iteration 787/1400 [0m                      

                       Computation: 9827 steps/s (collection: 2.245s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.0145
                       Mean reward: 9.48
               Mean episode length: 739.31
Episode_Reward/track_lin_vel_xy_exp: 0.5488
Episode_Reward/track_ang_vel_z_exp: 0.3037
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0573
         Episode_Reward/dof_acc_l2: -0.1328
     Episode_Reward/action_rate_l2: -0.0578
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1468
Metrics/base_velocity/error_vel_xy: 0.3980
Metrics/base_velocity/error_vel_yaw: 0.2941
      Episode_Termination/time_out: 0.5562
  Episode_Termination/base_contact: 0.4438
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.50s
                      Time elapsed: 00:33:22
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 788/1400 [0m                      

                       Computation: 9722 steps/s (collection: 2.271s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 6.0275
                       Mean reward: 8.44
               Mean episode length: 657.85
Episode_Reward/track_lin_vel_xy_exp: 0.3684
Episode_Reward/track_ang_vel_z_exp: 0.2161
       Episode_Reward/lin_vel_z_l2: -0.0321
      Episode_Reward/ang_vel_xy_l2: -0.0376
     Episode_Reward/dof_torques_l2: -0.0479
         Episode_Reward/dof_acc_l2: -0.1066
     Episode_Reward/action_rate_l2: -0.0439
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1522
Metrics/base_velocity/error_vel_xy: 0.3948
Metrics/base_velocity/error_vel_yaw: 0.2630
      Episode_Termination/time_out: 0.5532
  Episode_Termination/base_contact: 0.4468
--------------------------------------------------------------------------------
                   Total timesteps: 19390464
                    Iteration time: 2.53s
                      Time elapsed: 00:33:24
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 789/1400 [0m                      

                       Computation: 9728 steps/s (collection: 2.270s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.0269
                       Mean reward: 8.44
               Mean episode length: 652.14
Episode_Reward/track_lin_vel_xy_exp: 0.4742
Episode_Reward/track_ang_vel_z_exp: 0.2764
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0437
     Episode_Reward/dof_torques_l2: -0.0516
         Episode_Reward/dof_acc_l2: -0.1167
     Episode_Reward/action_rate_l2: -0.0528
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1504
Metrics/base_velocity/error_vel_xy: 0.4231
Metrics/base_velocity/error_vel_yaw: 0.2831
      Episode_Termination/time_out: 0.5511
  Episode_Termination/base_contact: 0.4489
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 2.53s
                      Time elapsed: 00:33:27
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 790/1400 [0m                      

                       Computation: 9909 steps/s (collection: 2.224s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.0410
                       Mean reward: 8.52
               Mean episode length: 666.26
Episode_Reward/track_lin_vel_xy_exp: 0.6047
Episode_Reward/track_ang_vel_z_exp: 0.3444
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0517
     Episode_Reward/dof_torques_l2: -0.0661
         Episode_Reward/dof_acc_l2: -0.1461
     Episode_Reward/action_rate_l2: -0.0658
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1509
Metrics/base_velocity/error_vel_xy: 0.4688
Metrics/base_velocity/error_vel_yaw: 0.3207
      Episode_Termination/time_out: 0.5508
  Episode_Termination/base_contact: 0.4492
--------------------------------------------------------------------------------
                   Total timesteps: 19439616
                    Iteration time: 2.48s
                      Time elapsed: 00:33:29
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 791/1400 [0m                      

                       Computation: 9772 steps/s (collection: 2.261s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 6.0414
                       Mean reward: 9.08
               Mean episode length: 713.59
Episode_Reward/track_lin_vel_xy_exp: 0.5244
Episode_Reward/track_ang_vel_z_exp: 0.3077
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.1409
     Episode_Reward/action_rate_l2: -0.0593
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0083
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1607
Metrics/base_velocity/error_vel_xy: 0.4674
Metrics/base_velocity/error_vel_yaw: 0.3072
      Episode_Termination/time_out: 0.5550
  Episode_Termination/base_contact: 0.4450
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.51s
                      Time elapsed: 00:33:32
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 792/1400 [0m                      

                       Computation: 9797 steps/s (collection: 2.252s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.0385
                       Mean reward: 9.08
               Mean episode length: 719.78
Episode_Reward/track_lin_vel_xy_exp: 0.4471
Episode_Reward/track_ang_vel_z_exp: 0.2504
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0436
     Episode_Reward/dof_torques_l2: -0.0486
         Episode_Reward/dof_acc_l2: -0.1236
     Episode_Reward/action_rate_l2: -0.0496
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0105
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1626
Metrics/base_velocity/error_vel_xy: 0.3769
Metrics/base_velocity/error_vel_yaw: 0.2744
      Episode_Termination/time_out: 0.5516
  Episode_Termination/base_contact: 0.4484
--------------------------------------------------------------------------------
                   Total timesteps: 19488768
                    Iteration time: 2.51s
                      Time elapsed: 00:33:34
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 793/1400 [0m                      

                       Computation: 9779 steps/s (collection: 2.258s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 6.0476
                       Mean reward: 9.22
               Mean episode length: 727.57
Episode_Reward/track_lin_vel_xy_exp: 0.4325
Episode_Reward/track_ang_vel_z_exp: 0.2513
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0406
     Episode_Reward/dof_torques_l2: -0.0494
         Episode_Reward/dof_acc_l2: -0.1135
     Episode_Reward/action_rate_l2: -0.0484
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1662
Metrics/base_velocity/error_vel_xy: 0.3808
Metrics/base_velocity/error_vel_yaw: 0.2620
      Episode_Termination/time_out: 0.5472
  Episode_Termination/base_contact: 0.4528
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 2.51s
                      Time elapsed: 00:33:37
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 794/1400 [0m                      

                       Computation: 9678 steps/s (collection: 2.282s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.0397
                       Mean reward: 8.63
               Mean episode length: 684.67
Episode_Reward/track_lin_vel_xy_exp: 0.4431
Episode_Reward/track_ang_vel_z_exp: 0.2586
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.0506
         Episode_Reward/dof_acc_l2: -0.1233
     Episode_Reward/action_rate_l2: -0.0506
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1643
Metrics/base_velocity/error_vel_xy: 0.4181
Metrics/base_velocity/error_vel_yaw: 0.2825
      Episode_Termination/time_out: 0.5434
  Episode_Termination/base_contact: 0.4566
--------------------------------------------------------------------------------
                   Total timesteps: 19537920
                    Iteration time: 2.54s
                      Time elapsed: 00:33:39
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 795/1400 [0m                      

                       Computation: 9720 steps/s (collection: 2.271s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.0437
                       Mean reward: 9.06
               Mean episode length: 706.44
Episode_Reward/track_lin_vel_xy_exp: 0.5815
Episode_Reward/track_ang_vel_z_exp: 0.3332
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0500
     Episode_Reward/dof_torques_l2: -0.0594
         Episode_Reward/dof_acc_l2: -0.1361
     Episode_Reward/action_rate_l2: -0.0627
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1703
Metrics/base_velocity/error_vel_xy: 0.4610
Metrics/base_velocity/error_vel_yaw: 0.3067
      Episode_Termination/time_out: 0.5418
  Episode_Termination/base_contact: 0.4582
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.53s
                      Time elapsed: 00:33:42
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 796/1400 [0m                      

                       Computation: 9645 steps/s (collection: 2.290s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 6.0607
                       Mean reward: 10.41
               Mean episode length: 781.47
Episode_Reward/track_lin_vel_xy_exp: 0.6156
Episode_Reward/track_ang_vel_z_exp: 0.3454
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0513
     Episode_Reward/dof_torques_l2: -0.0593
         Episode_Reward/dof_acc_l2: -0.1326
     Episode_Reward/action_rate_l2: -0.0635
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1770
Metrics/base_velocity/error_vel_xy: 0.4566
Metrics/base_velocity/error_vel_yaw: 0.3225
      Episode_Termination/time_out: 0.5398
  Episode_Termination/base_contact: 0.4602
--------------------------------------------------------------------------------
                   Total timesteps: 19587072
                    Iteration time: 2.55s
                      Time elapsed: 00:33:44
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 797/1400 [0m                      

                       Computation: 9509 steps/s (collection: 2.328s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 6.0647
                       Mean reward: 10.64
               Mean episode length: 771.97
Episode_Reward/track_lin_vel_xy_exp: 0.4991
Episode_Reward/track_ang_vel_z_exp: 0.2804
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0511
         Episode_Reward/dof_acc_l2: -0.1253
     Episode_Reward/action_rate_l2: -0.0533
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1757
Metrics/base_velocity/error_vel_xy: 0.3832
Metrics/base_velocity/error_vel_yaw: 0.2722
      Episode_Termination/time_out: 0.5407
  Episode_Termination/base_contact: 0.4593
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 2.58s
                      Time elapsed: 00:33:47
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 798/1400 [0m                      

                       Computation: 9513 steps/s (collection: 2.319s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 6.0569
                       Mean reward: 10.71
               Mean episode length: 785.95
Episode_Reward/track_lin_vel_xy_exp: 0.5173
Episode_Reward/track_ang_vel_z_exp: 0.3159
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0484
     Episode_Reward/dof_torques_l2: -0.0633
         Episode_Reward/dof_acc_l2: -0.1280
     Episode_Reward/action_rate_l2: -0.0605
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1775
Metrics/base_velocity/error_vel_xy: 0.5155
Metrics/base_velocity/error_vel_yaw: 0.3082
      Episode_Termination/time_out: 0.5433
  Episode_Termination/base_contact: 0.4567
--------------------------------------------------------------------------------
                   Total timesteps: 19636224
                    Iteration time: 2.58s
                      Time elapsed: 00:33:49
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 799/1400 [0m                      

                       Computation: 9650 steps/s (collection: 2.283s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 6.0548
                       Mean reward: 9.62
               Mean episode length: 717.07
Episode_Reward/track_lin_vel_xy_exp: 0.4893
Episode_Reward/track_ang_vel_z_exp: 0.2729
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0424
     Episode_Reward/dof_torques_l2: -0.0510
         Episode_Reward/dof_acc_l2: -0.1172
     Episode_Reward/action_rate_l2: -0.0515
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1775
Metrics/base_velocity/error_vel_xy: 0.3664
Metrics/base_velocity/error_vel_yaw: 0.2634
      Episode_Termination/time_out: 0.5461
  Episode_Termination/base_contact: 0.4539
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.55s
                      Time elapsed: 00:33:52
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 800/1400 [0m                      

                       Computation: 9501 steps/s (collection: 2.327s, learning 0.259s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.0569
                       Mean reward: 9.21
               Mean episode length: 701.45
Episode_Reward/track_lin_vel_xy_exp: 0.4949
Episode_Reward/track_ang_vel_z_exp: 0.2863
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0454
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1206
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1833
Metrics/base_velocity/error_vel_xy: 0.4363
Metrics/base_velocity/error_vel_yaw: 0.3013
      Episode_Termination/time_out: 0.5460
  Episode_Termination/base_contact: 0.4540
--------------------------------------------------------------------------------
                   Total timesteps: 19685376
                    Iteration time: 2.59s
                      Time elapsed: 00:33:55
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 801/1400 [0m                      

                       Computation: 9570 steps/s (collection: 2.311s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.0649
                       Mean reward: 9.21
               Mean episode length: 707.39
Episode_Reward/track_lin_vel_xy_exp: 0.5192
Episode_Reward/track_ang_vel_z_exp: 0.2993
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0573
         Episode_Reward/dof_acc_l2: -0.1197
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1891
Metrics/base_velocity/error_vel_xy: 0.4328
Metrics/base_velocity/error_vel_yaw: 0.2931
      Episode_Termination/time_out: 0.5421
  Episode_Termination/base_contact: 0.4579
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 2.57s
                      Time elapsed: 00:33:57
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 802/1400 [0m                      

                       Computation: 9537 steps/s (collection: 2.320s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 6.0752
                       Mean reward: 9.05
               Mean episode length: 727.65
Episode_Reward/track_lin_vel_xy_exp: 0.4825
Episode_Reward/track_ang_vel_z_exp: 0.3117
       Episode_Reward/lin_vel_z_l2: -0.0389
      Episode_Reward/ang_vel_xy_l2: -0.0531
     Episode_Reward/dof_torques_l2: -0.0625
         Episode_Reward/dof_acc_l2: -0.1443
     Episode_Reward/action_rate_l2: -0.0612
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0072
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1828
Metrics/base_velocity/error_vel_xy: 0.5652
Metrics/base_velocity/error_vel_yaw: 0.3307
      Episode_Termination/time_out: 0.5422
  Episode_Termination/base_contact: 0.4578
--------------------------------------------------------------------------------
                   Total timesteps: 19734528
                    Iteration time: 2.58s
                      Time elapsed: 00:34:00
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 803/1400 [0m                      

                       Computation: 9597 steps/s (collection: 2.304s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 6.0824
                       Mean reward: 9.39
               Mean episode length: 744.97
Episode_Reward/track_lin_vel_xy_exp: 0.5245
Episode_Reward/track_ang_vel_z_exp: 0.3006
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0447
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1193
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0101
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1775
Metrics/base_velocity/error_vel_xy: 0.4350
Metrics/base_velocity/error_vel_yaw: 0.3026
      Episode_Termination/time_out: 0.5442
  Episode_Termination/base_contact: 0.4558
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.56s
                      Time elapsed: 00:34:02
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 804/1400 [0m                      

                       Computation: 9642 steps/s (collection: 2.290s, learning 0.259s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 6.0868
                       Mean reward: 8.77
               Mean episode length: 711.33
Episode_Reward/track_lin_vel_xy_exp: 0.3515
Episode_Reward/track_ang_vel_z_exp: 0.2117
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0346
     Episode_Reward/dof_torques_l2: -0.0445
         Episode_Reward/dof_acc_l2: -0.0986
     Episode_Reward/action_rate_l2: -0.0423
      Episode_Reward/feet_air_time: -0.0062
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1796
Metrics/base_velocity/error_vel_xy: 0.3568
Metrics/base_velocity/error_vel_yaw: 0.2335
      Episode_Termination/time_out: 0.5431
  Episode_Termination/base_contact: 0.4569
--------------------------------------------------------------------------------
                   Total timesteps: 19783680
                    Iteration time: 2.55s
                      Time elapsed: 00:34:05
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 805/1400 [0m                      

                       Computation: 9544 steps/s (collection: 2.321s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 6.0875
                       Mean reward: 9.38
               Mean episode length: 722.52
Episode_Reward/track_lin_vel_xy_exp: 0.5452
Episode_Reward/track_ang_vel_z_exp: 0.3103
       Episode_Reward/lin_vel_z_l2: -0.0353
      Episode_Reward/ang_vel_xy_l2: -0.0453
     Episode_Reward/dof_torques_l2: -0.0570
         Episode_Reward/dof_acc_l2: -0.1323
     Episode_Reward/action_rate_l2: -0.0587
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1758
Metrics/base_velocity/error_vel_xy: 0.4170
Metrics/base_velocity/error_vel_yaw: 0.2831
      Episode_Termination/time_out: 0.5406
  Episode_Termination/base_contact: 0.4594
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 2.57s
                      Time elapsed: 00:34:07
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 806/1400 [0m                      

                       Computation: 9683 steps/s (collection: 2.273s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 6.0815
                       Mean reward: 9.95
               Mean episode length: 732.02
Episode_Reward/track_lin_vel_xy_exp: 0.5285
Episode_Reward/track_ang_vel_z_exp: 0.2965
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.0415
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.1161
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1779
Metrics/base_velocity/error_vel_xy: 0.3671
Metrics/base_velocity/error_vel_yaw: 0.2567
      Episode_Termination/time_out: 0.5433
  Episode_Termination/base_contact: 0.4567
--------------------------------------------------------------------------------
                   Total timesteps: 19832832
                    Iteration time: 2.54s
                      Time elapsed: 00:34:10
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 807/1400 [0m                      

                       Computation: 9666 steps/s (collection: 2.284s, learning 0.259s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.0820
                       Mean reward: 10.48
               Mean episode length: 763.85
Episode_Reward/track_lin_vel_xy_exp: 0.5168
Episode_Reward/track_ang_vel_z_exp: 0.2940
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0475
     Episode_Reward/dof_torques_l2: -0.0565
         Episode_Reward/dof_acc_l2: -0.1345
     Episode_Reward/action_rate_l2: -0.0569
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1835
Metrics/base_velocity/error_vel_xy: 0.4337
Metrics/base_velocity/error_vel_yaw: 0.3004
      Episode_Termination/time_out: 0.5436
  Episode_Termination/base_contact: 0.4564
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.54s
                      Time elapsed: 00:34:12
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 808/1400 [0m                      

                       Computation: 9592 steps/s (collection: 2.299s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.0773
                       Mean reward: 10.42
               Mean episode length: 775.47
Episode_Reward/track_lin_vel_xy_exp: 0.6004
Episode_Reward/track_ang_vel_z_exp: 0.3349
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0497
     Episode_Reward/dof_torques_l2: -0.0656
         Episode_Reward/dof_acc_l2: -0.1390
     Episode_Reward/action_rate_l2: -0.0658
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1862
Metrics/base_velocity/error_vel_xy: 0.4776
Metrics/base_velocity/error_vel_yaw: 0.3519
      Episode_Termination/time_out: 0.5426
  Episode_Termination/base_contact: 0.4574
--------------------------------------------------------------------------------
                   Total timesteps: 19881984
                    Iteration time: 2.56s
                      Time elapsed: 00:34:15
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 809/1400 [0m                      

                       Computation: 9568 steps/s (collection: 2.311s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 6.0805
                       Mean reward: 9.89
               Mean episode length: 773.03
Episode_Reward/track_lin_vel_xy_exp: 0.5098
Episode_Reward/track_ang_vel_z_exp: 0.2891
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0437
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1259
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1768
Metrics/base_velocity/error_vel_xy: 0.4070
Metrics/base_velocity/error_vel_yaw: 0.2799
      Episode_Termination/time_out: 0.5451
  Episode_Termination/base_contact: 0.4549
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 2.57s
                      Time elapsed: 00:34:18
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 810/1400 [0m                      

                       Computation: 9596 steps/s (collection: 2.302s, learning 0.259s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 6.0818
                       Mean reward: 9.56
               Mean episode length: 742.08
Episode_Reward/track_lin_vel_xy_exp: 0.5150
Episode_Reward/track_ang_vel_z_exp: 0.2934
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.0542
         Episode_Reward/dof_acc_l2: -0.1203
     Episode_Reward/action_rate_l2: -0.0556
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1785
Metrics/base_velocity/error_vel_xy: 0.4047
Metrics/base_velocity/error_vel_yaw: 0.2712
      Episode_Termination/time_out: 0.5458
  Episode_Termination/base_contact: 0.4542
--------------------------------------------------------------------------------
                   Total timesteps: 19931136
                    Iteration time: 2.56s
                      Time elapsed: 00:34:20
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 811/1400 [0m                      

                       Computation: 9493 steps/s (collection: 2.332s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.0720
                       Mean reward: 9.76
               Mean episode length: 738.99
Episode_Reward/track_lin_vel_xy_exp: 0.5654
Episode_Reward/track_ang_vel_z_exp: 0.3313
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0603
         Episode_Reward/dof_acc_l2: -0.1408
     Episode_Reward/action_rate_l2: -0.0629
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1844
Metrics/base_velocity/error_vel_xy: 0.4806
Metrics/base_velocity/error_vel_yaw: 0.3029
      Episode_Termination/time_out: 0.5449
  Episode_Termination/base_contact: 0.4551
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.59s
                      Time elapsed: 00:34:23
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 812/1400 [0m                      

                       Computation: 9600 steps/s (collection: 2.294s, learning 0.266s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 6.0534
                       Mean reward: 10.61
               Mean episode length: 767.65
Episode_Reward/track_lin_vel_xy_exp: 0.5828
Episode_Reward/track_ang_vel_z_exp: 0.3151
       Episode_Reward/lin_vel_z_l2: -0.0302
      Episode_Reward/ang_vel_xy_l2: -0.0418
     Episode_Reward/dof_torques_l2: -0.0570
         Episode_Reward/dof_acc_l2: -0.1204
     Episode_Reward/action_rate_l2: -0.0589
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1996
Metrics/base_velocity/error_vel_xy: 0.3599
Metrics/base_velocity/error_vel_yaw: 0.2714
      Episode_Termination/time_out: 0.5427
  Episode_Termination/base_contact: 0.4573
--------------------------------------------------------------------------------
                   Total timesteps: 19980288
                    Iteration time: 2.56s
                      Time elapsed: 00:34:25
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 813/1400 [0m                      

                       Computation: 9659 steps/s (collection: 2.287s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.0491
                       Mean reward: 11.11
               Mean episode length: 809.26
Episode_Reward/track_lin_vel_xy_exp: 0.5720
Episode_Reward/track_ang_vel_z_exp: 0.3381
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0516
     Episode_Reward/dof_torques_l2: -0.0591
         Episode_Reward/dof_acc_l2: -0.1392
     Episode_Reward/action_rate_l2: -0.0629
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0096
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2077
Metrics/base_velocity/error_vel_xy: 0.4951
Metrics/base_velocity/error_vel_yaw: 0.3182
      Episode_Termination/time_out: 0.5452
  Episode_Termination/base_contact: 0.4548
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 2.54s
                      Time elapsed: 00:34:28
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 814/1400 [0m                      

                       Computation: 9451 steps/s (collection: 2.335s, learning 0.266s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.0538
                       Mean reward: 10.85
               Mean episode length: 805.68
Episode_Reward/track_lin_vel_xy_exp: 0.5774
Episode_Reward/track_ang_vel_z_exp: 0.3286
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0618
         Episode_Reward/dof_acc_l2: -0.1448
     Episode_Reward/action_rate_l2: -0.0638
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2108
Metrics/base_velocity/error_vel_xy: 0.4687
Metrics/base_velocity/error_vel_yaw: 0.3236
      Episode_Termination/time_out: 0.5494
  Episode_Termination/base_contact: 0.4506
--------------------------------------------------------------------------------
                   Total timesteps: 20029440
                    Iteration time: 2.60s
                      Time elapsed: 00:34:30
                               ETA: 00:24:49

################################################################################
                     [1m Learning iteration 815/1400 [0m                      

                       Computation: 9549 steps/s (collection: 2.318s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 6.0478
                       Mean reward: 10.25
               Mean episode length: 775.75
Episode_Reward/track_lin_vel_xy_exp: 0.5023
Episode_Reward/track_ang_vel_z_exp: 0.2745
       Episode_Reward/lin_vel_z_l2: -0.0344
      Episode_Reward/ang_vel_xy_l2: -0.0397
     Episode_Reward/dof_torques_l2: -0.0456
         Episode_Reward/dof_acc_l2: -0.1061
     Episode_Reward/action_rate_l2: -0.0513
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2030
Metrics/base_velocity/error_vel_xy: 0.3548
Metrics/base_velocity/error_vel_yaw: 0.2572
      Episode_Termination/time_out: 0.5441
  Episode_Termination/base_contact: 0.4559
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.57s
                      Time elapsed: 00:34:33
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 816/1400 [0m                      

                       Computation: 9529 steps/s (collection: 2.323s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 6.0388
                       Mean reward: 10.16
               Mean episode length: 765.97
Episode_Reward/track_lin_vel_xy_exp: 0.5714
Episode_Reward/track_ang_vel_z_exp: 0.3273
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0474
     Episode_Reward/dof_torques_l2: -0.0601
         Episode_Reward/dof_acc_l2: -0.1409
     Episode_Reward/action_rate_l2: -0.0632
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2047
Metrics/base_velocity/error_vel_xy: 0.4464
Metrics/base_velocity/error_vel_yaw: 0.3059
      Episode_Termination/time_out: 0.5396
  Episode_Termination/base_contact: 0.4604
--------------------------------------------------------------------------------
                   Total timesteps: 20078592
                    Iteration time: 2.58s
                      Time elapsed: 00:34:36
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 817/1400 [0m                      

                       Computation: 9569 steps/s (collection: 2.303s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.0327
                       Mean reward: 10.24
               Mean episode length: 747.70
Episode_Reward/track_lin_vel_xy_exp: 0.5389
Episode_Reward/track_ang_vel_z_exp: 0.3025
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0437
     Episode_Reward/dof_torques_l2: -0.0519
         Episode_Reward/dof_acc_l2: -0.1248
     Episode_Reward/action_rate_l2: -0.0569
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2139
Metrics/base_velocity/error_vel_xy: 0.3904
Metrics/base_velocity/error_vel_yaw: 0.2735
      Episode_Termination/time_out: 0.5385
  Episode_Termination/base_contact: 0.4615
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 2.57s
                      Time elapsed: 00:34:38
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 818/1400 [0m                      

                       Computation: 9666 steps/s (collection: 2.285s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 6.0278
                       Mean reward: 9.66
               Mean episode length: 729.22
Episode_Reward/track_lin_vel_xy_exp: 0.4761
Episode_Reward/track_ang_vel_z_exp: 0.2746
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.0524
         Episode_Reward/dof_acc_l2: -0.1266
     Episode_Reward/action_rate_l2: -0.0533
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2174
Metrics/base_velocity/error_vel_xy: 0.4057
Metrics/base_velocity/error_vel_yaw: 0.2707
      Episode_Termination/time_out: 0.5387
  Episode_Termination/base_contact: 0.4613
--------------------------------------------------------------------------------
                   Total timesteps: 20127744
                    Iteration time: 2.54s
                      Time elapsed: 00:34:41
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 819/1400 [0m                      

                       Computation: 9475 steps/s (collection: 2.337s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.0114
                       Mean reward: 9.05
               Mean episode length: 687.06
Episode_Reward/track_lin_vel_xy_exp: 0.4800
Episode_Reward/track_ang_vel_z_exp: 0.2757
       Episode_Reward/lin_vel_z_l2: -0.0353
      Episode_Reward/ang_vel_xy_l2: -0.0436
     Episode_Reward/dof_torques_l2: -0.0523
         Episode_Reward/dof_acc_l2: -0.1186
     Episode_Reward/action_rate_l2: -0.0539
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2092
Metrics/base_velocity/error_vel_xy: 0.4193
Metrics/base_velocity/error_vel_yaw: 0.2850
      Episode_Termination/time_out: 0.5370
  Episode_Termination/base_contact: 0.4630
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.59s
                      Time elapsed: 00:34:43
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 820/1400 [0m                      

                       Computation: 9567 steps/s (collection: 2.303s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 6.0084
                       Mean reward: 9.35
               Mean episode length: 697.88
Episode_Reward/track_lin_vel_xy_exp: 0.4847
Episode_Reward/track_ang_vel_z_exp: 0.2754
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0384
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.1070
     Episode_Reward/action_rate_l2: -0.0516
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2103
Metrics/base_velocity/error_vel_xy: 0.3589
Metrics/base_velocity/error_vel_yaw: 0.2427
      Episode_Termination/time_out: 0.5326
  Episode_Termination/base_contact: 0.4674
--------------------------------------------------------------------------------
                   Total timesteps: 20176896
                    Iteration time: 2.57s
                      Time elapsed: 00:34:46
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 821/1400 [0m                      

                       Computation: 9428 steps/s (collection: 2.342s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.0183
                       Mean reward: 9.28
               Mean episode length: 692.62
Episode_Reward/track_lin_vel_xy_exp: 0.4679
Episode_Reward/track_ang_vel_z_exp: 0.2691
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.0421
     Episode_Reward/dof_torques_l2: -0.0513
         Episode_Reward/dof_acc_l2: -0.1114
     Episode_Reward/action_rate_l2: -0.0530
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0139
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2017
Metrics/base_velocity/error_vel_xy: 0.4108
Metrics/base_velocity/error_vel_yaw: 0.2878
      Episode_Termination/time_out: 0.5321
  Episode_Termination/base_contact: 0.4679
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 2.61s
                      Time elapsed: 00:34:48
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 822/1400 [0m                      

                       Computation: 9498 steps/s (collection: 2.330s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 6.0245
                       Mean reward: 9.80
               Mean episode length: 733.56
Episode_Reward/track_lin_vel_xy_exp: 0.5712
Episode_Reward/track_ang_vel_z_exp: 0.3308
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0496
     Episode_Reward/dof_torques_l2: -0.0629
         Episode_Reward/dof_acc_l2: -0.1379
     Episode_Reward/action_rate_l2: -0.0639
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0091
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1983
Metrics/base_velocity/error_vel_xy: 0.4887
Metrics/base_velocity/error_vel_yaw: 0.3107
      Episode_Termination/time_out: 0.5376
  Episode_Termination/base_contact: 0.4624
--------------------------------------------------------------------------------
                   Total timesteps: 20226048
                    Iteration time: 2.59s
                      Time elapsed: 00:34:51
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 823/1400 [0m                      

                       Computation: 9512 steps/s (collection: 2.326s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 6.0220
                       Mean reward: 10.35
               Mean episode length: 770.46
Episode_Reward/track_lin_vel_xy_exp: 0.5839
Episode_Reward/track_ang_vel_z_exp: 0.3256
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0480
     Episode_Reward/dof_torques_l2: -0.0602
         Episode_Reward/dof_acc_l2: -0.1347
     Episode_Reward/action_rate_l2: -0.0632
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2034
Metrics/base_velocity/error_vel_xy: 0.4414
Metrics/base_velocity/error_vel_yaw: 0.3201
      Episode_Termination/time_out: 0.5405
  Episode_Termination/base_contact: 0.4595
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.58s
                      Time elapsed: 00:34:54
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 824/1400 [0m                      

                       Computation: 9515 steps/s (collection: 2.328s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.0173
                       Mean reward: 10.04
               Mean episode length: 755.30
Episode_Reward/track_lin_vel_xy_exp: 0.4514
Episode_Reward/track_ang_vel_z_exp: 0.2657
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.1223
     Episode_Reward/action_rate_l2: -0.0517
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0077
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2077
Metrics/base_velocity/error_vel_xy: 0.4106
Metrics/base_velocity/error_vel_yaw: 0.2685
      Episode_Termination/time_out: 0.5375
  Episode_Termination/base_contact: 0.4625
--------------------------------------------------------------------------------
                   Total timesteps: 20275200
                    Iteration time: 2.58s
                      Time elapsed: 00:34:56
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 825/1400 [0m                      

                       Computation: 9734 steps/s (collection: 2.266s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 6.0175
                       Mean reward: 9.66
               Mean episode length: 721.80
Episode_Reward/track_lin_vel_xy_exp: 0.4622
Episode_Reward/track_ang_vel_z_exp: 0.2641
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0414
     Episode_Reward/dof_torques_l2: -0.0515
         Episode_Reward/dof_acc_l2: -0.1114
     Episode_Reward/action_rate_l2: -0.0527
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2094
Metrics/base_velocity/error_vel_xy: 0.3725
Metrics/base_velocity/error_vel_yaw: 0.2663
      Episode_Termination/time_out: 0.5382
  Episode_Termination/base_contact: 0.4618
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 2.52s
                      Time elapsed: 00:34:59
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 826/1400 [0m                      

                       Computation: 9572 steps/s (collection: 2.312s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 6.0211
                       Mean reward: 9.87
               Mean episode length: 743.06
Episode_Reward/track_lin_vel_xy_exp: 0.5639
Episode_Reward/track_ang_vel_z_exp: 0.3334
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1389
     Episode_Reward/action_rate_l2: -0.0627
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2122
Metrics/base_velocity/error_vel_xy: 0.4887
Metrics/base_velocity/error_vel_yaw: 0.3029
      Episode_Termination/time_out: 0.5404
  Episode_Termination/base_contact: 0.4596
--------------------------------------------------------------------------------
                   Total timesteps: 20324352
                    Iteration time: 2.57s
                      Time elapsed: 00:35:01
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 827/1400 [0m                      

                       Computation: 9563 steps/s (collection: 2.306s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 6.0262
                       Mean reward: 10.00
               Mean episode length: 760.90
Episode_Reward/track_lin_vel_xy_exp: 0.5389
Episode_Reward/track_ang_vel_z_exp: 0.3186
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0617
         Episode_Reward/dof_acc_l2: -0.1350
     Episode_Reward/action_rate_l2: -0.0619
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0081
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2164
Metrics/base_velocity/error_vel_xy: 0.4925
Metrics/base_velocity/error_vel_yaw: 0.3203
      Episode_Termination/time_out: 0.5402
  Episode_Termination/base_contact: 0.4598
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.57s
                      Time elapsed: 00:35:04
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 828/1400 [0m                      

                       Computation: 9764 steps/s (collection: 2.255s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 6.0229
                       Mean reward: 9.64
               Mean episode length: 742.73
Episode_Reward/track_lin_vel_xy_exp: 0.4317
Episode_Reward/track_ang_vel_z_exp: 0.2519
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.0407
     Episode_Reward/dof_torques_l2: -0.0509
         Episode_Reward/dof_acc_l2: -0.1096
     Episode_Reward/action_rate_l2: -0.0485
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2228
Metrics/base_velocity/error_vel_xy: 0.3801
Metrics/base_velocity/error_vel_yaw: 0.2493
      Episode_Termination/time_out: 0.5401
  Episode_Termination/base_contact: 0.4599
--------------------------------------------------------------------------------
                   Total timesteps: 20373504
                    Iteration time: 2.52s
                      Time elapsed: 00:35:06
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 829/1400 [0m                      

                       Computation: 9590 steps/s (collection: 2.307s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 6.0215
                       Mean reward: 9.32
               Mean episode length: 747.73
Episode_Reward/track_lin_vel_xy_exp: 0.5325
Episode_Reward/track_ang_vel_z_exp: 0.3059
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.0487
     Episode_Reward/dof_torques_l2: -0.0602
         Episode_Reward/dof_acc_l2: -0.1368
     Episode_Reward/action_rate_l2: -0.0599
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2302
Metrics/base_velocity/error_vel_xy: 0.4576
Metrics/base_velocity/error_vel_yaw: 0.3124
      Episode_Termination/time_out: 0.5353
  Episode_Termination/base_contact: 0.4647
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 2.56s
                      Time elapsed: 00:35:09
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 830/1400 [0m                      

                       Computation: 9558 steps/s (collection: 2.316s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 6.0174
                       Mean reward: 9.77
               Mean episode length: 755.27
Episode_Reward/track_lin_vel_xy_exp: 0.5903
Episode_Reward/track_ang_vel_z_exp: 0.3440
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0487
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1425
     Episode_Reward/action_rate_l2: -0.0652
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2424
Metrics/base_velocity/error_vel_xy: 0.4798
Metrics/base_velocity/error_vel_yaw: 0.3099
      Episode_Termination/time_out: 0.5378
  Episode_Termination/base_contact: 0.4622
--------------------------------------------------------------------------------
                   Total timesteps: 20422656
                    Iteration time: 2.57s
                      Time elapsed: 00:35:12
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 831/1400 [0m                      

                       Computation: 9599 steps/s (collection: 2.303s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.0240
                       Mean reward: 9.70
               Mean episode length: 757.45
Episode_Reward/track_lin_vel_xy_exp: 0.4783
Episode_Reward/track_ang_vel_z_exp: 0.2822
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0436
     Episode_Reward/dof_torques_l2: -0.0528
         Episode_Reward/dof_acc_l2: -0.1263
     Episode_Reward/action_rate_l2: -0.0544
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2521
Metrics/base_velocity/error_vel_xy: 0.4366
Metrics/base_velocity/error_vel_yaw: 0.2863
      Episode_Termination/time_out: 0.5424
  Episode_Termination/base_contact: 0.4576
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.56s
                      Time elapsed: 00:35:14
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 832/1400 [0m                      

                       Computation: 9539 steps/s (collection: 2.319s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 6.0285
                       Mean reward: 9.10
               Mean episode length: 737.17
Episode_Reward/track_lin_vel_xy_exp: 0.4975
Episode_Reward/track_ang_vel_z_exp: 0.2962
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0468
     Episode_Reward/dof_torques_l2: -0.0585
         Episode_Reward/dof_acc_l2: -0.1314
     Episode_Reward/action_rate_l2: -0.0588
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0117
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2625
Metrics/base_velocity/error_vel_xy: 0.4968
Metrics/base_velocity/error_vel_yaw: 0.3304
      Episode_Termination/time_out: 0.5372
  Episode_Termination/base_contact: 0.4628
--------------------------------------------------------------------------------
                   Total timesteps: 20471808
                    Iteration time: 2.58s
                      Time elapsed: 00:35:17
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 833/1400 [0m                      

                       Computation: 9692 steps/s (collection: 2.277s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 6.0252
                       Mean reward: 8.61
               Mean episode length: 717.05
Episode_Reward/track_lin_vel_xy_exp: 0.5335
Episode_Reward/track_ang_vel_z_exp: 0.3069
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0599
         Episode_Reward/dof_acc_l2: -0.1228
     Episode_Reward/action_rate_l2: -0.0596
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2634
Metrics/base_velocity/error_vel_xy: 0.4507
Metrics/base_velocity/error_vel_yaw: 0.3194
      Episode_Termination/time_out: 0.5389
  Episode_Termination/base_contact: 0.4611
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 2.54s
                      Time elapsed: 00:35:19
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 834/1400 [0m                      

                       Computation: 9464 steps/s (collection: 2.334s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.0341
                       Mean reward: 8.93
               Mean episode length: 724.32
Episode_Reward/track_lin_vel_xy_exp: 0.5384
Episode_Reward/track_ang_vel_z_exp: 0.3112
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0574
         Episode_Reward/dof_acc_l2: -0.1325
     Episode_Reward/action_rate_l2: -0.0599
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2620
Metrics/base_velocity/error_vel_xy: 0.4670
Metrics/base_velocity/error_vel_yaw: 0.3227
      Episode_Termination/time_out: 0.5385
  Episode_Termination/base_contact: 0.4615
--------------------------------------------------------------------------------
                   Total timesteps: 20520960
                    Iteration time: 2.60s
                      Time elapsed: 00:35:22
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 835/1400 [0m                      

                       Computation: 9598 steps/s (collection: 2.303s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.0317
                       Mean reward: 9.27
               Mean episode length: 689.30
Episode_Reward/track_lin_vel_xy_exp: 0.4535
Episode_Reward/track_ang_vel_z_exp: 0.2520
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0366
     Episode_Reward/dof_torques_l2: -0.0438
         Episode_Reward/dof_acc_l2: -0.0985
     Episode_Reward/action_rate_l2: -0.0481
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2505
Metrics/base_velocity/error_vel_xy: 0.3290
Metrics/base_velocity/error_vel_yaw: 0.2413
      Episode_Termination/time_out: 0.5350
  Episode_Termination/base_contact: 0.4650
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.56s
                      Time elapsed: 00:35:24
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 836/1400 [0m                      

                       Computation: 9535 steps/s (collection: 2.320s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 6.0085
                       Mean reward: 9.55
               Mean episode length: 702.99
Episode_Reward/track_lin_vel_xy_exp: 0.5996
Episode_Reward/track_ang_vel_z_exp: 0.3382
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1363
     Episode_Reward/action_rate_l2: -0.0638
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0108
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2478
Metrics/base_velocity/error_vel_xy: 0.4428
Metrics/base_velocity/error_vel_yaw: 0.3034
      Episode_Termination/time_out: 0.5352
  Episode_Termination/base_contact: 0.4648
--------------------------------------------------------------------------------
                   Total timesteps: 20570112
                    Iteration time: 2.58s
                      Time elapsed: 00:35:27
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 837/1400 [0m                      

                       Computation: 9531 steps/s (collection: 2.314s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.9933
                       Mean reward: 10.26
               Mean episode length: 739.51
Episode_Reward/track_lin_vel_xy_exp: 0.4991
Episode_Reward/track_ang_vel_z_exp: 0.2956
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1265
     Episode_Reward/action_rate_l2: -0.0569
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2530
Metrics/base_velocity/error_vel_xy: 0.4406
Metrics/base_velocity/error_vel_yaw: 0.2820
      Episode_Termination/time_out: 0.5380
  Episode_Termination/base_contact: 0.4620
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 2.58s
                      Time elapsed: 00:35:30
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 838/1400 [0m                      

                       Computation: 9506 steps/s (collection: 2.329s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0127
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.9928
                       Mean reward: 10.03
               Mean episode length: 761.20
Episode_Reward/track_lin_vel_xy_exp: 0.4553
Episode_Reward/track_ang_vel_z_exp: 0.2754
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0443
     Episode_Reward/dof_torques_l2: -0.0540
         Episode_Reward/dof_acc_l2: -0.1306
     Episode_Reward/action_rate_l2: -0.0540
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0069
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2561
Metrics/base_velocity/error_vel_xy: 0.4615
Metrics/base_velocity/error_vel_yaw: 0.2990
      Episode_Termination/time_out: 0.5402
  Episode_Termination/base_contact: 0.4598
--------------------------------------------------------------------------------
                   Total timesteps: 20619264
                    Iteration time: 2.59s
                      Time elapsed: 00:35:32
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 839/1400 [0m                      

                       Computation: 9602 steps/s (collection: 2.300s, learning 0.259s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.9862
                       Mean reward: 9.89
               Mean episode length: 754.07
Episode_Reward/track_lin_vel_xy_exp: 0.5707
Episode_Reward/track_ang_vel_z_exp: 0.3222
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0478
     Episode_Reward/dof_torques_l2: -0.0595
         Episode_Reward/dof_acc_l2: -0.1388
     Episode_Reward/action_rate_l2: -0.0621
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0064
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2574
Metrics/base_velocity/error_vel_xy: 0.4555
Metrics/base_velocity/error_vel_yaw: 0.3198
      Episode_Termination/time_out: 0.5366
  Episode_Termination/base_contact: 0.4634
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.56s
                      Time elapsed: 00:35:35
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 840/1400 [0m                      

                       Computation: 9578 steps/s (collection: 2.302s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.9846
                       Mean reward: 9.74
               Mean episode length: 740.26
Episode_Reward/track_lin_vel_xy_exp: 0.5419
Episode_Reward/track_ang_vel_z_exp: 0.3074
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0434
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1326
     Episode_Reward/action_rate_l2: -0.0599
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2639
Metrics/base_velocity/error_vel_xy: 0.4280
Metrics/base_velocity/error_vel_yaw: 0.3010
      Episode_Termination/time_out: 0.5361
  Episode_Termination/base_contact: 0.4639
--------------------------------------------------------------------------------
                   Total timesteps: 20668416
                    Iteration time: 2.57s
                      Time elapsed: 00:35:37
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 841/1400 [0m                      

                       Computation: 9634 steps/s (collection: 2.295s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.9870
                       Mean reward: 10.50
               Mean episode length: 759.17
Episode_Reward/track_lin_vel_xy_exp: 0.6355
Episode_Reward/track_ang_vel_z_exp: 0.3540
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.0506
     Episode_Reward/dof_torques_l2: -0.0630
         Episode_Reward/dof_acc_l2: -0.1394
     Episode_Reward/action_rate_l2: -0.0673
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2668
Metrics/base_velocity/error_vel_xy: 0.4560
Metrics/base_velocity/error_vel_yaw: 0.3271
      Episode_Termination/time_out: 0.5372
  Episode_Termination/base_contact: 0.4628
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 2.55s
                      Time elapsed: 00:35:40
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 842/1400 [0m                      

                       Computation: 9633 steps/s (collection: 2.294s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.9895
                       Mean reward: 10.19
               Mean episode length: 748.14
Episode_Reward/track_lin_vel_xy_exp: 0.4620
Episode_Reward/track_ang_vel_z_exp: 0.2686
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0424
     Episode_Reward/dof_torques_l2: -0.0527
         Episode_Reward/dof_acc_l2: -0.1162
     Episode_Reward/action_rate_l2: -0.0532
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2675
Metrics/base_velocity/error_vel_xy: 0.4211
Metrics/base_velocity/error_vel_yaw: 0.2930
      Episode_Termination/time_out: 0.5385
  Episode_Termination/base_contact: 0.4615
--------------------------------------------------------------------------------
                   Total timesteps: 20717568
                    Iteration time: 2.55s
                      Time elapsed: 00:35:42
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 843/1400 [0m                      

                       Computation: 9567 steps/s (collection: 2.314s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 5.9969
                       Mean reward: 10.16
               Mean episode length: 760.57
Episode_Reward/track_lin_vel_xy_exp: 0.5637
Episode_Reward/track_ang_vel_z_exp: 0.3243
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.0626
         Episode_Reward/dof_acc_l2: -0.1409
     Episode_Reward/action_rate_l2: -0.0623
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2683
Metrics/base_velocity/error_vel_xy: 0.4655
Metrics/base_velocity/error_vel_yaw: 0.3135
      Episode_Termination/time_out: 0.5347
  Episode_Termination/base_contact: 0.4653
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.57s
                      Time elapsed: 00:35:45
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 844/1400 [0m                      

                       Computation: 9662 steps/s (collection: 2.286s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 6.0043
                       Mean reward: 9.59
               Mean episode length: 735.23
Episode_Reward/track_lin_vel_xy_exp: 0.4827
Episode_Reward/track_ang_vel_z_exp: 0.2828
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0445
     Episode_Reward/dof_torques_l2: -0.0537
         Episode_Reward/dof_acc_l2: -0.1264
     Episode_Reward/action_rate_l2: -0.0551
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0071
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2807
Metrics/base_velocity/error_vel_xy: 0.4405
Metrics/base_velocity/error_vel_yaw: 0.2842
      Episode_Termination/time_out: 0.5384
  Episode_Termination/base_contact: 0.4616
--------------------------------------------------------------------------------
                   Total timesteps: 20766720
                    Iteration time: 2.54s
                      Time elapsed: 00:35:47
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 845/1400 [0m                      

                       Computation: 9599 steps/s (collection: 2.295s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 6.0131
                       Mean reward: 9.98
               Mean episode length: 746.87
Episode_Reward/track_lin_vel_xy_exp: 0.5559
Episode_Reward/track_ang_vel_z_exp: 0.3014
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0424
     Episode_Reward/dof_torques_l2: -0.0550
         Episode_Reward/dof_acc_l2: -0.1277
     Episode_Reward/action_rate_l2: -0.0575
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2952
Metrics/base_velocity/error_vel_xy: 0.3711
Metrics/base_velocity/error_vel_yaw: 0.2858
      Episode_Termination/time_out: 0.5431
  Episode_Termination/base_contact: 0.4569
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 2.56s
                      Time elapsed: 00:35:50
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 846/1400 [0m                      

                       Computation: 9676 steps/s (collection: 2.283s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.0211
                       Mean reward: 9.67
               Mean episode length: 707.81
Episode_Reward/track_lin_vel_xy_exp: 0.4917
Episode_Reward/track_ang_vel_z_exp: 0.2821
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0416
     Episode_Reward/dof_torques_l2: -0.0514
         Episode_Reward/dof_acc_l2: -0.1138
     Episode_Reward/action_rate_l2: -0.0533
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2928
Metrics/base_velocity/error_vel_xy: 0.4002
Metrics/base_velocity/error_vel_yaw: 0.2707
      Episode_Termination/time_out: 0.5442
  Episode_Termination/base_contact: 0.4558
--------------------------------------------------------------------------------
                   Total timesteps: 20815872
                    Iteration time: 2.54s
                      Time elapsed: 00:35:53
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 847/1400 [0m                      

                       Computation: 9600 steps/s (collection: 2.308s, learning 0.252s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 6.0125
                       Mean reward: 10.03
               Mean episode length: 727.25
Episode_Reward/track_lin_vel_xy_exp: 0.5496
Episode_Reward/track_ang_vel_z_exp: 0.3102
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1408
     Episode_Reward/action_rate_l2: -0.0603
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0100
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2865
Metrics/base_velocity/error_vel_xy: 0.4448
Metrics/base_velocity/error_vel_yaw: 0.3135
      Episode_Termination/time_out: 0.5419
  Episode_Termination/base_contact: 0.4581
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.56s
                      Time elapsed: 00:35:55
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 848/1400 [0m                      

                       Computation: 9641 steps/s (collection: 2.293s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.0154
                       Mean reward: 9.51
               Mean episode length: 707.93
Episode_Reward/track_lin_vel_xy_exp: 0.4910
Episode_Reward/track_ang_vel_z_exp: 0.2783
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0429
     Episode_Reward/dof_torques_l2: -0.0499
         Episode_Reward/dof_acc_l2: -0.1189
     Episode_Reward/action_rate_l2: -0.0529
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2828
Metrics/base_velocity/error_vel_xy: 0.3892
Metrics/base_velocity/error_vel_yaw: 0.2779
      Episode_Termination/time_out: 0.5356
  Episode_Termination/base_contact: 0.4644
--------------------------------------------------------------------------------
                   Total timesteps: 20865024
                    Iteration time: 2.55s
                      Time elapsed: 00:35:58
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 849/1400 [0m                      

                       Computation: 9591 steps/s (collection: 2.306s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 6.0224
                       Mean reward: 9.80
               Mean episode length: 725.09
Episode_Reward/track_lin_vel_xy_exp: 0.5522
Episode_Reward/track_ang_vel_z_exp: 0.3049
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0422
     Episode_Reward/dof_torques_l2: -0.0538
         Episode_Reward/dof_acc_l2: -0.1209
     Episode_Reward/action_rate_l2: -0.0582
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2871
Metrics/base_velocity/error_vel_xy: 0.3963
Metrics/base_velocity/error_vel_yaw: 0.2857
      Episode_Termination/time_out: 0.5332
  Episode_Termination/base_contact: 0.4668
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 2.56s
                      Time elapsed: 00:36:00
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 850/1400 [0m                      

                       Computation: 9726 steps/s (collection: 2.269s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.0214
                       Mean reward: 9.50
               Mean episode length: 716.12
Episode_Reward/track_lin_vel_xy_exp: 0.5450
Episode_Reward/track_ang_vel_z_exp: 0.3059
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0460
     Episode_Reward/dof_torques_l2: -0.0591
         Episode_Reward/dof_acc_l2: -0.1311
     Episode_Reward/action_rate_l2: -0.0587
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3001
Metrics/base_velocity/error_vel_xy: 0.4249
Metrics/base_velocity/error_vel_yaw: 0.3039
      Episode_Termination/time_out: 0.5347
  Episode_Termination/base_contact: 0.4653
--------------------------------------------------------------------------------
                   Total timesteps: 20914176
                    Iteration time: 2.53s
                      Time elapsed: 00:36:03
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 851/1400 [0m                      

                       Computation: 9822 steps/s (collection: 2.236s, learning 0.266s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 6.0263
                       Mean reward: 9.60
               Mean episode length: 724.31
Episode_Reward/track_lin_vel_xy_exp: 0.4385
Episode_Reward/track_ang_vel_z_exp: 0.2582
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.0512
         Episode_Reward/dof_acc_l2: -0.1237
     Episode_Reward/action_rate_l2: -0.0505
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3046
Metrics/base_velocity/error_vel_xy: 0.4070
Metrics/base_velocity/error_vel_yaw: 0.2636
      Episode_Termination/time_out: 0.5295
  Episode_Termination/base_contact: 0.4705
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.50s
                      Time elapsed: 00:36:05
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 852/1400 [0m                      

                       Computation: 9680 steps/s (collection: 2.283s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 6.0182
                       Mean reward: 10.21
               Mean episode length: 765.84
Episode_Reward/track_lin_vel_xy_exp: 0.5622
Episode_Reward/track_ang_vel_z_exp: 0.3181
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0447
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1239
     Episode_Reward/action_rate_l2: -0.0605
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3134
Metrics/base_velocity/error_vel_xy: 0.4224
Metrics/base_velocity/error_vel_yaw: 0.2935
      Episode_Termination/time_out: 0.5310
  Episode_Termination/base_contact: 0.4690
--------------------------------------------------------------------------------
                   Total timesteps: 20963328
                    Iteration time: 2.54s
                      Time elapsed: 00:36:08
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 853/1400 [0m                      

                       Computation: 9651 steps/s (collection: 2.283s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.0147
                       Mean reward: 9.82
               Mean episode length: 741.55
Episode_Reward/track_lin_vel_xy_exp: 0.5199
Episode_Reward/track_ang_vel_z_exp: 0.3029
       Episode_Reward/lin_vel_z_l2: -0.0378
      Episode_Reward/ang_vel_xy_l2: -0.0447
     Episode_Reward/dof_torques_l2: -0.0561
         Episode_Reward/dof_acc_l2: -0.1240
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3110
Metrics/base_velocity/error_vel_xy: 0.4417
Metrics/base_velocity/error_vel_yaw: 0.2892
      Episode_Termination/time_out: 0.5323
  Episode_Termination/base_contact: 0.4677
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 2.55s
                      Time elapsed: 00:36:10
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 854/1400 [0m                      

                       Computation: 9533 steps/s (collection: 2.324s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 6.0141
                       Mean reward: 10.17
               Mean episode length: 744.81
Episode_Reward/track_lin_vel_xy_exp: 0.4815
Episode_Reward/track_ang_vel_z_exp: 0.2688
       Episode_Reward/lin_vel_z_l2: -0.0327
      Episode_Reward/ang_vel_xy_l2: -0.0392
     Episode_Reward/dof_torques_l2: -0.0480
         Episode_Reward/dof_acc_l2: -0.1089
     Episode_Reward/action_rate_l2: -0.0507
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3080
Metrics/base_velocity/error_vel_xy: 0.3595
Metrics/base_velocity/error_vel_yaw: 0.2511
      Episode_Termination/time_out: 0.5305
  Episode_Termination/base_contact: 0.4695
--------------------------------------------------------------------------------
                   Total timesteps: 21012480
                    Iteration time: 2.58s
                      Time elapsed: 00:36:13
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 855/1400 [0m                      

                       Computation: 9542 steps/s (collection: 2.318s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 6.0164
                       Mean reward: 9.56
               Mean episode length: 684.11
Episode_Reward/track_lin_vel_xy_exp: 0.5065
Episode_Reward/track_ang_vel_z_exp: 0.2792
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0406
     Episode_Reward/dof_torques_l2: -0.0476
         Episode_Reward/dof_acc_l2: -0.1108
     Episode_Reward/action_rate_l2: -0.0520
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3091
Metrics/base_velocity/error_vel_xy: 0.3468
Metrics/base_velocity/error_vel_yaw: 0.2491
      Episode_Termination/time_out: 0.5256
  Episode_Termination/base_contact: 0.4744
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.58s
                      Time elapsed: 00:36:15
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 856/1400 [0m                      

                       Computation: 9631 steps/s (collection: 2.297s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 6.0179
                       Mean reward: 9.77
               Mean episode length: 712.78
Episode_Reward/track_lin_vel_xy_exp: 0.6263
Episode_Reward/track_ang_vel_z_exp: 0.3578
       Episode_Reward/lin_vel_z_l2: -0.0372
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0616
         Episode_Reward/dof_acc_l2: -0.1426
     Episode_Reward/action_rate_l2: -0.0659
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3208
Metrics/base_velocity/error_vel_xy: 0.4605
Metrics/base_velocity/error_vel_yaw: 0.3014
      Episode_Termination/time_out: 0.5293
  Episode_Termination/base_contact: 0.4707
--------------------------------------------------------------------------------
                   Total timesteps: 21061632
                    Iteration time: 2.55s
                      Time elapsed: 00:36:18
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 857/1400 [0m                      

                       Computation: 9583 steps/s (collection: 2.301s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 6.0136
                       Mean reward: 9.67
               Mean episode length: 716.03
Episode_Reward/track_lin_vel_xy_exp: 0.5267
Episode_Reward/track_ang_vel_z_exp: 0.2989
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.0453
     Episode_Reward/dof_torques_l2: -0.0553
         Episode_Reward/dof_acc_l2: -0.1407
     Episode_Reward/action_rate_l2: -0.0579
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3352
Metrics/base_velocity/error_vel_xy: 0.4322
Metrics/base_velocity/error_vel_yaw: 0.2992
      Episode_Termination/time_out: 0.5320
  Episode_Termination/base_contact: 0.4680
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 2.56s
                      Time elapsed: 00:36:21
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 858/1400 [0m                      

                       Computation: 9548 steps/s (collection: 2.317s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 6.0080
                       Mean reward: 10.11
               Mean episode length: 762.58
Episode_Reward/track_lin_vel_xy_exp: 0.5369
Episode_Reward/track_ang_vel_z_exp: 0.3186
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0497
     Episode_Reward/dof_torques_l2: -0.0608
         Episode_Reward/dof_acc_l2: -0.1443
     Episode_Reward/action_rate_l2: -0.0622
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3434
Metrics/base_velocity/error_vel_xy: 0.5074
Metrics/base_velocity/error_vel_yaw: 0.3263
      Episode_Termination/time_out: 0.5322
  Episode_Termination/base_contact: 0.4678
--------------------------------------------------------------------------------
                   Total timesteps: 21110784
                    Iteration time: 2.57s
                      Time elapsed: 00:36:23
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 859/1400 [0m                      

                       Computation: 9751 steps/s (collection: 2.265s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 6.0048
                       Mean reward: 9.93
               Mean episode length: 756.23
Episode_Reward/track_lin_vel_xy_exp: 0.6211
Episode_Reward/track_ang_vel_z_exp: 0.3531
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0505
     Episode_Reward/dof_torques_l2: -0.0625
         Episode_Reward/dof_acc_l2: -0.1495
     Episode_Reward/action_rate_l2: -0.0669
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3542
Metrics/base_velocity/error_vel_xy: 0.4676
Metrics/base_velocity/error_vel_yaw: 0.3161
      Episode_Termination/time_out: 0.5318
  Episode_Termination/base_contact: 0.4682
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.52s
                      Time elapsed: 00:36:26
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 860/1400 [0m                      

                       Computation: 9628 steps/s (collection: 2.295s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 6.0195
                       Mean reward: 10.09
               Mean episode length: 756.61
Episode_Reward/track_lin_vel_xy_exp: 0.5281
Episode_Reward/track_ang_vel_z_exp: 0.3011
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0431
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.1301
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3663
Metrics/base_velocity/error_vel_xy: 0.4190
Metrics/base_velocity/error_vel_yaw: 0.2813
      Episode_Termination/time_out: 0.5344
  Episode_Termination/base_contact: 0.4656
--------------------------------------------------------------------------------
                   Total timesteps: 21159936
                    Iteration time: 2.55s
                      Time elapsed: 00:36:28
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 861/1400 [0m                      

                       Computation: 9517 steps/s (collection: 2.318s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 6.0117
                       Mean reward: 10.44
               Mean episode length: 760.70
Episode_Reward/track_lin_vel_xy_exp: 0.5250
Episode_Reward/track_ang_vel_z_exp: 0.2987
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0439
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1209
     Episode_Reward/action_rate_l2: -0.0569
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3696
Metrics/base_velocity/error_vel_xy: 0.4212
Metrics/base_velocity/error_vel_yaw: 0.2875
      Episode_Termination/time_out: 0.5386
  Episode_Termination/base_contact: 0.4614
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 2.58s
                      Time elapsed: 00:36:31
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 862/1400 [0m                      

                       Computation: 9626 steps/s (collection: 2.296s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.9887
                       Mean reward: 10.31
               Mean episode length: 751.40
Episode_Reward/track_lin_vel_xy_exp: 0.5548
Episode_Reward/track_ang_vel_z_exp: 0.3089
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0462
     Episode_Reward/dof_torques_l2: -0.0564
         Episode_Reward/dof_acc_l2: -0.1344
     Episode_Reward/action_rate_l2: -0.0590
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3783
Metrics/base_velocity/error_vel_xy: 0.4200
Metrics/base_velocity/error_vel_yaw: 0.3089
      Episode_Termination/time_out: 0.5407
  Episode_Termination/base_contact: 0.4593
--------------------------------------------------------------------------------
                   Total timesteps: 21209088
                    Iteration time: 2.55s
                      Time elapsed: 00:36:33
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 863/1400 [0m                      

                       Computation: 9618 steps/s (collection: 2.298s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.9782
                       Mean reward: 10.51
               Mean episode length: 756.14
Episode_Reward/track_lin_vel_xy_exp: 0.5267
Episode_Reward/track_ang_vel_z_exp: 0.3061
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0434
     Episode_Reward/dof_torques_l2: -0.0574
         Episode_Reward/dof_acc_l2: -0.1288
     Episode_Reward/action_rate_l2: -0.0591
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3767
Metrics/base_velocity/error_vel_xy: 0.4604
Metrics/base_velocity/error_vel_yaw: 0.3015
      Episode_Termination/time_out: 0.5408
  Episode_Termination/base_contact: 0.4592
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.55s
                      Time elapsed: 00:36:36
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 864/1400 [0m                      

                       Computation: 9575 steps/s (collection: 2.310s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 5.9740
                       Mean reward: 10.81
               Mean episode length: 783.22
Episode_Reward/track_lin_vel_xy_exp: 0.6571
Episode_Reward/track_ang_vel_z_exp: 0.3675
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0650
         Episode_Reward/dof_acc_l2: -0.1479
     Episode_Reward/action_rate_l2: -0.0702
      Episode_Reward/feet_air_time: -0.0100
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3773
Metrics/base_velocity/error_vel_xy: 0.4858
Metrics/base_velocity/error_vel_yaw: 0.3429
      Episode_Termination/time_out: 0.5399
  Episode_Termination/base_contact: 0.4601
--------------------------------------------------------------------------------
                   Total timesteps: 21258240
                    Iteration time: 2.57s
                      Time elapsed: 00:36:38
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 865/1400 [0m                      

                       Computation: 9634 steps/s (collection: 2.294s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.9785
                       Mean reward: 10.27
               Mean episode length: 758.02
Episode_Reward/track_lin_vel_xy_exp: 0.5298
Episode_Reward/track_ang_vel_z_exp: 0.3032
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1319
     Episode_Reward/action_rate_l2: -0.0586
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3826
Metrics/base_velocity/error_vel_xy: 0.4413
Metrics/base_velocity/error_vel_yaw: 0.3054
      Episode_Termination/time_out: 0.5383
  Episode_Termination/base_contact: 0.4617
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 2.55s
                      Time elapsed: 00:36:41
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 866/1400 [0m                      

                       Computation: 9628 steps/s (collection: 2.289s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 5.9708
                       Mean reward: 10.17
               Mean episode length: 777.20
Episode_Reward/track_lin_vel_xy_exp: 0.5714
Episode_Reward/track_ang_vel_z_exp: 0.3464
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.0511
     Episode_Reward/dof_torques_l2: -0.0644
         Episode_Reward/dof_acc_l2: -0.1466
     Episode_Reward/action_rate_l2: -0.0663
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0069
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3830
Metrics/base_velocity/error_vel_xy: 0.5399
Metrics/base_velocity/error_vel_yaw: 0.3211
      Episode_Termination/time_out: 0.5431
  Episode_Termination/base_contact: 0.4569
--------------------------------------------------------------------------------
                   Total timesteps: 21307392
                    Iteration time: 2.55s
                      Time elapsed: 00:36:44
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 867/1400 [0m                      

                       Computation: 9768 steps/s (collection: 2.252s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 5.9636
                       Mean reward: 9.88
               Mean episode length: 754.37
Episode_Reward/track_lin_vel_xy_exp: 0.5641
Episode_Reward/track_ang_vel_z_exp: 0.3111
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0418
     Episode_Reward/dof_torques_l2: -0.0562
         Episode_Reward/dof_acc_l2: -0.1315
     Episode_Reward/action_rate_l2: -0.0579
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3865
Metrics/base_velocity/error_vel_xy: 0.3715
Metrics/base_velocity/error_vel_yaw: 0.2663
      Episode_Termination/time_out: 0.5432
  Episode_Termination/base_contact: 0.4568
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.52s
                      Time elapsed: 00:36:46
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 868/1400 [0m                      

                       Computation: 9462 steps/s (collection: 2.340s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.9654
                       Mean reward: 9.93
               Mean episode length: 761.35
Episode_Reward/track_lin_vel_xy_exp: 0.5008
Episode_Reward/track_ang_vel_z_exp: 0.2893
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0449
     Episode_Reward/dof_torques_l2: -0.0529
         Episode_Reward/dof_acc_l2: -0.1226
     Episode_Reward/action_rate_l2: -0.0556
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0071
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3892
Metrics/base_velocity/error_vel_xy: 0.4376
Metrics/base_velocity/error_vel_yaw: 0.3019
      Episode_Termination/time_out: 0.5429
  Episode_Termination/base_contact: 0.4571
--------------------------------------------------------------------------------
                   Total timesteps: 21356544
                    Iteration time: 2.60s
                      Time elapsed: 00:36:49
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 869/1400 [0m                      

                       Computation: 9495 steps/s (collection: 2.327s, learning 0.261s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.9587
                       Mean reward: 9.56
               Mean episode length: 717.22
Episode_Reward/track_lin_vel_xy_exp: 0.5399
Episode_Reward/track_ang_vel_z_exp: 0.3013
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0561
         Episode_Reward/dof_acc_l2: -0.1264
     Episode_Reward/action_rate_l2: -0.0573
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3771
Metrics/base_velocity/error_vel_xy: 0.4007
Metrics/base_velocity/error_vel_yaw: 0.2911
      Episode_Termination/time_out: 0.5398
  Episode_Termination/base_contact: 0.4602
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 2.59s
                      Time elapsed: 00:36:51
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 870/1400 [0m                      

                       Computation: 9674 steps/s (collection: 2.276s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.9587
                       Mean reward: 8.91
               Mean episode length: 707.32
Episode_Reward/track_lin_vel_xy_exp: 0.4121
Episode_Reward/track_ang_vel_z_exp: 0.2461
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0397
     Episode_Reward/dof_torques_l2: -0.0498
         Episode_Reward/dof_acc_l2: -0.1148
     Episode_Reward/action_rate_l2: -0.0496
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0151
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3789
Metrics/base_velocity/error_vel_xy: 0.4129
Metrics/base_velocity/error_vel_yaw: 0.2771
      Episode_Termination/time_out: 0.5411
  Episode_Termination/base_contact: 0.4589
--------------------------------------------------------------------------------
                   Total timesteps: 21405696
                    Iteration time: 2.54s
                      Time elapsed: 00:36:54
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 871/1400 [0m                      

                       Computation: 9586 steps/s (collection: 2.307s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.9605
                       Mean reward: 8.72
               Mean episode length: 717.19
Episode_Reward/track_lin_vel_xy_exp: 0.4424
Episode_Reward/track_ang_vel_z_exp: 0.2868
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0573
         Episode_Reward/dof_acc_l2: -0.1363
     Episode_Reward/action_rate_l2: -0.0562
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0096
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3870
Metrics/base_velocity/error_vel_xy: 0.5704
Metrics/base_velocity/error_vel_yaw: 0.3253
      Episode_Termination/time_out: 0.5402
  Episode_Termination/base_contact: 0.4598
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.56s
                      Time elapsed: 00:36:56
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 872/1400 [0m                      

                       Computation: 9430 steps/s (collection: 2.350s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 5.9591
                       Mean reward: 10.04
               Mean episode length: 782.50
Episode_Reward/track_lin_vel_xy_exp: 0.6812
Episode_Reward/track_ang_vel_z_exp: 0.3792
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0498
     Episode_Reward/dof_torques_l2: -0.0652
         Episode_Reward/dof_acc_l2: -0.1439
     Episode_Reward/action_rate_l2: -0.0705
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3956
Metrics/base_velocity/error_vel_xy: 0.4630
Metrics/base_velocity/error_vel_yaw: 0.3255
      Episode_Termination/time_out: 0.5365
  Episode_Termination/base_contact: 0.4635
--------------------------------------------------------------------------------
                   Total timesteps: 21454848
                    Iteration time: 2.61s
                      Time elapsed: 00:36:59
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 873/1400 [0m                      

                       Computation: 9590 steps/s (collection: 2.299s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 5.9678
                       Mean reward: 10.13
               Mean episode length: 778.00
Episode_Reward/track_lin_vel_xy_exp: 0.4635
Episode_Reward/track_ang_vel_z_exp: 0.2693
       Episode_Reward/lin_vel_z_l2: -0.0353
      Episode_Reward/ang_vel_xy_l2: -0.0419
     Episode_Reward/dof_torques_l2: -0.0514
         Episode_Reward/dof_acc_l2: -0.1185
     Episode_Reward/action_rate_l2: -0.0521
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0106
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3978
Metrics/base_velocity/error_vel_xy: 0.4142
Metrics/base_velocity/error_vel_yaw: 0.2768
      Episode_Termination/time_out: 0.5375
  Episode_Termination/base_contact: 0.4625
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 2.56s
                      Time elapsed: 00:37:02
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 874/1400 [0m                      

                       Computation: 9542 steps/s (collection: 2.313s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.9749
                       Mean reward: 10.08
               Mean episode length: 777.14
Episode_Reward/track_lin_vel_xy_exp: 0.4926
Episode_Reward/track_ang_vel_z_exp: 0.3042
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.0499
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.1403
     Episode_Reward/action_rate_l2: -0.0615
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3975
Metrics/base_velocity/error_vel_xy: 0.5937
Metrics/base_velocity/error_vel_yaw: 0.3757
      Episode_Termination/time_out: 0.5411
  Episode_Termination/base_contact: 0.4589
--------------------------------------------------------------------------------
                   Total timesteps: 21504000
                    Iteration time: 2.58s
                      Time elapsed: 00:37:04
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 875/1400 [0m                      

                       Computation: 9784 steps/s (collection: 2.256s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 5.9870
                       Mean reward: 9.01
               Mean episode length: 721.12
Episode_Reward/track_lin_vel_xy_exp: 0.4574
Episode_Reward/track_ang_vel_z_exp: 0.2635
       Episode_Reward/lin_vel_z_l2: -0.0327
      Episode_Reward/ang_vel_xy_l2: -0.0395
     Episode_Reward/dof_torques_l2: -0.0531
         Episode_Reward/dof_acc_l2: -0.1171
     Episode_Reward/action_rate_l2: -0.0526
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0112
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3963
Metrics/base_velocity/error_vel_xy: 0.4156
Metrics/base_velocity/error_vel_yaw: 0.2963
      Episode_Termination/time_out: 0.5427
  Episode_Termination/base_contact: 0.4573
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.51s
                      Time elapsed: 00:37:07
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 876/1400 [0m                      

                       Computation: 9547 steps/s (collection: 2.318s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.9975
                       Mean reward: 9.62
               Mean episode length: 773.89
Episode_Reward/track_lin_vel_xy_exp: 0.5704
Episode_Reward/track_ang_vel_z_exp: 0.3360
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.0538
     Episode_Reward/dof_torques_l2: -0.0661
         Episode_Reward/dof_acc_l2: -0.1615
     Episode_Reward/action_rate_l2: -0.0674
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4045
Metrics/base_velocity/error_vel_xy: 0.5648
Metrics/base_velocity/error_vel_yaw: 0.3861
      Episode_Termination/time_out: 0.5503
  Episode_Termination/base_contact: 0.4497
--------------------------------------------------------------------------------
                   Total timesteps: 21553152
                    Iteration time: 2.57s
                      Time elapsed: 00:37:09
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 877/1400 [0m                      

                       Computation: 9484 steps/s (collection: 2.335s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.9955
                       Mean reward: 9.27
               Mean episode length: 722.08
Episode_Reward/track_lin_vel_xy_exp: 0.4472
Episode_Reward/track_ang_vel_z_exp: 0.2557
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0391
     Episode_Reward/dof_torques_l2: -0.0492
         Episode_Reward/dof_acc_l2: -0.1101
     Episode_Reward/action_rate_l2: -0.0494
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4132
Metrics/base_velocity/error_vel_xy: 0.3648
Metrics/base_velocity/error_vel_yaw: 0.2551
      Episode_Termination/time_out: 0.5514
  Episode_Termination/base_contact: 0.4486
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 2.59s
                      Time elapsed: 00:37:12
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 878/1400 [0m                      

                       Computation: 9679 steps/s (collection: 2.274s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.9943
                       Mean reward: 9.06
               Mean episode length: 701.57
Episode_Reward/track_lin_vel_xy_exp: 0.3675
Episode_Reward/track_ang_vel_z_exp: 0.2191
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0446
     Episode_Reward/dof_torques_l2: -0.0438
         Episode_Reward/dof_acc_l2: -0.1060
     Episode_Reward/action_rate_l2: -0.0418
      Episode_Reward/feet_air_time: -0.0061
 Episode_Reward/undesired_contacts: -0.0079
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4129
Metrics/base_velocity/error_vel_xy: 0.3703
Metrics/base_velocity/error_vel_yaw: 0.2483
      Episode_Termination/time_out: 0.5477
  Episode_Termination/base_contact: 0.4523
--------------------------------------------------------------------------------
                   Total timesteps: 21602304
                    Iteration time: 2.54s
                      Time elapsed: 00:37:14
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 879/1400 [0m                      

                       Computation: 9567 steps/s (collection: 2.306s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.9906
                       Mean reward: 8.15
               Mean episode length: 639.18
Episode_Reward/track_lin_vel_xy_exp: 0.4509
Episode_Reward/track_ang_vel_z_exp: 0.2647
       Episode_Reward/lin_vel_z_l2: -0.0328
      Episode_Reward/ang_vel_xy_l2: -0.0397
     Episode_Reward/dof_torques_l2: -0.0507
         Episode_Reward/dof_acc_l2: -0.1137
     Episode_Reward/action_rate_l2: -0.0512
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4083
Metrics/base_velocity/error_vel_xy: 0.4090
Metrics/base_velocity/error_vel_yaw: 0.2695
      Episode_Termination/time_out: 0.5430
  Episode_Termination/base_contact: 0.4570
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 2.57s
                      Time elapsed: 00:37:17
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 880/1400 [0m                      

                       Computation: 9482 steps/s (collection: 2.335s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 5.9964
                       Mean reward: 9.01
               Mean episode length: 692.70
Episode_Reward/track_lin_vel_xy_exp: 0.5290
Episode_Reward/track_ang_vel_z_exp: 0.3049
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.0461
     Episode_Reward/dof_torques_l2: -0.0563
         Episode_Reward/dof_acc_l2: -0.1268
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4076
Metrics/base_velocity/error_vel_xy: 0.4440
Metrics/base_velocity/error_vel_yaw: 0.2982
      Episode_Termination/time_out: 0.5414
  Episode_Termination/base_contact: 0.4586
--------------------------------------------------------------------------------
                   Total timesteps: 21651456
                    Iteration time: 2.59s
                      Time elapsed: 00:37:20
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 881/1400 [0m                      

                       Computation: 9596 steps/s (collection: 2.304s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 5.9986
                       Mean reward: 9.10
               Mean episode length: 684.26
Episode_Reward/track_lin_vel_xy_exp: 0.5537
Episode_Reward/track_ang_vel_z_exp: 0.3092
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0567
         Episode_Reward/dof_acc_l2: -0.1300
     Episode_Reward/action_rate_l2: -0.0584
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4083
Metrics/base_velocity/error_vel_xy: 0.4048
Metrics/base_velocity/error_vel_yaw: 0.2869
      Episode_Termination/time_out: 0.5467
  Episode_Termination/base_contact: 0.4533
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 2.56s
                      Time elapsed: 00:37:22
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 882/1400 [0m                      

                       Computation: 9843 steps/s (collection: 2.240s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.9912
                       Mean reward: 10.20
               Mean episode length: 751.89
Episode_Reward/track_lin_vel_xy_exp: 0.6468
Episode_Reward/track_ang_vel_z_exp: 0.3745
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0687
         Episode_Reward/dof_acc_l2: -0.1516
     Episode_Reward/action_rate_l2: -0.0696
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4179
Metrics/base_velocity/error_vel_xy: 0.5022
Metrics/base_velocity/error_vel_yaw: 0.3329
      Episode_Termination/time_out: 0.5476
  Episode_Termination/base_contact: 0.4524
--------------------------------------------------------------------------------
                   Total timesteps: 21700608
                    Iteration time: 2.50s
                      Time elapsed: 00:37:25
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 883/1400 [0m                      

                       Computation: 9483 steps/s (collection: 2.338s, learning 0.253s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.9852
                       Mean reward: 10.20
               Mean episode length: 762.20
Episode_Reward/track_lin_vel_xy_exp: 0.5065
Episode_Reward/track_ang_vel_z_exp: 0.2880
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.0457
     Episode_Reward/dof_torques_l2: -0.0538
         Episode_Reward/dof_acc_l2: -0.1252
     Episode_Reward/action_rate_l2: -0.0554
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0073
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4257
Metrics/base_velocity/error_vel_xy: 0.4297
Metrics/base_velocity/error_vel_yaw: 0.3026
      Episode_Termination/time_out: 0.5482
  Episode_Termination/base_contact: 0.4518
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.59s
                      Time elapsed: 00:37:27
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 884/1400 [0m                      

                       Computation: 9819 steps/s (collection: 2.250s, learning 0.253s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.9797
                       Mean reward: 10.50
               Mean episode length: 770.83
Episode_Reward/track_lin_vel_xy_exp: 0.5222
Episode_Reward/track_ang_vel_z_exp: 0.2959
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0413
     Episode_Reward/dof_torques_l2: -0.0561
         Episode_Reward/dof_acc_l2: -0.1244
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4291
Metrics/base_velocity/error_vel_xy: 0.4124
Metrics/base_velocity/error_vel_yaw: 0.2749
      Episode_Termination/time_out: 0.5507
  Episode_Termination/base_contact: 0.4493
--------------------------------------------------------------------------------
                   Total timesteps: 21749760
                    Iteration time: 2.50s
                      Time elapsed: 00:37:30
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 885/1400 [0m                      

                       Computation: 9489 steps/s (collection: 2.335s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.9710
                       Mean reward: 9.81
               Mean episode length: 746.18
Episode_Reward/track_lin_vel_xy_exp: 0.5336
Episode_Reward/track_ang_vel_z_exp: 0.3104
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0585
         Episode_Reward/dof_acc_l2: -0.1404
     Episode_Reward/action_rate_l2: -0.0596
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4417
Metrics/base_velocity/error_vel_xy: 0.4661
Metrics/base_velocity/error_vel_yaw: 0.3119
      Episode_Termination/time_out: 0.5497
  Episode_Termination/base_contact: 0.4503
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 2.59s
                      Time elapsed: 00:37:32
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 886/1400 [0m                      

                       Computation: 9594 steps/s (collection: 2.299s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 5.9689
                       Mean reward: 10.00
               Mean episode length: 743.68
Episode_Reward/track_lin_vel_xy_exp: 0.6355
Episode_Reward/track_ang_vel_z_exp: 0.3488
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.0508
     Episode_Reward/dof_torques_l2: -0.0633
         Episode_Reward/dof_acc_l2: -0.1401
     Episode_Reward/action_rate_l2: -0.0652
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4467
Metrics/base_velocity/error_vel_xy: 0.4301
Metrics/base_velocity/error_vel_yaw: 0.3208
      Episode_Termination/time_out: 0.5465
  Episode_Termination/base_contact: 0.4535
--------------------------------------------------------------------------------
                   Total timesteps: 21798912
                    Iteration time: 2.56s
                      Time elapsed: 00:37:35
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 887/1400 [0m                      

                       Computation: 9609 steps/s (collection: 2.293s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 5.9724
                       Mean reward: 9.38
               Mean episode length: 712.40
Episode_Reward/track_lin_vel_xy_exp: 0.4788
Episode_Reward/track_ang_vel_z_exp: 0.2755
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0436
     Episode_Reward/dof_torques_l2: -0.0547
         Episode_Reward/dof_acc_l2: -0.1214
     Episode_Reward/action_rate_l2: -0.0535
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4466
Metrics/base_velocity/error_vel_xy: 0.4261
Metrics/base_velocity/error_vel_yaw: 0.3002
      Episode_Termination/time_out: 0.5440
  Episode_Termination/base_contact: 0.4560
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.56s
                      Time elapsed: 00:37:37
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 888/1400 [0m                      

                       Computation: 9632 steps/s (collection: 2.293s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.9783
                       Mean reward: 9.40
               Mean episode length: 710.83
Episode_Reward/track_lin_vel_xy_exp: 0.5666
Episode_Reward/track_ang_vel_z_exp: 0.3239
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0647
         Episode_Reward/dof_acc_l2: -0.1447
     Episode_Reward/action_rate_l2: -0.0633
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4574
Metrics/base_velocity/error_vel_xy: 0.4836
Metrics/base_velocity/error_vel_yaw: 0.3376
      Episode_Termination/time_out: 0.5440
  Episode_Termination/base_contact: 0.4560
--------------------------------------------------------------------------------
                   Total timesteps: 21848064
                    Iteration time: 2.55s
                      Time elapsed: 00:37:40
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 889/1400 [0m                      

                       Computation: 9660 steps/s (collection: 2.286s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.9850
                       Mean reward: 9.28
               Mean episode length: 709.06
Episode_Reward/track_lin_vel_xy_exp: 0.5924
Episode_Reward/track_ang_vel_z_exp: 0.3297
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.0541
     Episode_Reward/dof_torques_l2: -0.0611
         Episode_Reward/dof_acc_l2: -0.1469
     Episode_Reward/action_rate_l2: -0.0633
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4652
Metrics/base_velocity/error_vel_xy: 0.4379
Metrics/base_velocity/error_vel_yaw: 0.3232
      Episode_Termination/time_out: 0.5474
  Episode_Termination/base_contact: 0.4526
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 2.54s
                      Time elapsed: 00:37:42
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 890/1400 [0m                      

                       Computation: 9612 steps/s (collection: 2.299s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.9988
                       Mean reward: 9.46
               Mean episode length: 722.47
Episode_Reward/track_lin_vel_xy_exp: 0.4903
Episode_Reward/track_ang_vel_z_exp: 0.2789
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.0446
     Episode_Reward/dof_torques_l2: -0.0564
         Episode_Reward/dof_acc_l2: -0.1306
     Episode_Reward/action_rate_l2: -0.0547
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4790
Metrics/base_velocity/error_vel_xy: 0.4208
Metrics/base_velocity/error_vel_yaw: 0.3086
      Episode_Termination/time_out: 0.5523
  Episode_Termination/base_contact: 0.4477
--------------------------------------------------------------------------------
                   Total timesteps: 21897216
                    Iteration time: 2.56s
                      Time elapsed: 00:37:45
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 891/1400 [0m                      

                       Computation: 9537 steps/s (collection: 2.320s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 6.0002
                       Mean reward: 9.41
               Mean episode length: 697.51
Episode_Reward/track_lin_vel_xy_exp: 0.4868
Episode_Reward/track_ang_vel_z_exp: 0.2677
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0400
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.1144
     Episode_Reward/action_rate_l2: -0.0514
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4840
Metrics/base_velocity/error_vel_xy: 0.3511
Metrics/base_velocity/error_vel_yaw: 0.2645
      Episode_Termination/time_out: 0.5516
  Episode_Termination/base_contact: 0.4484
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.58s
                      Time elapsed: 00:37:48
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 892/1400 [0m                      

                       Computation: 9523 steps/s (collection: 2.316s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 5.9859
                       Mean reward: 9.78
               Mean episode length: 700.87
Episode_Reward/track_lin_vel_xy_exp: 0.5573
Episode_Reward/track_ang_vel_z_exp: 0.2952
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0439
     Episode_Reward/dof_torques_l2: -0.0540
         Episode_Reward/dof_acc_l2: -0.1148
     Episode_Reward/action_rate_l2: -0.0559
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4865
Metrics/base_velocity/error_vel_xy: 0.3528
Metrics/base_velocity/error_vel_yaw: 0.2928
      Episode_Termination/time_out: 0.5485
  Episode_Termination/base_contact: 0.4515
--------------------------------------------------------------------------------
                   Total timesteps: 21946368
                    Iteration time: 2.58s
                      Time elapsed: 00:37:50
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 893/1400 [0m                      

                       Computation: 9571 steps/s (collection: 2.312s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.9746
                       Mean reward: 9.63
               Mean episode length: 703.67
Episode_Reward/track_lin_vel_xy_exp: 0.5392
Episode_Reward/track_ang_vel_z_exp: 0.3175
       Episode_Reward/lin_vel_z_l2: -0.0378
      Episode_Reward/ang_vel_xy_l2: -0.0453
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1309
     Episode_Reward/action_rate_l2: -0.0598
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4928
Metrics/base_velocity/error_vel_xy: 0.4700
Metrics/base_velocity/error_vel_yaw: 0.3073
      Episode_Termination/time_out: 0.5526
  Episode_Termination/base_contact: 0.4483
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 2.57s
                      Time elapsed: 00:37:53
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 894/1400 [0m                      

                       Computation: 9636 steps/s (collection: 2.295s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.9666
                       Mean reward: 10.64
               Mean episode length: 773.19
Episode_Reward/track_lin_vel_xy_exp: 0.6751
Episode_Reward/track_ang_vel_z_exp: 0.3764
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.0532
     Episode_Reward/dof_torques_l2: -0.0698
         Episode_Reward/dof_acc_l2: -0.1576
     Episode_Reward/action_rate_l2: -0.0707
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5036
Metrics/base_velocity/error_vel_xy: 0.4957
Metrics/base_velocity/error_vel_yaw: 0.3536
      Episode_Termination/time_out: 0.5566
  Episode_Termination/base_contact: 0.4443
--------------------------------------------------------------------------------
                   Total timesteps: 21995520
                    Iteration time: 2.55s
                      Time elapsed: 00:37:55
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 895/1400 [0m                      

                       Computation: 9621 steps/s (collection: 2.297s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.9653
                       Mean reward: 10.79
               Mean episode length: 775.03
Episode_Reward/track_lin_vel_xy_exp: 0.5538
Episode_Reward/track_ang_vel_z_exp: 0.3114
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0580
         Episode_Reward/dof_acc_l2: -0.1377
     Episode_Reward/action_rate_l2: -0.0595
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5038
Metrics/base_velocity/error_vel_xy: 0.4313
Metrics/base_velocity/error_vel_yaw: 0.3088
      Episode_Termination/time_out: 0.5600
  Episode_Termination/base_contact: 0.4410
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.55s
                      Time elapsed: 00:37:58
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 896/1400 [0m                      

                       Computation: 9701 steps/s (collection: 2.277s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.9625
                       Mean reward: 10.26
               Mean episode length: 761.01
Episode_Reward/track_lin_vel_xy_exp: 0.4449
Episode_Reward/track_ang_vel_z_exp: 0.2716
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.1142
     Episode_Reward/action_rate_l2: -0.0520
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0119
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5014
Metrics/base_velocity/error_vel_xy: 0.4837
Metrics/base_velocity/error_vel_yaw: 0.3042
      Episode_Termination/time_out: 0.5606
  Episode_Termination/base_contact: 0.4403
--------------------------------------------------------------------------------
                   Total timesteps: 22044672
                    Iteration time: 2.53s
                      Time elapsed: 00:38:00
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 897/1400 [0m                      

                       Computation: 9618 steps/s (collection: 2.296s, learning 0.259s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.9707
                       Mean reward: 9.91
               Mean episode length: 732.39
Episode_Reward/track_lin_vel_xy_exp: 0.5138
Episode_Reward/track_ang_vel_z_exp: 0.2824
       Episode_Reward/lin_vel_z_l2: -0.0412
      Episode_Reward/ang_vel_xy_l2: -0.0439
     Episode_Reward/dof_torques_l2: -0.0570
         Episode_Reward/dof_acc_l2: -0.1281
     Episode_Reward/action_rate_l2: -0.0545
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5037
Metrics/base_velocity/error_vel_xy: 0.3724
Metrics/base_velocity/error_vel_yaw: 0.2740
      Episode_Termination/time_out: 0.5656
  Episode_Termination/base_contact: 0.4354
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 2.55s
                      Time elapsed: 00:38:03
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 898/1400 [0m                      

                       Computation: 9602 steps/s (collection: 2.304s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 5.9707
                       Mean reward: 9.14
               Mean episode length: 719.42
Episode_Reward/track_lin_vel_xy_exp: 0.4753
Episode_Reward/track_ang_vel_z_exp: 0.2767
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0456
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.1323
     Episode_Reward/action_rate_l2: -0.0552
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5076
Metrics/base_velocity/error_vel_xy: 0.4552
Metrics/base_velocity/error_vel_yaw: 0.3175
      Episode_Termination/time_out: 0.5660
  Episode_Termination/base_contact: 0.4349
--------------------------------------------------------------------------------
                   Total timesteps: 22093824
                    Iteration time: 2.56s
                      Time elapsed: 00:38:06
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 899/1400 [0m                      

                       Computation: 9606 steps/s (collection: 2.294s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 5.9701
                       Mean reward: 9.34
               Mean episode length: 719.63
Episode_Reward/track_lin_vel_xy_exp: 0.5187
Episode_Reward/track_ang_vel_z_exp: 0.2777
       Episode_Reward/lin_vel_z_l2: -0.0324
      Episode_Reward/ang_vel_xy_l2: -0.0400
     Episode_Reward/dof_torques_l2: -0.0479
         Episode_Reward/dof_acc_l2: -0.1034
     Episode_Reward/action_rate_l2: -0.0518
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5117
Metrics/base_velocity/error_vel_xy: 0.3483
Metrics/base_velocity/error_vel_yaw: 0.2821
      Episode_Termination/time_out: 0.5638
  Episode_Termination/base_contact: 0.4372
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.56s
                      Time elapsed: 00:38:08
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 900/1400 [0m                      

                       Computation: 9580 steps/s (collection: 2.310s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.9636
                       Mean reward: 8.92
               Mean episode length: 701.33
Episode_Reward/track_lin_vel_xy_exp: 0.4408
Episode_Reward/track_ang_vel_z_exp: 0.2622
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0510
         Episode_Reward/dof_acc_l2: -0.1191
     Episode_Reward/action_rate_l2: -0.0507
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5152
Metrics/base_velocity/error_vel_xy: 0.4337
Metrics/base_velocity/error_vel_yaw: 0.2829
      Episode_Termination/time_out: 0.5639
  Episode_Termination/base_contact: 0.4371
--------------------------------------------------------------------------------
                   Total timesteps: 22142976
                    Iteration time: 2.57s
                      Time elapsed: 00:38:11
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 901/1400 [0m                      

                       Computation: 9503 steps/s (collection: 2.330s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 5.9538
                       Mean reward: 9.24
               Mean episode length: 705.13
Episode_Reward/track_lin_vel_xy_exp: 0.5186
Episode_Reward/track_ang_vel_z_exp: 0.2948
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0445
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1236
     Episode_Reward/action_rate_l2: -0.0558
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0072
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5212
Metrics/base_velocity/error_vel_xy: 0.4165
Metrics/base_velocity/error_vel_yaw: 0.2848
      Episode_Termination/time_out: 0.5635
  Episode_Termination/base_contact: 0.4375
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 2.59s
                      Time elapsed: 00:38:13
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 902/1400 [0m                      

                       Computation: 9474 steps/s (collection: 2.339s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.9438
                       Mean reward: 8.78
               Mean episode length: 682.84
Episode_Reward/track_lin_vel_xy_exp: 0.4411
Episode_Reward/track_ang_vel_z_exp: 0.2436
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.0450
         Episode_Reward/dof_acc_l2: -0.1094
     Episode_Reward/action_rate_l2: -0.0463
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0084
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5246
Metrics/base_velocity/error_vel_xy: 0.3433
Metrics/base_velocity/error_vel_yaw: 0.2618
      Episode_Termination/time_out: 0.5651
  Episode_Termination/base_contact: 0.4359
--------------------------------------------------------------------------------
                   Total timesteps: 22192128
                    Iteration time: 2.59s
                      Time elapsed: 00:38:16
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 903/1400 [0m                      

                       Computation: 9529 steps/s (collection: 2.323s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 5.9332
                       Mean reward: 9.46
               Mean episode length: 714.14
Episode_Reward/track_lin_vel_xy_exp: 0.6235
Episode_Reward/track_ang_vel_z_exp: 0.3381
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.1241
     Episode_Reward/action_rate_l2: -0.0615
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5289
Metrics/base_velocity/error_vel_xy: 0.3881
Metrics/base_velocity/error_vel_yaw: 0.2917
      Episode_Termination/time_out: 0.5643
  Episode_Termination/base_contact: 0.4367
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.58s
                      Time elapsed: 00:38:18
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 904/1400 [0m                      

                       Computation: 9471 steps/s (collection: 2.338s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.9251
                       Mean reward: 10.37
               Mean episode length: 738.00
Episode_Reward/track_lin_vel_xy_exp: 0.6187
Episode_Reward/track_ang_vel_z_exp: 0.3308
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0465
     Episode_Reward/dof_torques_l2: -0.0594
         Episode_Reward/dof_acc_l2: -0.1351
     Episode_Reward/action_rate_l2: -0.0624
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5354
Metrics/base_velocity/error_vel_xy: 0.3917
Metrics/base_velocity/error_vel_yaw: 0.3182
      Episode_Termination/time_out: 0.5663
  Episode_Termination/base_contact: 0.4347
--------------------------------------------------------------------------------
                   Total timesteps: 22241280
                    Iteration time: 2.59s
                      Time elapsed: 00:38:21
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 905/1400 [0m                      

                       Computation: 9564 steps/s (collection: 2.313s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.9085
                       Mean reward: 10.33
               Mean episode length: 744.44
Episode_Reward/track_lin_vel_xy_exp: 0.5018
Episode_Reward/track_ang_vel_z_exp: 0.2786
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0449
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.1372
     Episode_Reward/action_rate_l2: -0.0556
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5440
Metrics/base_velocity/error_vel_xy: 0.4122
Metrics/base_velocity/error_vel_yaw: 0.3068
      Episode_Termination/time_out: 0.5647
  Episode_Termination/base_contact: 0.4363
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 2.57s
                      Time elapsed: 00:38:24
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 906/1400 [0m                      

                       Computation: 9527 steps/s (collection: 2.323s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 5.8994
                       Mean reward: 10.13
               Mean episode length: 738.37
Episode_Reward/track_lin_vel_xy_exp: 0.5208
Episode_Reward/track_ang_vel_z_exp: 0.2890
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0445
     Episode_Reward/dof_torques_l2: -0.0527
         Episode_Reward/dof_acc_l2: -0.1215
     Episode_Reward/action_rate_l2: -0.0538
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5486
Metrics/base_velocity/error_vel_xy: 0.3813
Metrics/base_velocity/error_vel_yaw: 0.2720
      Episode_Termination/time_out: 0.5623
  Episode_Termination/base_contact: 0.4386
--------------------------------------------------------------------------------
                   Total timesteps: 22290432
                    Iteration time: 2.58s
                      Time elapsed: 00:38:26
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 907/1400 [0m                      

                       Computation: 9632 steps/s (collection: 2.294s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.8850
                       Mean reward: 9.90
               Mean episode length: 738.11
Episode_Reward/track_lin_vel_xy_exp: 0.6144
Episode_Reward/track_ang_vel_z_exp: 0.3348
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0486
     Episode_Reward/dof_torques_l2: -0.0565
         Episode_Reward/dof_acc_l2: -0.1327
     Episode_Reward/action_rate_l2: -0.0619
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5401
Metrics/base_velocity/error_vel_xy: 0.4156
Metrics/base_velocity/error_vel_yaw: 0.3086
      Episode_Termination/time_out: 0.5669
  Episode_Termination/base_contact: 0.4340
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.55s
                      Time elapsed: 00:38:29
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 908/1400 [0m                      

                       Computation: 9596 steps/s (collection: 2.304s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.8757
                       Mean reward: 10.25
               Mean episode length: 747.98
Episode_Reward/track_lin_vel_xy_exp: 0.5750
Episode_Reward/track_ang_vel_z_exp: 0.3165
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0480
     Episode_Reward/dof_torques_l2: -0.0616
         Episode_Reward/dof_acc_l2: -0.1391
     Episode_Reward/action_rate_l2: -0.0614
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5383
Metrics/base_velocity/error_vel_xy: 0.4456
Metrics/base_velocity/error_vel_yaw: 0.3368
      Episode_Termination/time_out: 0.5673
  Episode_Termination/base_contact: 0.4336
--------------------------------------------------------------------------------
                   Total timesteps: 22339584
                    Iteration time: 2.56s
                      Time elapsed: 00:38:31
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 909/1400 [0m                      

                       Computation: 9638 steps/s (collection: 2.286s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.8802
                       Mean reward: 9.96
               Mean episode length: 751.82
Episode_Reward/track_lin_vel_xy_exp: 0.5182
Episode_Reward/track_ang_vel_z_exp: 0.2910
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0444
     Episode_Reward/dof_torques_l2: -0.0520
         Episode_Reward/dof_acc_l2: -0.1190
     Episode_Reward/action_rate_l2: -0.0544
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5390
Metrics/base_velocity/error_vel_xy: 0.4002
Metrics/base_velocity/error_vel_yaw: 0.2800
      Episode_Termination/time_out: 0.5645
  Episode_Termination/base_contact: 0.4365
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 2.55s
                      Time elapsed: 00:38:34
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 910/1400 [0m                      

                       Computation: 9636 steps/s (collection: 2.294s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 5.8881
                       Mean reward: 9.72
               Mean episode length: 724.50
Episode_Reward/track_lin_vel_xy_exp: 0.4958
Episode_Reward/track_ang_vel_z_exp: 0.2747
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0415
     Episode_Reward/dof_torques_l2: -0.0504
         Episode_Reward/dof_acc_l2: -0.1187
     Episode_Reward/action_rate_l2: -0.0518
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5473
Metrics/base_velocity/error_vel_xy: 0.3742
Metrics/base_velocity/error_vel_yaw: 0.2778
      Episode_Termination/time_out: 0.5635
  Episode_Termination/base_contact: 0.4375
--------------------------------------------------------------------------------
                   Total timesteps: 22388736
                    Iteration time: 2.55s
                      Time elapsed: 00:38:36
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 911/1400 [0m                      

                       Computation: 9621 steps/s (collection: 2.293s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.8853
                       Mean reward: 9.07
               Mean episode length: 697.55
Episode_Reward/track_lin_vel_xy_exp: 0.4575
Episode_Reward/track_ang_vel_z_exp: 0.2639
       Episode_Reward/lin_vel_z_l2: -0.0526
      Episode_Reward/ang_vel_xy_l2: -0.0445
     Episode_Reward/dof_torques_l2: -0.0526
         Episode_Reward/dof_acc_l2: -0.1318
     Episode_Reward/action_rate_l2: -0.0518
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5552
Metrics/base_velocity/error_vel_xy: 0.4001
Metrics/base_velocity/error_vel_yaw: 0.2707
      Episode_Termination/time_out: 0.5682
  Episode_Termination/base_contact: 0.4328
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.55s
                      Time elapsed: 00:38:39
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 912/1400 [0m                      

                       Computation: 9493 steps/s (collection: 2.324s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 5.8875
                       Mean reward: 8.96
               Mean episode length: 698.76
Episode_Reward/track_lin_vel_xy_exp: 0.4925
Episode_Reward/track_ang_vel_z_exp: 0.2883
       Episode_Reward/lin_vel_z_l2: -0.0367
      Episode_Reward/ang_vel_xy_l2: -0.0453
     Episode_Reward/dof_torques_l2: -0.0594
         Episode_Reward/dof_acc_l2: -0.1366
     Episode_Reward/action_rate_l2: -0.0559
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5535
Metrics/base_velocity/error_vel_xy: 0.4477
Metrics/base_velocity/error_vel_yaw: 0.2963
      Episode_Termination/time_out: 0.5669
  Episode_Termination/base_contact: 0.4341
--------------------------------------------------------------------------------
                   Total timesteps: 22437888
                    Iteration time: 2.59s
                      Time elapsed: 00:38:41
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 913/1400 [0m                      

                       Computation: 9572 steps/s (collection: 2.311s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.8817
                       Mean reward: 9.08
               Mean episode length: 719.37
Episode_Reward/track_lin_vel_xy_exp: 0.5082
Episode_Reward/track_ang_vel_z_exp: 0.3041
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1336
     Episode_Reward/action_rate_l2: -0.0581
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5528
Metrics/base_velocity/error_vel_xy: 0.5009
Metrics/base_velocity/error_vel_yaw: 0.3187
      Episode_Termination/time_out: 0.5659
  Episode_Termination/base_contact: 0.4351
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 2.57s
                      Time elapsed: 00:38:44
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 914/1400 [0m                      

                       Computation: 9425 steps/s (collection: 2.350s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 5.8758
                       Mean reward: 9.48
               Mean episode length: 744.17
Episode_Reward/track_lin_vel_xy_exp: 0.5891
Episode_Reward/track_ang_vel_z_exp: 0.3329
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0622
         Episode_Reward/dof_acc_l2: -0.1388
     Episode_Reward/action_rate_l2: -0.0619
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5542
Metrics/base_velocity/error_vel_xy: 0.4434
Metrics/base_velocity/error_vel_yaw: 0.3045
      Episode_Termination/time_out: 0.5612
  Episode_Termination/base_contact: 0.4398
--------------------------------------------------------------------------------
                   Total timesteps: 22487040
                    Iteration time: 2.61s
                      Time elapsed: 00:38:47
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 915/1400 [0m                      

                       Computation: 9439 steps/s (collection: 2.347s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 5.8778
                       Mean reward: 10.45
               Mean episode length: 788.13
Episode_Reward/track_lin_vel_xy_exp: 0.5988
Episode_Reward/track_ang_vel_z_exp: 0.3552
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0512
     Episode_Reward/dof_torques_l2: -0.0675
         Episode_Reward/dof_acc_l2: -0.1398
     Episode_Reward/action_rate_l2: -0.0672
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5617
Metrics/base_velocity/error_vel_xy: 0.5586
Metrics/base_velocity/error_vel_yaw: 0.3531
      Episode_Termination/time_out: 0.5631
  Episode_Termination/base_contact: 0.4379
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.60s
                      Time elapsed: 00:38:49
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 916/1400 [0m                      

                       Computation: 9427 steps/s (collection: 2.350s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.8804
                       Mean reward: 9.91
               Mean episode length: 758.75
Episode_Reward/track_lin_vel_xy_exp: 0.4042
Episode_Reward/track_ang_vel_z_exp: 0.2331
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0401
     Episode_Reward/dof_torques_l2: -0.0474
         Episode_Reward/dof_acc_l2: -0.1119
     Episode_Reward/action_rate_l2: -0.0456
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0073
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5585
Metrics/base_velocity/error_vel_xy: 0.3892
Metrics/base_velocity/error_vel_yaw: 0.2746
      Episode_Termination/time_out: 0.5638
  Episode_Termination/base_contact: 0.4372
--------------------------------------------------------------------------------
                   Total timesteps: 22536192
                    Iteration time: 2.61s
                      Time elapsed: 00:38:52
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 917/1400 [0m                      

                       Computation: 9599 steps/s (collection: 2.300s, learning 0.260s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 5.8890
                       Mean reward: 9.83
               Mean episode length: 776.80
Episode_Reward/track_lin_vel_xy_exp: 0.4925
Episode_Reward/track_ang_vel_z_exp: 0.2987
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1298
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0075
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5586
Metrics/base_velocity/error_vel_xy: 0.5384
Metrics/base_velocity/error_vel_yaw: 0.3440
      Episode_Termination/time_out: 0.5634
  Episode_Termination/base_contact: 0.4375
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 2.56s
                      Time elapsed: 00:38:54
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 918/1400 [0m                      

                       Computation: 9519 steps/s (collection: 2.323s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 5.8942
                       Mean reward: 8.71
               Mean episode length: 713.75
Episode_Reward/track_lin_vel_xy_exp: 0.4414
Episode_Reward/track_ang_vel_z_exp: 0.2521
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.0399
     Episode_Reward/dof_torques_l2: -0.0492
         Episode_Reward/dof_acc_l2: -0.1087
     Episode_Reward/action_rate_l2: -0.0485
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5616
Metrics/base_velocity/error_vel_xy: 0.3791
Metrics/base_velocity/error_vel_yaw: 0.2714
      Episode_Termination/time_out: 0.5612
  Episode_Termination/base_contact: 0.4398
--------------------------------------------------------------------------------
                   Total timesteps: 22585344
                    Iteration time: 2.58s
                      Time elapsed: 00:38:57
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 919/1400 [0m                      

                       Computation: 9494 steps/s (collection: 2.333s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.8882
                       Mean reward: 9.28
               Mean episode length: 745.82
Episode_Reward/track_lin_vel_xy_exp: 0.5606
Episode_Reward/track_ang_vel_z_exp: 0.3215
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.0533
     Episode_Reward/dof_torques_l2: -0.0638
         Episode_Reward/dof_acc_l2: -0.1461
     Episode_Reward/action_rate_l2: -0.0624
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0137
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5622
Metrics/base_velocity/error_vel_xy: 0.5286
Metrics/base_velocity/error_vel_yaw: 0.3720
      Episode_Termination/time_out: 0.5623
  Episode_Termination/base_contact: 0.4386
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.59s
                      Time elapsed: 00:39:00
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 920/1400 [0m                      

                       Computation: 9485 steps/s (collection: 2.328s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.8879
                       Mean reward: 9.16
               Mean episode length: 737.22
Episode_Reward/track_lin_vel_xy_exp: 0.5287
Episode_Reward/track_ang_vel_z_exp: 0.3073
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.0486
     Episode_Reward/dof_torques_l2: -0.0576
         Episode_Reward/dof_acc_l2: -0.1372
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5662
Metrics/base_velocity/error_vel_xy: 0.4654
Metrics/base_velocity/error_vel_yaw: 0.3017
      Episode_Termination/time_out: 0.5649
  Episode_Termination/base_contact: 0.4360
--------------------------------------------------------------------------------
                   Total timesteps: 22634496
                    Iteration time: 2.59s
                      Time elapsed: 00:39:02
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 921/1400 [0m                      

                       Computation: 9584 steps/s (collection: 2.309s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 5.8842
                       Mean reward: 10.11
               Mean episode length: 765.38
Episode_Reward/track_lin_vel_xy_exp: 0.6295
Episode_Reward/track_ang_vel_z_exp: 0.3544
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.0510
     Episode_Reward/dof_torques_l2: -0.0642
         Episode_Reward/dof_acc_l2: -0.1347
     Episode_Reward/action_rate_l2: -0.0652
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5752
Metrics/base_velocity/error_vel_xy: 0.4834
Metrics/base_velocity/error_vel_yaw: 0.3341
      Episode_Termination/time_out: 0.5701
  Episode_Termination/base_contact: 0.4308
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 2.56s
                      Time elapsed: 00:39:05
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 922/1400 [0m                      

                       Computation: 9627 steps/s (collection: 2.287s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.8762
                       Mean reward: 11.09
               Mean episode length: 795.42
Episode_Reward/track_lin_vel_xy_exp: 0.6251
Episode_Reward/track_ang_vel_z_exp: 0.3499
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0606
         Episode_Reward/dof_acc_l2: -0.1368
     Episode_Reward/action_rate_l2: -0.0641
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0067
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5821
Metrics/base_velocity/error_vel_xy: 0.4658
Metrics/base_velocity/error_vel_yaw: 0.3198
      Episode_Termination/time_out: 0.5699
  Episode_Termination/base_contact: 0.4310
--------------------------------------------------------------------------------
                   Total timesteps: 22683648
                    Iteration time: 2.55s
                      Time elapsed: 00:39:07
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 923/1400 [0m                      

                       Computation: 9511 steps/s (collection: 2.325s, learning 0.259s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.8588
                       Mean reward: 11.58
               Mean episode length: 820.32
Episode_Reward/track_lin_vel_xy_exp: 0.5821
Episode_Reward/track_ang_vel_z_exp: 0.3262
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0478
     Episode_Reward/dof_torques_l2: -0.0608
         Episode_Reward/dof_acc_l2: -0.1341
     Episode_Reward/action_rate_l2: -0.0620
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0112
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5853
Metrics/base_velocity/error_vel_xy: 0.4536
Metrics/base_velocity/error_vel_yaw: 0.3256
      Episode_Termination/time_out: 0.5703
  Episode_Termination/base_contact: 0.4307
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.58s
                      Time elapsed: 00:39:10
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 924/1400 [0m                      

                       Computation: 9632 steps/s (collection: 2.294s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 5.8646
                       Mean reward: 11.08
               Mean episode length: 775.62
Episode_Reward/track_lin_vel_xy_exp: 0.5051
Episode_Reward/track_ang_vel_z_exp: 0.2842
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.0465
     Episode_Reward/dof_torques_l2: -0.0514
         Episode_Reward/dof_acc_l2: -0.1136
     Episode_Reward/action_rate_l2: -0.0523
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5860
Metrics/base_velocity/error_vel_xy: 0.3885
Metrics/base_velocity/error_vel_yaw: 0.2721
      Episode_Termination/time_out: 0.5691
  Episode_Termination/base_contact: 0.4319
--------------------------------------------------------------------------------
                   Total timesteps: 22732800
                    Iteration time: 2.55s
                      Time elapsed: 00:39:12
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 925/1400 [0m                      

                       Computation: 9521 steps/s (collection: 2.317s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.8668
                       Mean reward: 10.45
               Mean episode length: 767.94
Episode_Reward/track_lin_vel_xy_exp: 0.5295
Episode_Reward/track_ang_vel_z_exp: 0.3025
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0453
     Episode_Reward/dof_torques_l2: -0.0578
         Episode_Reward/dof_acc_l2: -0.1202
     Episode_Reward/action_rate_l2: -0.0576
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5842
Metrics/base_velocity/error_vel_xy: 0.4404
Metrics/base_velocity/error_vel_yaw: 0.3037
      Episode_Termination/time_out: 0.5661
  Episode_Termination/base_contact: 0.4349
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 2.58s
                      Time elapsed: 00:39:15
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 926/1400 [0m                      

                       Computation: 9534 steps/s (collection: 2.313s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.8644
                       Mean reward: 9.73
               Mean episode length: 700.82
Episode_Reward/track_lin_vel_xy_exp: 0.5021
Episode_Reward/track_ang_vel_z_exp: 0.2769
       Episode_Reward/lin_vel_z_l2: -0.0342
      Episode_Reward/ang_vel_xy_l2: -0.0412
     Episode_Reward/dof_torques_l2: -0.0497
         Episode_Reward/dof_acc_l2: -0.1160
     Episode_Reward/action_rate_l2: -0.0522
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5830
Metrics/base_velocity/error_vel_xy: 0.3673
Metrics/base_velocity/error_vel_yaw: 0.2750
      Episode_Termination/time_out: 0.5629
  Episode_Termination/base_contact: 0.4381
--------------------------------------------------------------------------------
                   Total timesteps: 22781952
                    Iteration time: 2.58s
                      Time elapsed: 00:39:18
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 927/1400 [0m                      

                       Computation: 9588 steps/s (collection: 2.302s, learning 0.261s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.8698
                       Mean reward: 9.37
               Mean episode length: 701.08
Episode_Reward/track_lin_vel_xy_exp: 0.5473
Episode_Reward/track_ang_vel_z_exp: 0.3029
       Episode_Reward/lin_vel_z_l2: -0.0321
      Episode_Reward/ang_vel_xy_l2: -0.0446
     Episode_Reward/dof_torques_l2: -0.0541
         Episode_Reward/dof_acc_l2: -0.1177
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5795
Metrics/base_velocity/error_vel_xy: 0.3877
Metrics/base_velocity/error_vel_yaw: 0.2774
      Episode_Termination/time_out: 0.5608
  Episode_Termination/base_contact: 0.4397
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.56s
                      Time elapsed: 00:39:20
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 928/1400 [0m                      

                       Computation: 9567 steps/s (collection: 2.310s, learning 0.259s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.8612
                       Mean reward: 9.24
               Mean episode length: 681.94
Episode_Reward/track_lin_vel_xy_exp: 0.5208
Episode_Reward/track_ang_vel_z_exp: 0.2891
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.0443
     Episode_Reward/dof_torques_l2: -0.0516
         Episode_Reward/dof_acc_l2: -0.1226
     Episode_Reward/action_rate_l2: -0.0536
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0085
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5760
Metrics/base_velocity/error_vel_xy: 0.3775
Metrics/base_velocity/error_vel_yaw: 0.2760
      Episode_Termination/time_out: 0.5611
  Episode_Termination/base_contact: 0.4389
--------------------------------------------------------------------------------
                   Total timesteps: 22831104
                    Iteration time: 2.57s
                      Time elapsed: 00:39:23
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 929/1400 [0m                      

                       Computation: 9399 steps/s (collection: 2.360s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.8722
                       Mean reward: 9.50
               Mean episode length: 712.21
Episode_Reward/track_lin_vel_xy_exp: 0.4858
Episode_Reward/track_ang_vel_z_exp: 0.2717
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.0427
     Episode_Reward/dof_torques_l2: -0.0507
         Episode_Reward/dof_acc_l2: -0.1174
     Episode_Reward/action_rate_l2: -0.0521
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0069
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5826
Metrics/base_velocity/error_vel_xy: 0.3898
Metrics/base_velocity/error_vel_yaw: 0.2891
      Episode_Termination/time_out: 0.5573
  Episode_Termination/base_contact: 0.4427
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 2.61s
                      Time elapsed: 00:39:25
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 930/1400 [0m                      

                       Computation: 9476 steps/s (collection: 2.337s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 5.8745
                       Mean reward: 9.91
               Mean episode length: 730.10
Episode_Reward/track_lin_vel_xy_exp: 0.5585
Episode_Reward/track_ang_vel_z_exp: 0.3140
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0547
         Episode_Reward/dof_acc_l2: -0.1280
     Episode_Reward/action_rate_l2: -0.0577
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5845
Metrics/base_velocity/error_vel_xy: 0.4205
Metrics/base_velocity/error_vel_yaw: 0.3034
      Episode_Termination/time_out: 0.5483
  Episode_Termination/base_contact: 0.4517
--------------------------------------------------------------------------------
                   Total timesteps: 22880256
                    Iteration time: 2.59s
                      Time elapsed: 00:39:28
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 931/1400 [0m                      

                       Computation: 9487 steps/s (collection: 2.334s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.8678
                       Mean reward: 9.18
               Mean episode length: 711.22
Episode_Reward/track_lin_vel_xy_exp: 0.4660
Episode_Reward/track_ang_vel_z_exp: 0.2769
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0462
     Episode_Reward/dof_torques_l2: -0.0546
         Episode_Reward/dof_acc_l2: -0.1351
     Episode_Reward/action_rate_l2: -0.0528
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5794
Metrics/base_velocity/error_vel_xy: 0.4362
Metrics/base_velocity/error_vel_yaw: 0.2864
      Episode_Termination/time_out: 0.5465
  Episode_Termination/base_contact: 0.4535
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.59s
                      Time elapsed: 00:39:31
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 932/1400 [0m                      

                       Computation: 9603 steps/s (collection: 2.296s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 5.8582
                       Mean reward: 9.79
               Mean episode length: 742.87
Episode_Reward/track_lin_vel_xy_exp: 0.5989
Episode_Reward/track_ang_vel_z_exp: 0.3286
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0616
         Episode_Reward/dof_acc_l2: -0.1401
     Episode_Reward/action_rate_l2: -0.0618
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5826
Metrics/base_velocity/error_vel_xy: 0.4388
Metrics/base_velocity/error_vel_yaw: 0.3271
      Episode_Termination/time_out: 0.5459
  Episode_Termination/base_contact: 0.4541
--------------------------------------------------------------------------------
                   Total timesteps: 22929408
                    Iteration time: 2.56s
                      Time elapsed: 00:39:33
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 933/1400 [0m                      

                       Computation: 9658 steps/s (collection: 2.288s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.8444
                       Mean reward: 10.24
               Mean episode length: 765.57
Episode_Reward/track_lin_vel_xy_exp: 0.6387
Episode_Reward/track_ang_vel_z_exp: 0.3488
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.0496
     Episode_Reward/dof_torques_l2: -0.0621
         Episode_Reward/dof_acc_l2: -0.1457
     Episode_Reward/action_rate_l2: -0.0638
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5946
Metrics/base_velocity/error_vel_xy: 0.4196
Metrics/base_velocity/error_vel_yaw: 0.3116
      Episode_Termination/time_out: 0.5484
  Episode_Termination/base_contact: 0.4516
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 2.54s
                      Time elapsed: 00:39:36
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 934/1400 [0m                      

                       Computation: 9577 steps/s (collection: 2.301s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 5.8393
                       Mean reward: 10.73
               Mean episode length: 772.66
Episode_Reward/track_lin_vel_xy_exp: 0.4864
Episode_Reward/track_ang_vel_z_exp: 0.2695
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.0379
     Episode_Reward/dof_torques_l2: -0.0509
         Episode_Reward/dof_acc_l2: -0.1135
     Episode_Reward/action_rate_l2: -0.0501
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5894
Metrics/base_velocity/error_vel_xy: 0.3483
Metrics/base_velocity/error_vel_yaw: 0.2486
      Episode_Termination/time_out: 0.5486
  Episode_Termination/base_contact: 0.4514
--------------------------------------------------------------------------------
                   Total timesteps: 22978560
                    Iteration time: 2.57s
                      Time elapsed: 00:39:38
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 935/1400 [0m                      

                       Computation: 9685 steps/s (collection: 2.282s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.8355
                       Mean reward: 10.65
               Mean episode length: 745.22
Episode_Reward/track_lin_vel_xy_exp: 0.5009
Episode_Reward/track_ang_vel_z_exp: 0.2900
       Episode_Reward/lin_vel_z_l2: -0.0327
      Episode_Reward/ang_vel_xy_l2: -0.0425
     Episode_Reward/dof_torques_l2: -0.0514
         Episode_Reward/dof_acc_l2: -0.1109
     Episode_Reward/action_rate_l2: -0.0539
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5843
Metrics/base_velocity/error_vel_xy: 0.4302
Metrics/base_velocity/error_vel_yaw: 0.2911
      Episode_Termination/time_out: 0.5478
  Episode_Termination/base_contact: 0.4522
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.54s
                      Time elapsed: 00:39:41
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 936/1400 [0m                      

                       Computation: 9650 steps/s (collection: 2.289s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.8315
                       Mean reward: 9.92
               Mean episode length: 708.51
Episode_Reward/track_lin_vel_xy_exp: 0.4885
Episode_Reward/track_ang_vel_z_exp: 0.2809
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.0443
     Episode_Reward/dof_torques_l2: -0.0528
         Episode_Reward/dof_acc_l2: -0.1202
     Episode_Reward/action_rate_l2: -0.0530
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5839
Metrics/base_velocity/error_vel_xy: 0.4213
Metrics/base_velocity/error_vel_yaw: 0.2830
      Episode_Termination/time_out: 0.5464
  Episode_Termination/base_contact: 0.4536
--------------------------------------------------------------------------------
                   Total timesteps: 23027712
                    Iteration time: 2.55s
                      Time elapsed: 00:39:43
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 937/1400 [0m                      

                       Computation: 9549 steps/s (collection: 2.307s, learning 0.266s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 5.8225
                       Mean reward: 9.96
               Mean episode length: 721.65
Episode_Reward/track_lin_vel_xy_exp: 0.4995
Episode_Reward/track_ang_vel_z_exp: 0.2906
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0551
         Episode_Reward/dof_acc_l2: -0.1245
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5870
Metrics/base_velocity/error_vel_xy: 0.4532
Metrics/base_velocity/error_vel_yaw: 0.3082
      Episode_Termination/time_out: 0.5427
  Episode_Termination/base_contact: 0.4573
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 2.57s
                      Time elapsed: 00:39:46
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 938/1400 [0m                      

                       Computation: 9634 steps/s (collection: 2.293s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.8153
                       Mean reward: 9.80
               Mean episode length: 722.53
Episode_Reward/track_lin_vel_xy_exp: 0.5239
Episode_Reward/track_ang_vel_z_exp: 0.3103
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.0473
     Episode_Reward/dof_torques_l2: -0.0543
         Episode_Reward/dof_acc_l2: -0.1308
     Episode_Reward/action_rate_l2: -0.0568
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5887
Metrics/base_velocity/error_vel_xy: 0.4562
Metrics/base_velocity/error_vel_yaw: 0.2768
      Episode_Termination/time_out: 0.5423
  Episode_Termination/base_contact: 0.4577
--------------------------------------------------------------------------------
                   Total timesteps: 23076864
                    Iteration time: 2.55s
                      Time elapsed: 00:39:48
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 939/1400 [0m                      

                       Computation: 9700 steps/s (collection: 2.279s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 5.8110
                       Mean reward: 10.01
               Mean episode length: 718.35
Episode_Reward/track_lin_vel_xy_exp: 0.5561
Episode_Reward/track_ang_vel_z_exp: 0.2985
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.0401
     Episode_Reward/dof_torques_l2: -0.0493
         Episode_Reward/dof_acc_l2: -0.1046
     Episode_Reward/action_rate_l2: -0.0537
      Episode_Reward/feet_air_time: -0.0059
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5849
Metrics/base_velocity/error_vel_xy: 0.3291
Metrics/base_velocity/error_vel_yaw: 0.2484
      Episode_Termination/time_out: 0.5412
  Episode_Termination/base_contact: 0.4588
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.53s
                      Time elapsed: 00:39:51
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 940/1400 [0m                      

                       Computation: 9562 steps/s (collection: 2.314s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0201
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.8168
                       Mean reward: 9.74
               Mean episode length: 696.84
Episode_Reward/track_lin_vel_xy_exp: 0.5297
Episode_Reward/track_ang_vel_z_exp: 0.2954
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.0520
         Episode_Reward/dof_acc_l2: -0.1240
     Episode_Reward/action_rate_l2: -0.0542
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5877
Metrics/base_velocity/error_vel_xy: 0.3832
Metrics/base_velocity/error_vel_yaw: 0.2800
      Episode_Termination/time_out: 0.5405
  Episode_Termination/base_contact: 0.4595
--------------------------------------------------------------------------------
                   Total timesteps: 23126016
                    Iteration time: 2.57s
                      Time elapsed: 00:39:54
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 941/1400 [0m                      

                       Computation: 9675 steps/s (collection: 2.285s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 5.8203
                       Mean reward: 10.11
               Mean episode length: 713.48
Episode_Reward/track_lin_vel_xy_exp: 0.5318
Episode_Reward/track_ang_vel_z_exp: 0.3115
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0577
         Episode_Reward/dof_acc_l2: -0.1287
     Episode_Reward/action_rate_l2: -0.0573
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0097
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5959
Metrics/base_velocity/error_vel_xy: 0.4549
Metrics/base_velocity/error_vel_yaw: 0.2936
      Episode_Termination/time_out: 0.5372
  Episode_Termination/base_contact: 0.4628
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 2.54s
                      Time elapsed: 00:39:56
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 942/1400 [0m                      

                       Computation: 9430 steps/s (collection: 2.349s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.8081
                       Mean reward: 9.43
               Mean episode length: 704.78
Episode_Reward/track_lin_vel_xy_exp: 0.4739
Episode_Reward/track_ang_vel_z_exp: 0.2533
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0406
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.1114
     Episode_Reward/action_rate_l2: -0.0481
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0069
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6005
Metrics/base_velocity/error_vel_xy: 0.3271
Metrics/base_velocity/error_vel_yaw: 0.2617
      Episode_Termination/time_out: 0.5363
  Episode_Termination/base_contact: 0.4637
--------------------------------------------------------------------------------
                   Total timesteps: 23175168
                    Iteration time: 2.61s
                      Time elapsed: 00:39:59
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 943/1400 [0m                      

                       Computation: 9627 steps/s (collection: 2.291s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.7941
                       Mean reward: 9.49
               Mean episode length: 718.69
Episode_Reward/track_lin_vel_xy_exp: 0.5537
Episode_Reward/track_ang_vel_z_exp: 0.3140
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0565
         Episode_Reward/dof_acc_l2: -0.1345
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5926
Metrics/base_velocity/error_vel_xy: 0.4544
Metrics/base_velocity/error_vel_yaw: 0.3180
      Episode_Termination/time_out: 0.5328
  Episode_Termination/base_contact: 0.4672
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.55s
                      Time elapsed: 00:40:01
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 944/1400 [0m                      

                       Computation: 9541 steps/s (collection: 2.311s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0188
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.7886
                       Mean reward: 10.09
               Mean episode length: 742.31
Episode_Reward/track_lin_vel_xy_exp: 0.6276
Episode_Reward/track_ang_vel_z_exp: 0.3461
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0505
     Episode_Reward/dof_torques_l2: -0.0584
         Episode_Reward/dof_acc_l2: -0.1386
     Episode_Reward/action_rate_l2: -0.0631
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5958
Metrics/base_velocity/error_vel_xy: 0.4323
Metrics/base_velocity/error_vel_yaw: 0.3139
      Episode_Termination/time_out: 0.5373
  Episode_Termination/base_contact: 0.4627
--------------------------------------------------------------------------------
                   Total timesteps: 23224320
                    Iteration time: 2.58s
                      Time elapsed: 00:40:04
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 945/1400 [0m                      

                       Computation: 9552 steps/s (collection: 2.317s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7893
                       Mean reward: 10.07
               Mean episode length: 735.79
Episode_Reward/track_lin_vel_xy_exp: 0.4700
Episode_Reward/track_ang_vel_z_exp: 0.2685
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.0403
     Episode_Reward/dof_torques_l2: -0.0489
         Episode_Reward/dof_acc_l2: -0.1082
     Episode_Reward/action_rate_l2: -0.0496
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5988
Metrics/base_velocity/error_vel_xy: 0.3839
Metrics/base_velocity/error_vel_yaw: 0.2627
      Episode_Termination/time_out: 0.5352
  Episode_Termination/base_contact: 0.4648
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 2.57s
                      Time elapsed: 00:40:06
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 946/1400 [0m                      

                       Computation: 9648 steps/s (collection: 2.289s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.7960
                       Mean reward: 9.83
               Mean episode length: 726.10
Episode_Reward/track_lin_vel_xy_exp: 0.5738
Episode_Reward/track_ang_vel_z_exp: 0.3363
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0516
     Episode_Reward/dof_torques_l2: -0.0660
         Episode_Reward/dof_acc_l2: -0.1498
     Episode_Reward/action_rate_l2: -0.0639
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5968
Metrics/base_velocity/error_vel_xy: 0.5296
Metrics/base_velocity/error_vel_yaw: 0.3496
      Episode_Termination/time_out: 0.5280
  Episode_Termination/base_contact: 0.4720
--------------------------------------------------------------------------------
                   Total timesteps: 23273472
                    Iteration time: 2.55s
                      Time elapsed: 00:40:09
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 947/1400 [0m                      

                       Computation: 9568 steps/s (collection: 2.309s, learning 0.259s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.7980
                       Mean reward: 8.79
               Mean episode length: 681.88
Episode_Reward/track_lin_vel_xy_exp: 0.4883
Episode_Reward/track_ang_vel_z_exp: 0.2728
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0450
     Episode_Reward/dof_torques_l2: -0.0543
         Episode_Reward/dof_acc_l2: -0.1252
     Episode_Reward/action_rate_l2: -0.0532
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5962
Metrics/base_velocity/error_vel_xy: 0.4137
Metrics/base_velocity/error_vel_yaw: 0.3114
      Episode_Termination/time_out: 0.5252
  Episode_Termination/base_contact: 0.4748
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.57s
                      Time elapsed: 00:40:11
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 948/1400 [0m                      

                       Computation: 9581 steps/s (collection: 2.307s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7998
                       Mean reward: 9.96
               Mean episode length: 741.67
Episode_Reward/track_lin_vel_xy_exp: 0.6325
Episode_Reward/track_ang_vel_z_exp: 0.3434
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0512
     Episode_Reward/dof_torques_l2: -0.0602
         Episode_Reward/dof_acc_l2: -0.1342
     Episode_Reward/action_rate_l2: -0.0646
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5996
Metrics/base_velocity/error_vel_xy: 0.4281
Metrics/base_velocity/error_vel_yaw: 0.3294
      Episode_Termination/time_out: 0.5248
  Episode_Termination/base_contact: 0.4752
--------------------------------------------------------------------------------
                   Total timesteps: 23322624
                    Iteration time: 2.56s
                      Time elapsed: 00:40:14
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 949/1400 [0m                      

                       Computation: 9596 steps/s (collection: 2.303s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 5.8230
                       Mean reward: 10.22
               Mean episode length: 747.01
Episode_Reward/track_lin_vel_xy_exp: 0.5716
Episode_Reward/track_ang_vel_z_exp: 0.3177
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0511
     Episode_Reward/dof_torques_l2: -0.0575
         Episode_Reward/dof_acc_l2: -0.1375
     Episode_Reward/action_rate_l2: -0.0593
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5997
Metrics/base_velocity/error_vel_xy: 0.4376
Metrics/base_velocity/error_vel_yaw: 0.3185
      Episode_Termination/time_out: 0.5225
  Episode_Termination/base_contact: 0.4775
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 2.56s
                      Time elapsed: 00:40:17
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 950/1400 [0m                      

                       Computation: 9636 steps/s (collection: 2.295s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.8206
                       Mean reward: 11.72
               Mean episode length: 813.14
Episode_Reward/track_lin_vel_xy_exp: 0.5945
Episode_Reward/track_ang_vel_z_exp: 0.3291
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.1279
     Episode_Reward/action_rate_l2: -0.0600
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0080
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6033
Metrics/base_velocity/error_vel_xy: 0.4230
Metrics/base_velocity/error_vel_yaw: 0.3003
      Episode_Termination/time_out: 0.5239
  Episode_Termination/base_contact: 0.4761
--------------------------------------------------------------------------------
                   Total timesteps: 23371776
                    Iteration time: 2.55s
                      Time elapsed: 00:40:19
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 951/1400 [0m                      

                       Computation: 9684 steps/s (collection: 2.281s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 5.8122
                       Mean reward: 10.78
               Mean episode length: 790.29
Episode_Reward/track_lin_vel_xy_exp: 0.5397
Episode_Reward/track_ang_vel_z_exp: 0.3285
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.0575
     Episode_Reward/dof_torques_l2: -0.0664
         Episode_Reward/dof_acc_l2: -0.1638
     Episode_Reward/action_rate_l2: -0.0648
      Episode_Reward/feet_air_time: -0.0104
 Episode_Reward/undesired_contacts: -0.0083
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6153
Metrics/base_velocity/error_vel_xy: 0.5896
Metrics/base_velocity/error_vel_yaw: 0.3815
      Episode_Termination/time_out: 0.5306
  Episode_Termination/base_contact: 0.4694
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.54s
                      Time elapsed: 00:40:22
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 952/1400 [0m                      

                       Computation: 9504 steps/s (collection: 2.328s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7986
                       Mean reward: 10.30
               Mean episode length: 787.30
Episode_Reward/track_lin_vel_xy_exp: 0.4938
Episode_Reward/track_ang_vel_z_exp: 0.2824
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.0558
         Episode_Reward/dof_acc_l2: -0.1270
     Episode_Reward/action_rate_l2: -0.0550
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0110
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6213
Metrics/base_velocity/error_vel_xy: 0.4596
Metrics/base_velocity/error_vel_yaw: 0.3382
      Episode_Termination/time_out: 0.5350
  Episode_Termination/base_contact: 0.4650
--------------------------------------------------------------------------------
                   Total timesteps: 23420928
                    Iteration time: 2.59s
                      Time elapsed: 00:40:24
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 953/1400 [0m                      

                       Computation: 9657 steps/s (collection: 2.283s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.7897
                       Mean reward: 9.84
               Mean episode length: 762.47
Episode_Reward/track_lin_vel_xy_exp: 0.5002
Episode_Reward/track_ang_vel_z_exp: 0.2698
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.0463
     Episode_Reward/dof_torques_l2: -0.0484
         Episode_Reward/dof_acc_l2: -0.1182
     Episode_Reward/action_rate_l2: -0.0505
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0143
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6259
Metrics/base_velocity/error_vel_xy: 0.3543
Metrics/base_velocity/error_vel_yaw: 0.2761
      Episode_Termination/time_out: 0.5356
  Episode_Termination/base_contact: 0.4644
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 2.54s
                      Time elapsed: 00:40:27
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 954/1400 [0m                      

                       Computation: 9566 steps/s (collection: 2.311s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.7980
                       Mean reward: 9.52
               Mean episode length: 734.30
Episode_Reward/track_lin_vel_xy_exp: 0.5770
Episode_Reward/track_ang_vel_z_exp: 0.3318
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0510
     Episode_Reward/dof_torques_l2: -0.0673
         Episode_Reward/dof_acc_l2: -0.1497
     Episode_Reward/action_rate_l2: -0.0624
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6298
Metrics/base_velocity/error_vel_xy: 0.4584
Metrics/base_velocity/error_vel_yaw: 0.3117
      Episode_Termination/time_out: 0.5416
  Episode_Termination/base_contact: 0.4584
--------------------------------------------------------------------------------
                   Total timesteps: 23470080
                    Iteration time: 2.57s
                      Time elapsed: 00:40:29
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 955/1400 [0m                      

                       Computation: 9486 steps/s (collection: 2.333s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 5.8067
                       Mean reward: 9.73
               Mean episode length: 749.94
Episode_Reward/track_lin_vel_xy_exp: 0.5917
Episode_Reward/track_ang_vel_z_exp: 0.3430
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.0530
     Episode_Reward/dof_torques_l2: -0.0610
         Episode_Reward/dof_acc_l2: -0.1419
     Episode_Reward/action_rate_l2: -0.0634
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0090
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6247
Metrics/base_velocity/error_vel_xy: 0.5054
Metrics/base_velocity/error_vel_yaw: 0.3418
      Episode_Termination/time_out: 0.5431
  Episode_Termination/base_contact: 0.4569
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.59s
                      Time elapsed: 00:40:32
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 956/1400 [0m                      

                       Computation: 9522 steps/s (collection: 2.323s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 5.8094
                       Mean reward: 10.07
               Mean episode length: 781.54
Episode_Reward/track_lin_vel_xy_exp: 0.5125
Episode_Reward/track_ang_vel_z_exp: 0.2935
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1205
     Episode_Reward/action_rate_l2: -0.0546
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6142
Metrics/base_velocity/error_vel_xy: 0.4452
Metrics/base_velocity/error_vel_yaw: 0.3077
      Episode_Termination/time_out: 0.5494
  Episode_Termination/base_contact: 0.4506
--------------------------------------------------------------------------------
                   Total timesteps: 23519232
                    Iteration time: 2.58s
                      Time elapsed: 00:40:35
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 957/1400 [0m                      

                       Computation: 9571 steps/s (collection: 2.311s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.8075
                       Mean reward: 10.22
               Mean episode length: 799.33
Episode_Reward/track_lin_vel_xy_exp: 0.5968
Episode_Reward/track_ang_vel_z_exp: 0.3389
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.0534
     Episode_Reward/dof_torques_l2: -0.0678
         Episode_Reward/dof_acc_l2: -0.1606
     Episode_Reward/action_rate_l2: -0.0657
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6167
Metrics/base_velocity/error_vel_xy: 0.5129
Metrics/base_velocity/error_vel_yaw: 0.3699
      Episode_Termination/time_out: 0.5496
  Episode_Termination/base_contact: 0.4504
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 2.57s
                      Time elapsed: 00:40:37
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 958/1400 [0m                      

                       Computation: 9628 steps/s (collection: 2.296s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 5.8055
                       Mean reward: 9.65
               Mean episode length: 760.96
Episode_Reward/track_lin_vel_xy_exp: 0.5221
Episode_Reward/track_ang_vel_z_exp: 0.2913
       Episode_Reward/lin_vel_z_l2: -0.0422
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0552
         Episode_Reward/dof_acc_l2: -0.1263
     Episode_Reward/action_rate_l2: -0.0544
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0072
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6157
Metrics/base_velocity/error_vel_xy: 0.4049
Metrics/base_velocity/error_vel_yaw: 0.2981
      Episode_Termination/time_out: 0.5506
  Episode_Termination/base_contact: 0.4494
--------------------------------------------------------------------------------
                   Total timesteps: 23568384
                    Iteration time: 2.55s
                      Time elapsed: 00:40:40
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 959/1400 [0m                      

                       Computation: 9616 steps/s (collection: 2.293s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.8029
                       Mean reward: 9.26
               Mean episode length: 731.48
Episode_Reward/track_lin_vel_xy_exp: 0.4947
Episode_Reward/track_ang_vel_z_exp: 0.2754
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0451
     Episode_Reward/dof_torques_l2: -0.0518
         Episode_Reward/dof_acc_l2: -0.1209
     Episode_Reward/action_rate_l2: -0.0517
      Episode_Reward/feet_air_time: -0.0070
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6158
Metrics/base_velocity/error_vel_xy: 0.3820
Metrics/base_velocity/error_vel_yaw: 0.2836
      Episode_Termination/time_out: 0.5478
  Episode_Termination/base_contact: 0.4522
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.56s
                      Time elapsed: 00:40:42
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 960/1400 [0m                      

                       Computation: 9608 steps/s (collection: 2.301s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7996
                       Mean reward: 9.11
               Mean episode length: 706.18
Episode_Reward/track_lin_vel_xy_exp: 0.4597
Episode_Reward/track_ang_vel_z_exp: 0.2698
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0458
     Episode_Reward/dof_torques_l2: -0.0527
         Episode_Reward/dof_acc_l2: -0.1166
     Episode_Reward/action_rate_l2: -0.0512
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6195
Metrics/base_velocity/error_vel_xy: 0.4345
Metrics/base_velocity/error_vel_yaw: 0.2920
      Episode_Termination/time_out: 0.5492
  Episode_Termination/base_contact: 0.4508
--------------------------------------------------------------------------------
                   Total timesteps: 23617536
                    Iteration time: 2.56s
                      Time elapsed: 00:40:45
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 961/1400 [0m                      

                       Computation: 9590 steps/s (collection: 2.308s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.7790
                       Mean reward: 10.22
               Mean episode length: 764.91
Episode_Reward/track_lin_vel_xy_exp: 0.5421
Episode_Reward/track_ang_vel_z_exp: 0.3050
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.0503
     Episode_Reward/dof_torques_l2: -0.0570
         Episode_Reward/dof_acc_l2: -0.1324
     Episode_Reward/action_rate_l2: -0.0579
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6230
Metrics/base_velocity/error_vel_xy: 0.4646
Metrics/base_velocity/error_vel_yaw: 0.3317
      Episode_Termination/time_out: 0.5549
  Episode_Termination/base_contact: 0.4451
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 2.56s
                      Time elapsed: 00:40:47
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 962/1400 [0m                      

                       Computation: 9611 steps/s (collection: 2.293s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.7771
                       Mean reward: 10.34
               Mean episode length: 770.86
Episode_Reward/track_lin_vel_xy_exp: 0.5776
Episode_Reward/track_ang_vel_z_exp: 0.3378
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.0517
     Episode_Reward/dof_torques_l2: -0.0650
         Episode_Reward/dof_acc_l2: -0.1427
     Episode_Reward/action_rate_l2: -0.0630
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0071
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6360
Metrics/base_velocity/error_vel_xy: 0.5036
Metrics/base_velocity/error_vel_yaw: 0.3318
      Episode_Termination/time_out: 0.5578
  Episode_Termination/base_contact: 0.4422
--------------------------------------------------------------------------------
                   Total timesteps: 23666688
                    Iteration time: 2.56s
                      Time elapsed: 00:40:50
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 963/1400 [0m                      

                       Computation: 9583 steps/s (collection: 2.307s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7899
                       Mean reward: 10.61
               Mean episode length: 775.07
Episode_Reward/track_lin_vel_xy_exp: 0.5252
Episode_Reward/track_ang_vel_z_exp: 0.2929
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0461
     Episode_Reward/dof_torques_l2: -0.0518
         Episode_Reward/dof_acc_l2: -0.1241
     Episode_Reward/action_rate_l2: -0.0533
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6431
Metrics/base_velocity/error_vel_xy: 0.3854
Metrics/base_velocity/error_vel_yaw: 0.2772
      Episode_Termination/time_out: 0.5581
  Episode_Termination/base_contact: 0.4419
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.56s
                      Time elapsed: 00:40:52
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 964/1400 [0m                      

                       Computation: 9664 steps/s (collection: 2.285s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 5.8015
                       Mean reward: 10.55
               Mean episode length: 756.69
Episode_Reward/track_lin_vel_xy_exp: 0.6096
Episode_Reward/track_ang_vel_z_exp: 0.3318
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0577
         Episode_Reward/dof_acc_l2: -0.1378
     Episode_Reward/action_rate_l2: -0.0602
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6503
Metrics/base_velocity/error_vel_xy: 0.4052
Metrics/base_velocity/error_vel_yaw: 0.2973
      Episode_Termination/time_out: 0.5569
  Episode_Termination/base_contact: 0.4431
--------------------------------------------------------------------------------
                   Total timesteps: 23715840
                    Iteration time: 2.54s
                      Time elapsed: 00:40:55
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 965/1400 [0m                      

                       Computation: 9519 steps/s (collection: 2.324s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 5.8060
                       Mean reward: 10.54
               Mean episode length: 755.70
Episode_Reward/track_lin_vel_xy_exp: 0.5691
Episode_Reward/track_ang_vel_z_exp: 0.3245
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0590
         Episode_Reward/dof_acc_l2: -0.1369
     Episode_Reward/action_rate_l2: -0.0592
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6473
Metrics/base_velocity/error_vel_xy: 0.4290
Metrics/base_velocity/error_vel_yaw: 0.2927
      Episode_Termination/time_out: 0.5608
  Episode_Termination/base_contact: 0.4392
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 2.58s
                      Time elapsed: 00:40:58
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 966/1400 [0m                      

                       Computation: 9546 steps/s (collection: 2.310s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 5.8094
                       Mean reward: 10.47
               Mean episode length: 744.34
Episode_Reward/track_lin_vel_xy_exp: 0.5689
Episode_Reward/track_ang_vel_z_exp: 0.3087
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.0462
     Episode_Reward/dof_torques_l2: -0.0542
         Episode_Reward/dof_acc_l2: -0.1189
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6390
Metrics/base_velocity/error_vel_xy: 0.3694
Metrics/base_velocity/error_vel_yaw: 0.2791
      Episode_Termination/time_out: 0.5583
  Episode_Termination/base_contact: 0.4417
--------------------------------------------------------------------------------
                   Total timesteps: 23764992
                    Iteration time: 2.57s
                      Time elapsed: 00:41:00
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 967/1400 [0m                      

                       Computation: 9389 steps/s (collection: 2.361s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.8132
                       Mean reward: 9.90
               Mean episode length: 738.61
Episode_Reward/track_lin_vel_xy_exp: 0.5666
Episode_Reward/track_ang_vel_z_exp: 0.3100
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1282
     Episode_Reward/action_rate_l2: -0.0576
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0103
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6457
Metrics/base_velocity/error_vel_xy: 0.4139
Metrics/base_velocity/error_vel_yaw: 0.3206
      Episode_Termination/time_out: 0.5621
  Episode_Termination/base_contact: 0.4379
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.62s
                      Time elapsed: 00:41:03
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 968/1400 [0m                      

                       Computation: 9641 steps/s (collection: 2.287s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.8218
                       Mean reward: 9.62
               Mean episode length: 721.69
Episode_Reward/track_lin_vel_xy_exp: 0.5540
Episode_Reward/track_ang_vel_z_exp: 0.3119
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1330
     Episode_Reward/action_rate_l2: -0.0596
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6595
Metrics/base_velocity/error_vel_xy: 0.4454
Metrics/base_velocity/error_vel_yaw: 0.3251
      Episode_Termination/time_out: 0.5660
  Episode_Termination/base_contact: 0.4340
--------------------------------------------------------------------------------
                   Total timesteps: 23814144
                    Iteration time: 2.55s
                      Time elapsed: 00:41:05
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 969/1400 [0m                      

                       Computation: 9577 steps/s (collection: 2.311s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.8180
                       Mean reward: 10.06
               Mean episode length: 740.96
Episode_Reward/track_lin_vel_xy_exp: 0.5804
Episode_Reward/track_ang_vel_z_exp: 0.3162
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0591
         Episode_Reward/dof_acc_l2: -0.1258
     Episode_Reward/action_rate_l2: -0.0583
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6750
Metrics/base_velocity/error_vel_xy: 0.3921
Metrics/base_velocity/error_vel_yaw: 0.2879
      Episode_Termination/time_out: 0.5665
  Episode_Termination/base_contact: 0.4335
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 2.57s
                      Time elapsed: 00:41:08
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 970/1400 [0m                      

                       Computation: 9667 steps/s (collection: 2.279s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.8195
                       Mean reward: 10.87
               Mean episode length: 778.29
Episode_Reward/track_lin_vel_xy_exp: 0.6527
Episode_Reward/track_ang_vel_z_exp: 0.3609
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.0515
     Episode_Reward/dof_torques_l2: -0.0631
         Episode_Reward/dof_acc_l2: -0.1440
     Episode_Reward/action_rate_l2: -0.0654
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6862
Metrics/base_velocity/error_vel_xy: 0.4610
Metrics/base_velocity/error_vel_yaw: 0.3416
      Episode_Termination/time_out: 0.5697
  Episode_Termination/base_contact: 0.4303
--------------------------------------------------------------------------------
                   Total timesteps: 23863296
                    Iteration time: 2.54s
                      Time elapsed: 00:41:10
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 971/1400 [0m                      

                       Computation: 9568 steps/s (collection: 2.314s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.8147
                       Mean reward: 10.69
               Mean episode length: 791.44
Episode_Reward/track_lin_vel_xy_exp: 0.5015
Episode_Reward/track_ang_vel_z_exp: 0.2999
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0506
     Episode_Reward/dof_torques_l2: -0.0617
         Episode_Reward/dof_acc_l2: -0.1380
     Episode_Reward/action_rate_l2: -0.0576
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0162
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6905
Metrics/base_velocity/error_vel_xy: 0.5207
Metrics/base_velocity/error_vel_yaw: 0.3370
      Episode_Termination/time_out: 0.5693
  Episode_Termination/base_contact: 0.4307
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.57s
                      Time elapsed: 00:41:13
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 972/1400 [0m                      

                       Computation: 9562 steps/s (collection: 2.315s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.7961
                       Mean reward: 9.63
               Mean episode length: 757.29
Episode_Reward/track_lin_vel_xy_exp: 0.5255
Episode_Reward/track_ang_vel_z_exp: 0.2974
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0581
         Episode_Reward/dof_acc_l2: -0.1307
     Episode_Reward/action_rate_l2: -0.0559
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6944
Metrics/base_velocity/error_vel_xy: 0.4412
Metrics/base_velocity/error_vel_yaw: 0.3146
      Episode_Termination/time_out: 0.5738
  Episode_Termination/base_contact: 0.4262
--------------------------------------------------------------------------------
                   Total timesteps: 23912448
                    Iteration time: 2.57s
                      Time elapsed: 00:41:16
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 973/1400 [0m                      

                       Computation: 9706 steps/s (collection: 2.268s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 5.7920
                       Mean reward: 8.89
               Mean episode length: 723.95
Episode_Reward/track_lin_vel_xy_exp: 0.5034
Episode_Reward/track_ang_vel_z_exp: 0.2935
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0487
     Episode_Reward/dof_torques_l2: -0.0598
         Episode_Reward/dof_acc_l2: -0.1390
     Episode_Reward/action_rate_l2: -0.0575
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6960
Metrics/base_velocity/error_vel_xy: 0.4792
Metrics/base_velocity/error_vel_yaw: 0.3367
      Episode_Termination/time_out: 0.5705
  Episode_Termination/base_contact: 0.4295
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 2.53s
                      Time elapsed: 00:41:18
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 974/1400 [0m                      

                       Computation: 9510 steps/s (collection: 2.327s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.7913
                       Mean reward: 9.48
               Mean episode length: 738.42
Episode_Reward/track_lin_vel_xy_exp: 0.5367
Episode_Reward/track_ang_vel_z_exp: 0.3190
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.0550
     Episode_Reward/dof_torques_l2: -0.0593
         Episode_Reward/dof_acc_l2: -0.1453
     Episode_Reward/action_rate_l2: -0.0601
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6942
Metrics/base_velocity/error_vel_xy: 0.5090
Metrics/base_velocity/error_vel_yaw: 0.3283
      Episode_Termination/time_out: 0.5743
  Episode_Termination/base_contact: 0.4257
--------------------------------------------------------------------------------
                   Total timesteps: 23961600
                    Iteration time: 2.58s
                      Time elapsed: 00:41:21
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 975/1400 [0m                      

                       Computation: 9499 steps/s (collection: 2.326s, learning 0.261s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0117
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.7877
                       Mean reward: 9.94
               Mean episode length: 743.03
Episode_Reward/track_lin_vel_xy_exp: 0.5096
Episode_Reward/track_ang_vel_z_exp: 0.2790
       Episode_Reward/lin_vel_z_l2: -0.0294
      Episode_Reward/ang_vel_xy_l2: -0.0408
     Episode_Reward/dof_torques_l2: -0.0481
         Episode_Reward/dof_acc_l2: -0.1004
     Episode_Reward/action_rate_l2: -0.0501
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6947
Metrics/base_velocity/error_vel_xy: 0.3442
Metrics/base_velocity/error_vel_yaw: 0.2576
      Episode_Termination/time_out: 0.5766
  Episode_Termination/base_contact: 0.4234
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.59s
                      Time elapsed: 00:41:23
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 976/1400 [0m                      

                       Computation: 9551 steps/s (collection: 2.318s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.7758
                       Mean reward: 11.04
               Mean episode length: 785.20
Episode_Reward/track_lin_vel_xy_exp: 0.6171
Episode_Reward/track_ang_vel_z_exp: 0.3458
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0496
     Episode_Reward/dof_torques_l2: -0.0650
         Episode_Reward/dof_acc_l2: -0.1445
     Episode_Reward/action_rate_l2: -0.0638
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6991
Metrics/base_velocity/error_vel_xy: 0.4526
Metrics/base_velocity/error_vel_yaw: 0.3203
      Episode_Termination/time_out: 0.5815
  Episode_Termination/base_contact: 0.4185
--------------------------------------------------------------------------------
                   Total timesteps: 24010752
                    Iteration time: 2.57s
                      Time elapsed: 00:41:26
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 977/1400 [0m                      

                       Computation: 9576 steps/s (collection: 2.311s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.7797
                       Mean reward: 11.31
               Mean episode length: 774.76
Episode_Reward/track_lin_vel_xy_exp: 0.5937
Episode_Reward/track_ang_vel_z_exp: 0.3203
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0465
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1282
     Episode_Reward/action_rate_l2: -0.0590
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7057
Metrics/base_velocity/error_vel_xy: 0.3763
Metrics/base_velocity/error_vel_yaw: 0.2879
      Episode_Termination/time_out: 0.5843
  Episode_Termination/base_contact: 0.4157
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 2.57s
                      Time elapsed: 00:41:28
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 978/1400 [0m                      

                       Computation: 9565 steps/s (collection: 2.305s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.7915
                       Mean reward: 11.78
               Mean episode length: 809.55
Episode_Reward/track_lin_vel_xy_exp: 0.6770
Episode_Reward/track_ang_vel_z_exp: 0.3691
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0666
         Episode_Reward/dof_acc_l2: -0.1607
     Episode_Reward/action_rate_l2: -0.0669
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7167
Metrics/base_velocity/error_vel_xy: 0.4567
Metrics/base_velocity/error_vel_yaw: 0.3401
      Episode_Termination/time_out: 0.5885
  Episode_Termination/base_contact: 0.4115
--------------------------------------------------------------------------------
                   Total timesteps: 24059904
                    Iteration time: 2.57s
                      Time elapsed: 00:41:31
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 979/1400 [0m                      

                       Computation: 9640 steps/s (collection: 2.292s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7965
                       Mean reward: 11.35
               Mean episode length: 787.92
Episode_Reward/track_lin_vel_xy_exp: 0.4859
Episode_Reward/track_ang_vel_z_exp: 0.2701
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0438
     Episode_Reward/dof_torques_l2: -0.0489
         Episode_Reward/dof_acc_l2: -0.1184
     Episode_Reward/action_rate_l2: -0.0507
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7236
Metrics/base_velocity/error_vel_xy: 0.3899
Metrics/base_velocity/error_vel_yaw: 0.2910
      Episode_Termination/time_out: 0.5867
  Episode_Termination/base_contact: 0.4133
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.55s
                      Time elapsed: 00:41:34
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 980/1400 [0m                      

                       Computation: 9612 steps/s (collection: 2.301s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 5.7988
                       Mean reward: 9.59
               Mean episode length: 713.25
Episode_Reward/track_lin_vel_xy_exp: 0.4471
Episode_Reward/track_ang_vel_z_exp: 0.2496
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0493
         Episode_Reward/dof_acc_l2: -0.1238
     Episode_Reward/action_rate_l2: -0.0492
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0104
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7228
Metrics/base_velocity/error_vel_xy: 0.3951
Metrics/base_velocity/error_vel_yaw: 0.3051
      Episode_Termination/time_out: 0.5826
  Episode_Termination/base_contact: 0.4174
--------------------------------------------------------------------------------
                   Total timesteps: 24109056
                    Iteration time: 2.56s
                      Time elapsed: 00:41:36
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 981/1400 [0m                      

                       Computation: 9658 steps/s (collection: 2.286s, learning 0.259s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.7918
                       Mean reward: 9.17
               Mean episode length: 697.15
Episode_Reward/track_lin_vel_xy_exp: 0.5933
Episode_Reward/track_ang_vel_z_exp: 0.3186
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0486
     Episode_Reward/dof_torques_l2: -0.0569
         Episode_Reward/dof_acc_l2: -0.1370
     Episode_Reward/action_rate_l2: -0.0586
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7269
Metrics/base_velocity/error_vel_xy: 0.3954
Metrics/base_velocity/error_vel_yaw: 0.3151
      Episode_Termination/time_out: 0.5790
  Episode_Termination/base_contact: 0.4210
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 2.54s
                      Time elapsed: 00:41:39
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 982/1400 [0m                      

                       Computation: 9842 steps/s (collection: 2.242s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 5.7972
                       Mean reward: 9.82
               Mean episode length: 721.06
Episode_Reward/track_lin_vel_xy_exp: 0.5386
Episode_Reward/track_ang_vel_z_exp: 0.3007
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0538
         Episode_Reward/dof_acc_l2: -0.1213
     Episode_Reward/action_rate_l2: -0.0546
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0077
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7265
Metrics/base_velocity/error_vel_xy: 0.4139
Metrics/base_velocity/error_vel_yaw: 0.2914
      Episode_Termination/time_out: 0.5784
  Episode_Termination/base_contact: 0.4216
--------------------------------------------------------------------------------
                   Total timesteps: 24158208
                    Iteration time: 2.50s
                      Time elapsed: 00:41:41
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 983/1400 [0m                      

                       Computation: 9698 steps/s (collection: 2.270s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.8137
                       Mean reward: 10.00
               Mean episode length: 735.81
Episode_Reward/track_lin_vel_xy_exp: 0.5456
Episode_Reward/track_ang_vel_z_exp: 0.3007
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1313
     Episode_Reward/action_rate_l2: -0.0555
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7125
Metrics/base_velocity/error_vel_xy: 0.3964
Metrics/base_velocity/error_vel_yaw: 0.2989
      Episode_Termination/time_out: 0.5778
  Episode_Termination/base_contact: 0.4222
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.53s
                      Time elapsed: 00:41:44
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 984/1400 [0m                      

                       Computation: 9495 steps/s (collection: 2.333s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.8271
                       Mean reward: 9.37
               Mean episode length: 718.32
Episode_Reward/track_lin_vel_xy_exp: 0.5251
Episode_Reward/track_ang_vel_z_exp: 0.3058
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0508
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.1362
     Episode_Reward/action_rate_l2: -0.0577
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0078
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7161
Metrics/base_velocity/error_vel_xy: 0.4743
Metrics/base_velocity/error_vel_yaw: 0.3198
      Episode_Termination/time_out: 0.5760
  Episode_Termination/base_contact: 0.4240
--------------------------------------------------------------------------------
                   Total timesteps: 24207360
                    Iteration time: 2.59s
                      Time elapsed: 00:41:46
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 985/1400 [0m                      

                       Computation: 9898 steps/s (collection: 2.231s, learning 0.252s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 5.8398
                       Mean reward: 9.54
               Mean episode length: 729.44
Episode_Reward/track_lin_vel_xy_exp: 0.6330
Episode_Reward/track_ang_vel_z_exp: 0.3392
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0486
     Episode_Reward/dof_torques_l2: -0.0608
         Episode_Reward/dof_acc_l2: -0.1466
     Episode_Reward/action_rate_l2: -0.0619
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7295
Metrics/base_velocity/error_vel_xy: 0.3921
Metrics/base_velocity/error_vel_yaw: 0.3068
      Episode_Termination/time_out: 0.5742
  Episode_Termination/base_contact: 0.4258
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 2.48s
                      Time elapsed: 00:41:49
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 986/1400 [0m                      

                       Computation: 9760 steps/s (collection: 2.266s, learning 0.252s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.8370
                       Mean reward: 10.25
               Mean episode length: 751.26
Episode_Reward/track_lin_vel_xy_exp: 0.5354
Episode_Reward/track_ang_vel_z_exp: 0.3053
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0547
         Episode_Reward/dof_acc_l2: -0.1258
     Episode_Reward/action_rate_l2: -0.0558
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7318
Metrics/base_velocity/error_vel_xy: 0.4445
Metrics/base_velocity/error_vel_yaw: 0.3081
      Episode_Termination/time_out: 0.5695
  Episode_Termination/base_contact: 0.4305
--------------------------------------------------------------------------------
                   Total timesteps: 24256512
                    Iteration time: 2.52s
                      Time elapsed: 00:41:51
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 987/1400 [0m                      

                       Computation: 9793 steps/s (collection: 2.253s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.8270
                       Mean reward: 10.39
               Mean episode length: 758.58
Episode_Reward/track_lin_vel_xy_exp: 0.5956
Episode_Reward/track_ang_vel_z_exp: 0.3312
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0495
     Episode_Reward/dof_torques_l2: -0.0615
         Episode_Reward/dof_acc_l2: -0.1466
     Episode_Reward/action_rate_l2: -0.0625
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7373
Metrics/base_velocity/error_vel_xy: 0.4568
Metrics/base_velocity/error_vel_yaw: 0.3335
      Episode_Termination/time_out: 0.5704
  Episode_Termination/base_contact: 0.4296
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.51s
                      Time elapsed: 00:41:54
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 988/1400 [0m                      

                       Computation: 9886 steps/s (collection: 2.233s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.8178
                       Mean reward: 10.45
               Mean episode length: 771.66
Episode_Reward/track_lin_vel_xy_exp: 0.4960
Episode_Reward/track_ang_vel_z_exp: 0.2963
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.0516
     Episode_Reward/dof_torques_l2: -0.0552
         Episode_Reward/dof_acc_l2: -0.1347
     Episode_Reward/action_rate_l2: -0.0555
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7445
Metrics/base_velocity/error_vel_xy: 0.4920
Metrics/base_velocity/error_vel_yaw: 0.3061
      Episode_Termination/time_out: 0.5771
  Episode_Termination/base_contact: 0.4229
--------------------------------------------------------------------------------
                   Total timesteps: 24305664
                    Iteration time: 2.49s
                      Time elapsed: 00:41:56
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 989/1400 [0m                      

                       Computation: 9736 steps/s (collection: 2.269s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.8161
                       Mean reward: 10.74
               Mean episode length: 783.03
Episode_Reward/track_lin_vel_xy_exp: 0.5523
Episode_Reward/track_ang_vel_z_exp: 0.2993
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0440
     Episode_Reward/dof_torques_l2: -0.0534
         Episode_Reward/dof_acc_l2: -0.1099
     Episode_Reward/action_rate_l2: -0.0542
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7485
Metrics/base_velocity/error_vel_xy: 0.3712
Metrics/base_velocity/error_vel_yaw: 0.2833
      Episode_Termination/time_out: 0.5798
  Episode_Termination/base_contact: 0.4202
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 2.52s
                      Time elapsed: 00:41:59
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 990/1400 [0m                      

                       Computation: 9854 steps/s (collection: 2.230s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.8205
                       Mean reward: 10.43
               Mean episode length: 765.96
Episode_Reward/track_lin_vel_xy_exp: 0.5447
Episode_Reward/track_ang_vel_z_exp: 0.3140
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.0513
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.1509
     Episode_Reward/action_rate_l2: -0.0596
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0064
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7380
Metrics/base_velocity/error_vel_xy: 0.4778
Metrics/base_velocity/error_vel_yaw: 0.3281
      Episode_Termination/time_out: 0.5811
  Episode_Termination/base_contact: 0.4199
--------------------------------------------------------------------------------
                   Total timesteps: 24354816
                    Iteration time: 2.49s
                      Time elapsed: 00:42:01
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 991/1400 [0m                      

                       Computation: 9826 steps/s (collection: 2.246s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.8168
                       Mean reward: 10.84
               Mean episode length: 799.34
Episode_Reward/track_lin_vel_xy_exp: 0.6140
Episode_Reward/track_ang_vel_z_exp: 0.3498
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.0562
     Episode_Reward/dof_torques_l2: -0.0682
         Episode_Reward/dof_acc_l2: -0.1676
     Episode_Reward/action_rate_l2: -0.0666
      Episode_Reward/feet_air_time: -0.0099
 Episode_Reward/undesired_contacts: -0.0072
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7432
Metrics/base_velocity/error_vel_xy: 0.5292
Metrics/base_velocity/error_vel_yaw: 0.3789
      Episode_Termination/time_out: 0.5844
  Episode_Termination/base_contact: 0.4166
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.50s
                      Time elapsed: 00:42:04
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 992/1400 [0m                      

                       Computation: 9727 steps/s (collection: 2.262s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.8090
                       Mean reward: 10.58
               Mean episode length: 788.97
Episode_Reward/track_lin_vel_xy_exp: 0.4691
Episode_Reward/track_ang_vel_z_exp: 0.2565
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0396
     Episode_Reward/dof_torques_l2: -0.0448
         Episode_Reward/dof_acc_l2: -0.1151
     Episode_Reward/action_rate_l2: -0.0483
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7382
Metrics/base_velocity/error_vel_xy: 0.3498
Metrics/base_velocity/error_vel_yaw: 0.2714
      Episode_Termination/time_out: 0.5794
  Episode_Termination/base_contact: 0.4216
--------------------------------------------------------------------------------
                   Total timesteps: 24403968
                    Iteration time: 2.53s
                      Time elapsed: 00:42:06
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 993/1400 [0m                      

                       Computation: 9772 steps/s (collection: 2.260s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.8239
                       Mean reward: 10.27
               Mean episode length: 768.29
Episode_Reward/track_lin_vel_xy_exp: 0.6099
Episode_Reward/track_ang_vel_z_exp: 0.3398
       Episode_Reward/lin_vel_z_l2: -0.0422
      Episode_Reward/ang_vel_xy_l2: -0.0524
     Episode_Reward/dof_torques_l2: -0.0631
         Episode_Reward/dof_acc_l2: -0.1532
     Episode_Reward/action_rate_l2: -0.0630
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7378
Metrics/base_velocity/error_vel_xy: 0.4483
Metrics/base_velocity/error_vel_yaw: 0.3246
      Episode_Termination/time_out: 0.5801
  Episode_Termination/base_contact: 0.4209
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 2.51s
                      Time elapsed: 00:42:09
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 994/1400 [0m                      

                       Computation: 9797 steps/s (collection: 2.253s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 5.8244
                       Mean reward: 9.73
               Mean episode length: 723.96
Episode_Reward/track_lin_vel_xy_exp: 0.5272
Episode_Reward/track_ang_vel_z_exp: 0.2884
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0453
     Episode_Reward/dof_torques_l2: -0.0516
         Episode_Reward/dof_acc_l2: -0.1167
     Episode_Reward/action_rate_l2: -0.0527
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7502
Metrics/base_velocity/error_vel_xy: 0.3850
Metrics/base_velocity/error_vel_yaw: 0.2805
      Episode_Termination/time_out: 0.5752
  Episode_Termination/base_contact: 0.4257
--------------------------------------------------------------------------------
                   Total timesteps: 24453120
                    Iteration time: 2.51s
                      Time elapsed: 00:42:11
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 995/1400 [0m                      

                       Computation: 9717 steps/s (collection: 2.274s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 5.8194
                       Mean reward: 10.12
               Mean episode length: 746.55
Episode_Reward/track_lin_vel_xy_exp: 0.5676
Episode_Reward/track_ang_vel_z_exp: 0.3317
       Episode_Reward/lin_vel_z_l2: -0.0454
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0651
         Episode_Reward/dof_acc_l2: -0.1376
     Episode_Reward/action_rate_l2: -0.0634
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0081
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7491
Metrics/base_velocity/error_vel_xy: 0.5439
Metrics/base_velocity/error_vel_yaw: 0.3706
      Episode_Termination/time_out: 0.5774
  Episode_Termination/base_contact: 0.4236
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.53s
                      Time elapsed: 00:42:14
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 996/1400 [0m                      

                       Computation: 9834 steps/s (collection: 2.236s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 5.8116
                       Mean reward: 9.43
               Mean episode length: 709.96
Episode_Reward/track_lin_vel_xy_exp: 0.5172
Episode_Reward/track_ang_vel_z_exp: 0.2796
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0474
     Episode_Reward/dof_torques_l2: -0.0541
         Episode_Reward/dof_acc_l2: -0.1241
     Episode_Reward/action_rate_l2: -0.0531
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7399
Metrics/base_velocity/error_vel_xy: 0.3909
Metrics/base_velocity/error_vel_yaw: 0.3204
      Episode_Termination/time_out: 0.5773
  Episode_Termination/base_contact: 0.4237
--------------------------------------------------------------------------------
                   Total timesteps: 24502272
                    Iteration time: 2.50s
                      Time elapsed: 00:42:16
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 997/1400 [0m                      

                       Computation: 9676 steps/s (collection: 2.286s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.7988
                       Mean reward: 9.82
               Mean episode length: 745.82
Episode_Reward/track_lin_vel_xy_exp: 0.6157
Episode_Reward/track_ang_vel_z_exp: 0.3445
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.0521
     Episode_Reward/dof_torques_l2: -0.0627
         Episode_Reward/dof_acc_l2: -0.1411
     Episode_Reward/action_rate_l2: -0.0630
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7465
Metrics/base_velocity/error_vel_xy: 0.4597
Metrics/base_velocity/error_vel_yaw: 0.3151
      Episode_Termination/time_out: 0.5732
  Episode_Termination/base_contact: 0.4276
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 2.54s
                      Time elapsed: 00:42:19
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 998/1400 [0m                      

                       Computation: 9686 steps/s (collection: 2.279s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.8000
                       Mean reward: 9.71
               Mean episode length: 731.77
Episode_Reward/track_lin_vel_xy_exp: 0.5444
Episode_Reward/track_ang_vel_z_exp: 0.2954
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0539
         Episode_Reward/dof_acc_l2: -0.1282
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7507
Metrics/base_velocity/error_vel_xy: 0.4000
Metrics/base_velocity/error_vel_yaw: 0.3125
      Episode_Termination/time_out: 0.5691
  Episode_Termination/base_contact: 0.4309
--------------------------------------------------------------------------------
                   Total timesteps: 24551424
                    Iteration time: 2.54s
                      Time elapsed: 00:42:21
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 999/1400 [0m                      

                       Computation: 9867 steps/s (collection: 2.229s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.8060
                       Mean reward: 10.18
               Mean episode length: 747.26
Episode_Reward/track_lin_vel_xy_exp: 0.5739
Episode_Reward/track_ang_vel_z_exp: 0.3193
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0465
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1295
     Episode_Reward/action_rate_l2: -0.0581
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7485
Metrics/base_velocity/error_vel_xy: 0.4219
Metrics/base_velocity/error_vel_yaw: 0.3078
      Episode_Termination/time_out: 0.5701
  Episode_Termination/base_contact: 0.4299
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.49s
                      Time elapsed: 00:42:24
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1000/1400 [0m                     

                       Computation: 9862 steps/s (collection: 2.228s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.8148
                       Mean reward: 9.98
               Mean episode length: 739.08
Episode_Reward/track_lin_vel_xy_exp: 0.5557
Episode_Reward/track_ang_vel_z_exp: 0.3205
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0500
     Episode_Reward/dof_torques_l2: -0.0569
         Episode_Reward/dof_acc_l2: -0.1374
     Episode_Reward/action_rate_l2: -0.0584
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0201
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7498
Metrics/base_velocity/error_vel_xy: 0.4842
Metrics/base_velocity/error_vel_yaw: 0.3379
      Episode_Termination/time_out: 0.5707
  Episode_Termination/base_contact: 0.4293
--------------------------------------------------------------------------------
                   Total timesteps: 24600576
                    Iteration time: 2.49s
                      Time elapsed: 00:42:26
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1001/1400 [0m                     

                       Computation: 9911 steps/s (collection: 2.224s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.8192
                       Mean reward: 10.03
               Mean episode length: 741.93
Episode_Reward/track_lin_vel_xy_exp: 0.5421
Episode_Reward/track_ang_vel_z_exp: 0.3063
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0588
         Episode_Reward/dof_acc_l2: -0.1378
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7560
Metrics/base_velocity/error_vel_xy: 0.4431
Metrics/base_velocity/error_vel_yaw: 0.3195
      Episode_Termination/time_out: 0.5723
  Episode_Termination/base_contact: 0.4277
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 2.48s
                      Time elapsed: 00:42:29
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 1002/1400 [0m                     

                       Computation: 9784 steps/s (collection: 2.255s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 5.8222
                       Mean reward: 9.82
               Mean episode length: 735.00
Episode_Reward/track_lin_vel_xy_exp: 0.5147
Episode_Reward/track_ang_vel_z_exp: 0.2841
       Episode_Reward/lin_vel_z_l2: -0.0338
      Episode_Reward/ang_vel_xy_l2: -0.0440
     Episode_Reward/dof_torques_l2: -0.0526
         Episode_Reward/dof_acc_l2: -0.1243
     Episode_Reward/action_rate_l2: -0.0522
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7677
Metrics/base_velocity/error_vel_xy: 0.3829
Metrics/base_velocity/error_vel_yaw: 0.2804
      Episode_Termination/time_out: 0.5738
  Episode_Termination/base_contact: 0.4262
--------------------------------------------------------------------------------
                   Total timesteps: 24649728
                    Iteration time: 2.51s
                      Time elapsed: 00:42:31
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1003/1400 [0m                     

                       Computation: 9757 steps/s (collection: 2.256s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.8283
                       Mean reward: 10.06
               Mean episode length: 740.63
Episode_Reward/track_lin_vel_xy_exp: 0.5595
Episode_Reward/track_ang_vel_z_exp: 0.2960
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0432
     Episode_Reward/dof_torques_l2: -0.0497
         Episode_Reward/dof_acc_l2: -0.1174
     Episode_Reward/action_rate_l2: -0.0538
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7724
Metrics/base_velocity/error_vel_xy: 0.3338
Metrics/base_velocity/error_vel_yaw: 0.2695
      Episode_Termination/time_out: 0.5743
  Episode_Termination/base_contact: 0.4257
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.52s
                      Time elapsed: 00:42:34
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1004/1400 [0m                     

                       Computation: 9836 steps/s (collection: 2.236s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 5.8441
                       Mean reward: 9.74
               Mean episode length: 717.24
Episode_Reward/track_lin_vel_xy_exp: 0.4132
Episode_Reward/track_ang_vel_z_exp: 0.2414
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0484
         Episode_Reward/dof_acc_l2: -0.1143
     Episode_Reward/action_rate_l2: -0.0465
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7790
Metrics/base_velocity/error_vel_xy: 0.4117
Metrics/base_velocity/error_vel_yaw: 0.2871
      Episode_Termination/time_out: 0.5719
  Episode_Termination/base_contact: 0.4281
--------------------------------------------------------------------------------
                   Total timesteps: 24698880
                    Iteration time: 2.50s
                      Time elapsed: 00:42:36
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1005/1400 [0m                     

                       Computation: 9669 steps/s (collection: 2.286s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.8489
                       Mean reward: 9.01
               Mean episode length: 677.66
Episode_Reward/track_lin_vel_xy_exp: 0.4200
Episode_Reward/track_ang_vel_z_exp: 0.2319
       Episode_Reward/lin_vel_z_l2: -0.0338
      Episode_Reward/ang_vel_xy_l2: -0.0391
     Episode_Reward/dof_torques_l2: -0.0443
         Episode_Reward/dof_acc_l2: -0.1028
     Episode_Reward/action_rate_l2: -0.0437
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0088
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7799
Metrics/base_velocity/error_vel_xy: 0.3452
Metrics/base_velocity/error_vel_yaw: 0.2570
      Episode_Termination/time_out: 0.5666
  Episode_Termination/base_contact: 0.4334
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 2.54s
                      Time elapsed: 00:42:39
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1006/1400 [0m                     

                       Computation: 9827 steps/s (collection: 2.237s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.8443
                       Mean reward: 8.75
               Mean episode length: 681.32
Episode_Reward/track_lin_vel_xy_exp: 0.5045
Episode_Reward/track_ang_vel_z_exp: 0.2919
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0508
     Episode_Reward/dof_torques_l2: -0.0583
         Episode_Reward/dof_acc_l2: -0.1335
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7849
Metrics/base_velocity/error_vel_xy: 0.4769
Metrics/base_velocity/error_vel_yaw: 0.3203
      Episode_Termination/time_out: 0.5640
  Episode_Termination/base_contact: 0.4360
--------------------------------------------------------------------------------
                   Total timesteps: 24748032
                    Iteration time: 2.50s
                      Time elapsed: 00:42:41
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1007/1400 [0m                     

                       Computation: 9906 steps/s (collection: 2.223s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 5.8362
                       Mean reward: 9.32
               Mean episode length: 698.80
Episode_Reward/track_lin_vel_xy_exp: 0.5760
Episode_Reward/track_ang_vel_z_exp: 0.3184
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.1223
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0090
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7919
Metrics/base_velocity/error_vel_xy: 0.4193
Metrics/base_velocity/error_vel_yaw: 0.3108
      Episode_Termination/time_out: 0.5631
  Episode_Termination/base_contact: 0.4369
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.48s
                      Time elapsed: 00:42:44
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1008/1400 [0m                     

                       Computation: 9765 steps/s (collection: 2.260s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 5.8340
                       Mean reward: 10.07
               Mean episode length: 753.09
Episode_Reward/track_lin_vel_xy_exp: 0.5123
Episode_Reward/track_ang_vel_z_exp: 0.3030
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.0503
     Episode_Reward/dof_torques_l2: -0.0586
         Episode_Reward/dof_acc_l2: -0.1400
     Episode_Reward/action_rate_l2: -0.0573
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8009
Metrics/base_velocity/error_vel_xy: 0.5101
Metrics/base_velocity/error_vel_yaw: 0.3450
      Episode_Termination/time_out: 0.5645
  Episode_Termination/base_contact: 0.4355
--------------------------------------------------------------------------------
                   Total timesteps: 24797184
                    Iteration time: 2.52s
                      Time elapsed: 00:42:46
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1009/1400 [0m                     

                       Computation: 9690 steps/s (collection: 2.280s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.8284
                       Mean reward: 10.61
               Mean episode length: 755.83
Episode_Reward/track_lin_vel_xy_exp: 0.6106
Episode_Reward/track_ang_vel_z_exp: 0.3241
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0570
         Episode_Reward/dof_acc_l2: -0.1341
     Episode_Reward/action_rate_l2: -0.0590
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8105
Metrics/base_velocity/error_vel_xy: 0.3781
Metrics/base_velocity/error_vel_yaw: 0.3014
      Episode_Termination/time_out: 0.5645
  Episode_Termination/base_contact: 0.4355
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 2.54s
                      Time elapsed: 00:42:49
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1010/1400 [0m                     

                       Computation: 9878 steps/s (collection: 2.234s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.8237
                       Mean reward: 10.22
               Mean episode length: 741.47
Episode_Reward/track_lin_vel_xy_exp: 0.4904
Episode_Reward/track_ang_vel_z_exp: 0.2714
       Episode_Reward/lin_vel_z_l2: -0.0372
      Episode_Reward/ang_vel_xy_l2: -0.0437
     Episode_Reward/dof_torques_l2: -0.0510
         Episode_Reward/dof_acc_l2: -0.1266
     Episode_Reward/action_rate_l2: -0.0508
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8099
Metrics/base_velocity/error_vel_xy: 0.3667
Metrics/base_velocity/error_vel_yaw: 0.2741
      Episode_Termination/time_out: 0.5627
  Episode_Termination/base_contact: 0.4373
--------------------------------------------------------------------------------
                   Total timesteps: 24846336
                    Iteration time: 2.49s
                      Time elapsed: 00:42:52
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1011/1400 [0m                     

                       Computation: 9756 steps/s (collection: 2.263s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 5.8224
                       Mean reward: 10.14
               Mean episode length: 739.06
Episode_Reward/track_lin_vel_xy_exp: 0.5114
Episode_Reward/track_ang_vel_z_exp: 0.2826
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0468
     Episode_Reward/dof_torques_l2: -0.0528
         Episode_Reward/dof_acc_l2: -0.1234
     Episode_Reward/action_rate_l2: -0.0516
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0089
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8139
Metrics/base_velocity/error_vel_xy: 0.3817
Metrics/base_velocity/error_vel_yaw: 0.2834
      Episode_Termination/time_out: 0.5638
  Episode_Termination/base_contact: 0.4362
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.52s
                      Time elapsed: 00:42:54
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1012/1400 [0m                     

                       Computation: 9907 steps/s (collection: 2.225s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.8163
                       Mean reward: 9.88
               Mean episode length: 726.98
Episode_Reward/track_lin_vel_xy_exp: 0.6103
Episode_Reward/track_ang_vel_z_exp: 0.3299
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1301
     Episode_Reward/action_rate_l2: -0.0596
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8169
Metrics/base_velocity/error_vel_xy: 0.4048
Metrics/base_velocity/error_vel_yaw: 0.3065
      Episode_Termination/time_out: 0.5616
  Episode_Termination/base_contact: 0.4384
--------------------------------------------------------------------------------
                   Total timesteps: 24895488
                    Iteration time: 2.48s
                      Time elapsed: 00:42:57
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1013/1400 [0m                     

                       Computation: 9860 steps/s (collection: 2.237s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.8152
                       Mean reward: 10.13
               Mean episode length: 738.42
Episode_Reward/track_lin_vel_xy_exp: 0.5816
Episode_Reward/track_ang_vel_z_exp: 0.3347
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.0506
     Episode_Reward/dof_torques_l2: -0.0608
         Episode_Reward/dof_acc_l2: -0.1387
     Episode_Reward/action_rate_l2: -0.0616
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8211
Metrics/base_velocity/error_vel_xy: 0.5170
Metrics/base_velocity/error_vel_yaw: 0.3571
      Episode_Termination/time_out: 0.5647
  Episode_Termination/base_contact: 0.4353
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 2.49s
                      Time elapsed: 00:42:59
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1014/1400 [0m                     

                       Computation: 9728 steps/s (collection: 2.272s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.8097
                       Mean reward: 10.61
               Mean episode length: 754.13
Episode_Reward/track_lin_vel_xy_exp: 0.5675
Episode_Reward/track_ang_vel_z_exp: 0.3158
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0457
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.1255
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0064
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8180
Metrics/base_velocity/error_vel_xy: 0.4393
Metrics/base_velocity/error_vel_yaw: 0.3153
      Episode_Termination/time_out: 0.5670
  Episode_Termination/base_contact: 0.4330
--------------------------------------------------------------------------------
                   Total timesteps: 24944640
                    Iteration time: 2.53s
                      Time elapsed: 00:43:02
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1015/1400 [0m                     

                       Computation: 9785 steps/s (collection: 2.256s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 5.8090
                       Mean reward: 10.19
               Mean episode length: 722.77
Episode_Reward/track_lin_vel_xy_exp: 0.5273
Episode_Reward/track_ang_vel_z_exp: 0.2849
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.0414
     Episode_Reward/dof_torques_l2: -0.0537
         Episode_Reward/dof_acc_l2: -0.1195
     Episode_Reward/action_rate_l2: -0.0525
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8171
Metrics/base_velocity/error_vel_xy: 0.3596
Metrics/base_velocity/error_vel_yaw: 0.2774
      Episode_Termination/time_out: 0.5676
  Episode_Termination/base_contact: 0.4324
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.51s
                      Time elapsed: 00:43:04
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1016/1400 [0m                     

                       Computation: 9696 steps/s (collection: 2.281s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.7967
                       Mean reward: 9.97
               Mean episode length: 724.42
Episode_Reward/track_lin_vel_xy_exp: 0.5710
Episode_Reward/track_ang_vel_z_exp: 0.3173
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.0510
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.1478
     Episode_Reward/action_rate_l2: -0.0589
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8098
Metrics/base_velocity/error_vel_xy: 0.4308
Metrics/base_velocity/error_vel_yaw: 0.3134
      Episode_Termination/time_out: 0.5648
  Episode_Termination/base_contact: 0.4352
--------------------------------------------------------------------------------
                   Total timesteps: 24993792
                    Iteration time: 2.53s
                      Time elapsed: 00:43:07
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1017/1400 [0m                     

                       Computation: 9691 steps/s (collection: 2.278s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.7958
                       Mean reward: 9.93
               Mean episode length: 732.41
Episode_Reward/track_lin_vel_xy_exp: 0.5350
Episode_Reward/track_ang_vel_z_exp: 0.2979
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0565
         Episode_Reward/dof_acc_l2: -0.1230
     Episode_Reward/action_rate_l2: -0.0551
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8134
Metrics/base_velocity/error_vel_xy: 0.4171
Metrics/base_velocity/error_vel_yaw: 0.3080
      Episode_Termination/time_out: 0.5614
  Episode_Termination/base_contact: 0.4386
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 2.54s
                      Time elapsed: 00:43:09
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 1018/1400 [0m                     

                       Computation: 9775 steps/s (collection: 2.256s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 5.7941
                       Mean reward: 10.02
               Mean episode length: 725.64
Episode_Reward/track_lin_vel_xy_exp: 0.4936
Episode_Reward/track_ang_vel_z_exp: 0.2807
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0456
     Episode_Reward/dof_torques_l2: -0.0511
         Episode_Reward/dof_acc_l2: -0.1236
     Episode_Reward/action_rate_l2: -0.0511
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8157
Metrics/base_velocity/error_vel_xy: 0.3892
Metrics/base_velocity/error_vel_yaw: 0.2724
      Episode_Termination/time_out: 0.5628
  Episode_Termination/base_contact: 0.4372
--------------------------------------------------------------------------------
                   Total timesteps: 25042944
                    Iteration time: 2.51s
                      Time elapsed: 00:43:12
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1019/1400 [0m                     

                       Computation: 9790 steps/s (collection: 2.253s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.7989
                       Mean reward: 9.81
               Mean episode length: 731.48
Episode_Reward/track_lin_vel_xy_exp: 0.5257
Episode_Reward/track_ang_vel_z_exp: 0.3034
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0575
         Episode_Reward/dof_acc_l2: -0.1410
     Episode_Reward/action_rate_l2: -0.0576
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8215
Metrics/base_velocity/error_vel_xy: 0.4912
Metrics/base_velocity/error_vel_yaw: 0.3420
      Episode_Termination/time_out: 0.5614
  Episode_Termination/base_contact: 0.4386
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.51s
                      Time elapsed: 00:43:14
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1020/1400 [0m                     

                       Computation: 9764 steps/s (collection: 2.262s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.8024
                       Mean reward: 10.69
               Mean episode length: 769.52
Episode_Reward/track_lin_vel_xy_exp: 0.5770
Episode_Reward/track_ang_vel_z_exp: 0.3176
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0454
     Episode_Reward/dof_torques_l2: -0.0553
         Episode_Reward/dof_acc_l2: -0.1213
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8298
Metrics/base_velocity/error_vel_xy: 0.4025
Metrics/base_velocity/error_vel_yaw: 0.2966
      Episode_Termination/time_out: 0.5625
  Episode_Termination/base_contact: 0.4375
--------------------------------------------------------------------------------
                   Total timesteps: 25092096
                    Iteration time: 2.52s
                      Time elapsed: 00:43:17
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1021/1400 [0m                     

                       Computation: 9755 steps/s (collection: 2.265s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.8047
                       Mean reward: 11.09
               Mean episode length: 790.90
Episode_Reward/track_lin_vel_xy_exp: 0.6000
Episode_Reward/track_ang_vel_z_exp: 0.3348
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0498
     Episode_Reward/dof_torques_l2: -0.0592
         Episode_Reward/dof_acc_l2: -0.1398
     Episode_Reward/action_rate_l2: -0.0615
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8324
Metrics/base_velocity/error_vel_xy: 0.4583
Metrics/base_velocity/error_vel_yaw: 0.3245
      Episode_Termination/time_out: 0.5717
  Episode_Termination/base_contact: 0.4283
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 2.52s
                      Time elapsed: 00:43:19
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1022/1400 [0m                     

                       Computation: 9809 steps/s (collection: 2.248s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.8085
                       Mean reward: 11.37
               Mean episode length: 794.12
Episode_Reward/track_lin_vel_xy_exp: 0.5565
Episode_Reward/track_ang_vel_z_exp: 0.3132
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0595
         Episode_Reward/dof_acc_l2: -0.1375
     Episode_Reward/action_rate_l2: -0.0579
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0067
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8307
Metrics/base_velocity/error_vel_xy: 0.4306
Metrics/base_velocity/error_vel_yaw: 0.3141
      Episode_Termination/time_out: 0.5753
  Episode_Termination/base_contact: 0.4247
--------------------------------------------------------------------------------
                   Total timesteps: 25141248
                    Iteration time: 2.51s
                      Time elapsed: 00:43:22
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1023/1400 [0m                     

                       Computation: 9722 steps/s (collection: 2.273s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.8151
                       Mean reward: 10.69
               Mean episode length: 769.83
Episode_Reward/track_lin_vel_xy_exp: 0.5696
Episode_Reward/track_ang_vel_z_exp: 0.3246
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0590
         Episode_Reward/dof_acc_l2: -0.1373
     Episode_Reward/action_rate_l2: -0.0590
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8306
Metrics/base_velocity/error_vel_xy: 0.4678
Metrics/base_velocity/error_vel_yaw: 0.3216
      Episode_Termination/time_out: 0.5743
  Episode_Termination/base_contact: 0.4257
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.53s
                      Time elapsed: 00:43:24
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1024/1400 [0m                     

                       Computation: 9761 steps/s (collection: 2.255s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.8178
                       Mean reward: 9.88
               Mean episode length: 743.72
Episode_Reward/track_lin_vel_xy_exp: 0.4240
Episode_Reward/track_ang_vel_z_exp: 0.2451
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0433
     Episode_Reward/dof_torques_l2: -0.0511
         Episode_Reward/dof_acc_l2: -0.1195
     Episode_Reward/action_rate_l2: -0.0479
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0075
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8291
Metrics/base_velocity/error_vel_xy: 0.3933
Metrics/base_velocity/error_vel_yaw: 0.2801
      Episode_Termination/time_out: 0.5734
  Episode_Termination/base_contact: 0.4266
--------------------------------------------------------------------------------
                   Total timesteps: 25190400
                    Iteration time: 2.52s
                      Time elapsed: 00:43:27
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1025/1400 [0m                     

                       Computation: 9663 steps/s (collection: 2.286s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 5.8194
                       Mean reward: 9.45
               Mean episode length: 738.88
Episode_Reward/track_lin_vel_xy_exp: 0.5636
Episode_Reward/track_ang_vel_z_exp: 0.3076
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0498
     Episode_Reward/dof_torques_l2: -0.0604
         Episode_Reward/dof_acc_l2: -0.1442
     Episode_Reward/action_rate_l2: -0.0586
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8270
Metrics/base_velocity/error_vel_xy: 0.4417
Metrics/base_velocity/error_vel_yaw: 0.3557
      Episode_Termination/time_out: 0.5736
  Episode_Termination/base_contact: 0.4264
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 2.54s
                      Time elapsed: 00:43:29
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1026/1400 [0m                     

                       Computation: 9788 steps/s (collection: 2.255s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.8216
                       Mean reward: 9.73
               Mean episode length: 719.69
Episode_Reward/track_lin_vel_xy_exp: 0.5584
Episode_Reward/track_ang_vel_z_exp: 0.3016
       Episode_Reward/lin_vel_z_l2: -0.0329
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.0531
         Episode_Reward/dof_acc_l2: -0.1230
     Episode_Reward/action_rate_l2: -0.0550
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8232
Metrics/base_velocity/error_vel_xy: 0.3856
Metrics/base_velocity/error_vel_yaw: 0.2965
      Episode_Termination/time_out: 0.5739
  Episode_Termination/base_contact: 0.4261
--------------------------------------------------------------------------------
                   Total timesteps: 25239552
                    Iteration time: 2.51s
                      Time elapsed: 00:43:32
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1027/1400 [0m                     

                       Computation: 9678 steps/s (collection: 2.277s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.8242
                       Mean reward: 10.34
               Mean episode length: 732.58
Episode_Reward/track_lin_vel_xy_exp: 0.5518
Episode_Reward/track_ang_vel_z_exp: 0.3080
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0564
         Episode_Reward/dof_acc_l2: -0.1303
     Episode_Reward/action_rate_l2: -0.0565
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8177
Metrics/base_velocity/error_vel_xy: 0.4222
Metrics/base_velocity/error_vel_yaw: 0.3065
      Episode_Termination/time_out: 0.5730
  Episode_Termination/base_contact: 0.4270
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.54s
                      Time elapsed: 00:43:34
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1028/1400 [0m                     

                       Computation: 9733 steps/s (collection: 2.262s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.8226
                       Mean reward: 9.77
               Mean episode length: 718.23
Episode_Reward/track_lin_vel_xy_exp: 0.4807
Episode_Reward/track_ang_vel_z_exp: 0.2752
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1269
     Episode_Reward/action_rate_l2: -0.0527
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8159
Metrics/base_velocity/error_vel_xy: 0.4456
Metrics/base_velocity/error_vel_yaw: 0.3155
      Episode_Termination/time_out: 0.5716
  Episode_Termination/base_contact: 0.4284
--------------------------------------------------------------------------------
                   Total timesteps: 25288704
                    Iteration time: 2.52s
                      Time elapsed: 00:43:37
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1029/1400 [0m                     

                       Computation: 9786 steps/s (collection: 2.257s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0191
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.8322
                       Mean reward: 9.70
               Mean episode length: 725.24
Episode_Reward/track_lin_vel_xy_exp: 0.5243
Episode_Reward/track_ang_vel_z_exp: 0.2825
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.0400
     Episode_Reward/dof_torques_l2: -0.0485
         Episode_Reward/dof_acc_l2: -0.1172
     Episode_Reward/action_rate_l2: -0.0514
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8182
Metrics/base_velocity/error_vel_xy: 0.3342
Metrics/base_velocity/error_vel_yaw: 0.2561
      Episode_Termination/time_out: 0.5680
  Episode_Termination/base_contact: 0.4320
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 2.51s
                      Time elapsed: 00:43:39
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1030/1400 [0m                     

                       Computation: 9702 steps/s (collection: 2.275s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 5.8335
                       Mean reward: 9.71
               Mean episode length: 723.13
Episode_Reward/track_lin_vel_xy_exp: 0.5676
Episode_Reward/track_ang_vel_z_exp: 0.3332
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0540
     Episode_Reward/dof_torques_l2: -0.0604
         Episode_Reward/dof_acc_l2: -0.1536
     Episode_Reward/action_rate_l2: -0.0617
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0096
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8166
Metrics/base_velocity/error_vel_xy: 0.5234
Metrics/base_velocity/error_vel_yaw: 0.3506
      Episode_Termination/time_out: 0.5694
  Episode_Termination/base_contact: 0.4306
--------------------------------------------------------------------------------
                   Total timesteps: 25337856
                    Iteration time: 2.53s
                      Time elapsed: 00:43:42
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1031/1400 [0m                     

                       Computation: 9784 steps/s (collection: 2.251s, learning 0.260s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.8388
                       Mean reward: 10.56
               Mean episode length: 752.75
Episode_Reward/track_lin_vel_xy_exp: 0.6163
Episode_Reward/track_ang_vel_z_exp: 0.3474
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0605
         Episode_Reward/dof_acc_l2: -0.1463
     Episode_Reward/action_rate_l2: -0.0626
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8175
Metrics/base_velocity/error_vel_xy: 0.4348
Metrics/base_velocity/error_vel_yaw: 0.3005
      Episode_Termination/time_out: 0.5725
  Episode_Termination/base_contact: 0.4275
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.51s
                      Time elapsed: 00:43:44
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1032/1400 [0m                     

                       Computation: 9846 steps/s (collection: 2.238s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.8423
                       Mean reward: 10.28
               Mean episode length: 744.23
Episode_Reward/track_lin_vel_xy_exp: 0.5152
Episode_Reward/track_ang_vel_z_exp: 0.2856
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.0444
     Episode_Reward/dof_torques_l2: -0.0518
         Episode_Reward/dof_acc_l2: -0.1224
     Episode_Reward/action_rate_l2: -0.0523
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8256
Metrics/base_velocity/error_vel_xy: 0.3968
Metrics/base_velocity/error_vel_yaw: 0.2837
      Episode_Termination/time_out: 0.5714
  Episode_Termination/base_contact: 0.4286
--------------------------------------------------------------------------------
                   Total timesteps: 25387008
                    Iteration time: 2.50s
                      Time elapsed: 00:43:47
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1033/1400 [0m                     

                       Computation: 9710 steps/s (collection: 2.275s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.8549
                       Mean reward: 10.03
               Mean episode length: 730.32
Episode_Reward/track_lin_vel_xy_exp: 0.4890
Episode_Reward/track_ang_vel_z_exp: 0.2730
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0419
     Episode_Reward/dof_torques_l2: -0.0517
         Episode_Reward/dof_acc_l2: -0.1216
     Episode_Reward/action_rate_l2: -0.0511
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8298
Metrics/base_velocity/error_vel_xy: 0.3793
Metrics/base_velocity/error_vel_yaw: 0.2763
      Episode_Termination/time_out: 0.5696
  Episode_Termination/base_contact: 0.4304
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 2.53s
                      Time elapsed: 00:43:49
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1034/1400 [0m                     

                       Computation: 9889 steps/s (collection: 2.221s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.8404
                       Mean reward: 9.58
               Mean episode length: 700.99
Episode_Reward/track_lin_vel_xy_exp: 0.5004
Episode_Reward/track_ang_vel_z_exp: 0.2801
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0438
     Episode_Reward/dof_torques_l2: -0.0503
         Episode_Reward/dof_acc_l2: -0.1166
     Episode_Reward/action_rate_l2: -0.0524
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8287
Metrics/base_velocity/error_vel_xy: 0.4088
Metrics/base_velocity/error_vel_yaw: 0.2948
      Episode_Termination/time_out: 0.5710
  Episode_Termination/base_contact: 0.4290
--------------------------------------------------------------------------------
                   Total timesteps: 25436160
                    Iteration time: 2.49s
                      Time elapsed: 00:43:52
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1035/1400 [0m                     

                       Computation: 9806 steps/s (collection: 2.245s, learning 0.261s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.8189
                       Mean reward: 9.49
               Mean episode length: 704.34
Episode_Reward/track_lin_vel_xy_exp: 0.5647
Episode_Reward/track_ang_vel_z_exp: 0.3042
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0567
         Episode_Reward/dof_acc_l2: -0.1304
     Episode_Reward/action_rate_l2: -0.0567
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0116
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8228
Metrics/base_velocity/error_vel_xy: 0.3685
Metrics/base_velocity/error_vel_yaw: 0.2876
      Episode_Termination/time_out: 0.5682
  Episode_Termination/base_contact: 0.4318
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.51s
                      Time elapsed: 00:43:54
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1036/1400 [0m                     

                       Computation: 9881 steps/s (collection: 2.233s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.8164
                       Mean reward: 9.52
               Mean episode length: 713.73
Episode_Reward/track_lin_vel_xy_exp: 0.5386
Episode_Reward/track_ang_vel_z_exp: 0.3036
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0457
     Episode_Reward/dof_torques_l2: -0.0549
         Episode_Reward/dof_acc_l2: -0.1288
     Episode_Reward/action_rate_l2: -0.0559
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8121
Metrics/base_velocity/error_vel_xy: 0.4230
Metrics/base_velocity/error_vel_yaw: 0.2957
      Episode_Termination/time_out: 0.5682
  Episode_Termination/base_contact: 0.4318
--------------------------------------------------------------------------------
                   Total timesteps: 25485312
                    Iteration time: 2.49s
                      Time elapsed: 00:43:57
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1037/1400 [0m                     

                       Computation: 9848 steps/s (collection: 2.241s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.8183
                       Mean reward: 9.89
               Mean episode length: 715.60
Episode_Reward/track_lin_vel_xy_exp: 0.5900
Episode_Reward/track_ang_vel_z_exp: 0.3199
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.0609
         Episode_Reward/dof_acc_l2: -0.1393
     Episode_Reward/action_rate_l2: -0.0591
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8108
Metrics/base_velocity/error_vel_xy: 0.4075
Metrics/base_velocity/error_vel_yaw: 0.3123
      Episode_Termination/time_out: 0.5674
  Episode_Termination/base_contact: 0.4326
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 2.50s
                      Time elapsed: 00:43:59
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1038/1400 [0m                     

                       Computation: 9994 steps/s (collection: 2.201s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.8085
                       Mean reward: 10.78
               Mean episode length: 765.60
Episode_Reward/track_lin_vel_xy_exp: 0.6651
Episode_Reward/track_ang_vel_z_exp: 0.3622
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0529
     Episode_Reward/dof_torques_l2: -0.0662
         Episode_Reward/dof_acc_l2: -0.1615
     Episode_Reward/action_rate_l2: -0.0653
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8182
Metrics/base_velocity/error_vel_xy: 0.4293
Metrics/base_velocity/error_vel_yaw: 0.3230
      Episode_Termination/time_out: 0.5680
  Episode_Termination/base_contact: 0.4320
--------------------------------------------------------------------------------
                   Total timesteps: 25534464
                    Iteration time: 2.46s
                      Time elapsed: 00:44:02
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1039/1400 [0m                     

                       Computation: 9702 steps/s (collection: 2.280s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 5.7979
                       Mean reward: 10.31
               Mean episode length: 751.22
Episode_Reward/track_lin_vel_xy_exp: 0.5065
Episode_Reward/track_ang_vel_z_exp: 0.2971
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0534
         Episode_Reward/dof_acc_l2: -0.1284
     Episode_Reward/action_rate_l2: -0.0552
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8282
Metrics/base_velocity/error_vel_xy: 0.4555
Metrics/base_velocity/error_vel_yaw: 0.3002
      Episode_Termination/time_out: 0.5684
  Episode_Termination/base_contact: 0.4316
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.53s
                      Time elapsed: 00:44:04
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1040/1400 [0m                     

                       Computation: 9809 steps/s (collection: 2.240s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.7882
                       Mean reward: 9.96
               Mean episode length: 748.94
Episode_Reward/track_lin_vel_xy_exp: 0.5830
Episode_Reward/track_ang_vel_z_exp: 0.3274
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0480
     Episode_Reward/dof_torques_l2: -0.0596
         Episode_Reward/dof_acc_l2: -0.1328
     Episode_Reward/action_rate_l2: -0.0594
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8233
Metrics/base_velocity/error_vel_xy: 0.4505
Metrics/base_velocity/error_vel_yaw: 0.3112
      Episode_Termination/time_out: 0.5653
  Episode_Termination/base_contact: 0.4347
--------------------------------------------------------------------------------
                   Total timesteps: 25583616
                    Iteration time: 2.51s
                      Time elapsed: 00:44:07
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1041/1400 [0m                     

                       Computation: 9954 steps/s (collection: 2.211s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.7787
                       Mean reward: 9.59
               Mean episode length: 713.16
Episode_Reward/track_lin_vel_xy_exp: 0.4306
Episode_Reward/track_ang_vel_z_exp: 0.2476
       Episode_Reward/lin_vel_z_l2: -0.0299
      Episode_Reward/ang_vel_xy_l2: -0.0388
     Episode_Reward/dof_torques_l2: -0.0462
         Episode_Reward/dof_acc_l2: -0.1054
     Episode_Reward/action_rate_l2: -0.0467
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8236
Metrics/base_velocity/error_vel_xy: 0.3784
Metrics/base_velocity/error_vel_yaw: 0.2626
      Episode_Termination/time_out: 0.5649
  Episode_Termination/base_contact: 0.4351
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 2.47s
                      Time elapsed: 00:44:09
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1042/1400 [0m                     

                       Computation: 9845 steps/s (collection: 2.239s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 5.7877
                       Mean reward: 9.04
               Mean episode length: 702.80
Episode_Reward/track_lin_vel_xy_exp: 0.5621
Episode_Reward/track_ang_vel_z_exp: 0.3363
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0622
         Episode_Reward/dof_acc_l2: -0.1382
     Episode_Reward/action_rate_l2: -0.0617
      Episode_Reward/feet_air_time: -0.0101
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8241
Metrics/base_velocity/error_vel_xy: 0.5207
Metrics/base_velocity/error_vel_yaw: 0.3326
      Episode_Termination/time_out: 0.5643
  Episode_Termination/base_contact: 0.4357
--------------------------------------------------------------------------------
                   Total timesteps: 25632768
                    Iteration time: 2.50s
                      Time elapsed: 00:44:12
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1043/1400 [0m                     

                       Computation: 9743 steps/s (collection: 2.267s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.7894
                       Mean reward: 9.77
               Mean episode length: 745.63
Episode_Reward/track_lin_vel_xy_exp: 0.5933
Episode_Reward/track_ang_vel_z_exp: 0.3427
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.0533
     Episode_Reward/dof_torques_l2: -0.0632
         Episode_Reward/dof_acc_l2: -0.1469
     Episode_Reward/action_rate_l2: -0.0631
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8224
Metrics/base_velocity/error_vel_xy: 0.5122
Metrics/base_velocity/error_vel_yaw: 0.3472
      Episode_Termination/time_out: 0.5639
  Episode_Termination/base_contact: 0.4361
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.52s
                      Time elapsed: 00:44:14
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1044/1400 [0m                     

                       Computation: 9757 steps/s (collection: 2.263s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0183
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7906
                       Mean reward: 9.47
               Mean episode length: 716.25
Episode_Reward/track_lin_vel_xy_exp: 0.5255
Episode_Reward/track_ang_vel_z_exp: 0.2899
       Episode_Reward/lin_vel_z_l2: -0.0342
      Episode_Reward/ang_vel_xy_l2: -0.0433
     Episode_Reward/dof_torques_l2: -0.0526
         Episode_Reward/dof_acc_l2: -0.1226
     Episode_Reward/action_rate_l2: -0.0540
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8245
Metrics/base_velocity/error_vel_xy: 0.3825
Metrics/base_velocity/error_vel_yaw: 0.2839
      Episode_Termination/time_out: 0.5601
  Episode_Termination/base_contact: 0.4399
--------------------------------------------------------------------------------
                   Total timesteps: 25681920
                    Iteration time: 2.52s
                      Time elapsed: 00:44:17
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1045/1400 [0m                     

                       Computation: 9744 steps/s (collection: 2.265s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.7877
                       Mean reward: 10.07
               Mean episode length: 741.37
Episode_Reward/track_lin_vel_xy_exp: 0.6072
Episode_Reward/track_ang_vel_z_exp: 0.3387
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0514
     Episode_Reward/dof_torques_l2: -0.0603
         Episode_Reward/dof_acc_l2: -0.1529
     Episode_Reward/action_rate_l2: -0.0632
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8282
Metrics/base_velocity/error_vel_xy: 0.4821
Metrics/base_velocity/error_vel_yaw: 0.3460
      Episode_Termination/time_out: 0.5577
  Episode_Termination/base_contact: 0.4423
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 2.52s
                      Time elapsed: 00:44:19
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1046/1400 [0m                     

                       Computation: 9844 steps/s (collection: 2.240s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7962
                       Mean reward: 9.52
               Mean episode length: 704.33
Episode_Reward/track_lin_vel_xy_exp: 0.4402
Episode_Reward/track_ang_vel_z_exp: 0.2562
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.0478
         Episode_Reward/dof_acc_l2: -0.1139
     Episode_Reward/action_rate_l2: -0.0488
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0080
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8295
Metrics/base_velocity/error_vel_xy: 0.4205
Metrics/base_velocity/error_vel_yaw: 0.2887
      Episode_Termination/time_out: 0.5575
  Episode_Termination/base_contact: 0.4425
--------------------------------------------------------------------------------
                   Total timesteps: 25731072
                    Iteration time: 2.50s
                      Time elapsed: 00:44:22
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1047/1400 [0m                     

                       Computation: 9612 steps/s (collection: 2.293s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0163
                 Mean entropy loss: 5.7953
                       Mean reward: 9.53
               Mean episode length: 706.53
Episode_Reward/track_lin_vel_xy_exp: 0.5099
Episode_Reward/track_ang_vel_z_exp: 0.2788
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.0452
     Episode_Reward/dof_torques_l2: -0.0516
         Episode_Reward/dof_acc_l2: -0.1210
     Episode_Reward/action_rate_l2: -0.0516
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8373
Metrics/base_velocity/error_vel_xy: 0.3717
Metrics/base_velocity/error_vel_yaw: 0.2823
      Episode_Termination/time_out: 0.5579
  Episode_Termination/base_contact: 0.4421
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.56s
                      Time elapsed: 00:44:24
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1048/1400 [0m                     

                       Computation: 9823 steps/s (collection: 2.246s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.8033
                       Mean reward: 9.57
               Mean episode length: 708.90
Episode_Reward/track_lin_vel_xy_exp: 0.5131
Episode_Reward/track_ang_vel_z_exp: 0.3062
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0478
     Episode_Reward/dof_torques_l2: -0.0588
         Episode_Reward/dof_acc_l2: -0.1379
     Episode_Reward/action_rate_l2: -0.0581
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8392
Metrics/base_velocity/error_vel_xy: 0.4997
Metrics/base_velocity/error_vel_yaw: 0.3171
      Episode_Termination/time_out: 0.5530
  Episode_Termination/base_contact: 0.4470
--------------------------------------------------------------------------------
                   Total timesteps: 25780224
                    Iteration time: 2.50s
                      Time elapsed: 00:44:27
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1049/1400 [0m                     

                       Computation: 9857 steps/s (collection: 2.234s, learning 0.259s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.8109
                       Mean reward: 9.77
               Mean episode length: 715.46
Episode_Reward/track_lin_vel_xy_exp: 0.4444
Episode_Reward/track_ang_vel_z_exp: 0.2523
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0381
     Episode_Reward/dof_torques_l2: -0.0476
         Episode_Reward/dof_acc_l2: -0.1138
     Episode_Reward/action_rate_l2: -0.0474
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0073
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8435
Metrics/base_velocity/error_vel_xy: 0.3506
Metrics/base_velocity/error_vel_yaw: 0.2429
      Episode_Termination/time_out: 0.5456
  Episode_Termination/base_contact: 0.4544
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 2.49s
                      Time elapsed: 00:44:29
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1050/1400 [0m                     

                       Computation: 9820 steps/s (collection: 2.247s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.8068
                       Mean reward: 9.28
               Mean episode length: 719.74
Episode_Reward/track_lin_vel_xy_exp: 0.3938
Episode_Reward/track_ang_vel_z_exp: 0.2455
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.0411
     Episode_Reward/dof_torques_l2: -0.0513
         Episode_Reward/dof_acc_l2: -0.1160
     Episode_Reward/action_rate_l2: -0.0484
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0113
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8493
Metrics/base_velocity/error_vel_xy: 0.4829
Metrics/base_velocity/error_vel_yaw: 0.3059
      Episode_Termination/time_out: 0.5425
  Episode_Termination/base_contact: 0.4575
--------------------------------------------------------------------------------
                   Total timesteps: 25829376
                    Iteration time: 2.50s
                      Time elapsed: 00:44:32
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1051/1400 [0m                     

                       Computation: 9682 steps/s (collection: 2.281s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.8100
                       Mean reward: 9.60
               Mean episode length: 726.64
Episode_Reward/track_lin_vel_xy_exp: 0.5911
Episode_Reward/track_ang_vel_z_exp: 0.3252
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0498
     Episode_Reward/dof_torques_l2: -0.0581
         Episode_Reward/dof_acc_l2: -0.1398
     Episode_Reward/action_rate_l2: -0.0604
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8537
Metrics/base_velocity/error_vel_xy: 0.4275
Metrics/base_velocity/error_vel_yaw: 0.3166
      Episode_Termination/time_out: 0.5405
  Episode_Termination/base_contact: 0.4595
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.54s
                      Time elapsed: 00:44:35
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1052/1400 [0m                     

                       Computation: 9694 steps/s (collection: 2.280s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.8196
                       Mean reward: 9.99
               Mean episode length: 741.67
Episode_Reward/track_lin_vel_xy_exp: 0.4988
Episode_Reward/track_ang_vel_z_exp: 0.2809
       Episode_Reward/lin_vel_z_l2: -0.0334
      Episode_Reward/ang_vel_xy_l2: -0.0434
     Episode_Reward/dof_torques_l2: -0.0519
         Episode_Reward/dof_acc_l2: -0.1225
     Episode_Reward/action_rate_l2: -0.0524
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8655
Metrics/base_velocity/error_vel_xy: 0.3931
Metrics/base_velocity/error_vel_yaw: 0.2802
      Episode_Termination/time_out: 0.5389
  Episode_Termination/base_contact: 0.4611
--------------------------------------------------------------------------------
                   Total timesteps: 25878528
                    Iteration time: 2.53s
                      Time elapsed: 00:44:37
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1053/1400 [0m                     

                       Computation: 9876 steps/s (collection: 2.231s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.8263
                       Mean reward: 10.38
               Mean episode length: 752.67
Episode_Reward/track_lin_vel_xy_exp: 0.5823
Episode_Reward/track_ang_vel_z_exp: 0.3332
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1351
     Episode_Reward/action_rate_l2: -0.0623
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8734
Metrics/base_velocity/error_vel_xy: 0.4956
Metrics/base_velocity/error_vel_yaw: 0.3358
      Episode_Termination/time_out: 0.5396
  Episode_Termination/base_contact: 0.4604
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 2.49s
                      Time elapsed: 00:44:40
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1054/1400 [0m                     

                       Computation: 9723 steps/s (collection: 2.265s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.8211
                       Mean reward: 9.45
               Mean episode length: 717.25
Episode_Reward/track_lin_vel_xy_exp: 0.4717
Episode_Reward/track_ang_vel_z_exp: 0.2753
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0423
     Episode_Reward/dof_torques_l2: -0.0491
         Episode_Reward/dof_acc_l2: -0.1139
     Episode_Reward/action_rate_l2: -0.0517
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8713
Metrics/base_velocity/error_vel_xy: 0.4331
Metrics/base_velocity/error_vel_yaw: 0.2947
      Episode_Termination/time_out: 0.5385
  Episode_Termination/base_contact: 0.4615
--------------------------------------------------------------------------------
                   Total timesteps: 25927680
                    Iteration time: 2.53s
                      Time elapsed: 00:44:42
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1055/1400 [0m                     

                       Computation: 9688 steps/s (collection: 2.281s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 5.8140
                       Mean reward: 10.27
               Mean episode length: 731.75
Episode_Reward/track_lin_vel_xy_exp: 0.5473
Episode_Reward/track_ang_vel_z_exp: 0.2978
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0445
     Episode_Reward/dof_torques_l2: -0.0526
         Episode_Reward/dof_acc_l2: -0.1257
     Episode_Reward/action_rate_l2: -0.0550
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8647
Metrics/base_velocity/error_vel_xy: 0.3837
Metrics/base_velocity/error_vel_yaw: 0.2936
      Episode_Termination/time_out: 0.5369
  Episode_Termination/base_contact: 0.4631
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.54s
                      Time elapsed: 00:44:45
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1056/1400 [0m                     

                       Computation: 9706 steps/s (collection: 2.277s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.8047
                       Mean reward: 9.76
               Mean episode length: 718.27
Episode_Reward/track_lin_vel_xy_exp: 0.5010
Episode_Reward/track_ang_vel_z_exp: 0.2923
       Episode_Reward/lin_vel_z_l2: -0.0353
      Episode_Reward/ang_vel_xy_l2: -0.0448
     Episode_Reward/dof_torques_l2: -0.0556
         Episode_Reward/dof_acc_l2: -0.1350
     Episode_Reward/action_rate_l2: -0.0560
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8624
Metrics/base_velocity/error_vel_xy: 0.4728
Metrics/base_velocity/error_vel_yaw: 0.3101
      Episode_Termination/time_out: 0.5398
  Episode_Termination/base_contact: 0.4602
--------------------------------------------------------------------------------
                   Total timesteps: 25976832
                    Iteration time: 2.53s
                      Time elapsed: 00:44:47
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1057/1400 [0m                     

                       Computation: 9709 steps/s (collection: 2.279s, learning 0.252s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.7974
                       Mean reward: 9.24
               Mean episode length: 699.47
Episode_Reward/track_lin_vel_xy_exp: 0.4424
Episode_Reward/track_ang_vel_z_exp: 0.2593
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.1269
     Episode_Reward/action_rate_l2: -0.0498
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8594
Metrics/base_velocity/error_vel_xy: 0.4237
Metrics/base_velocity/error_vel_yaw: 0.2840
      Episode_Termination/time_out: 0.5366
  Episode_Termination/base_contact: 0.4634
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 2.53s
                      Time elapsed: 00:44:50
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1058/1400 [0m                     

                       Computation: 9688 steps/s (collection: 2.280s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 5.7926
                       Mean reward: 9.47
               Mean episode length: 735.44
Episode_Reward/track_lin_vel_xy_exp: 0.5857
Episode_Reward/track_ang_vel_z_exp: 0.3367
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0514
     Episode_Reward/dof_torques_l2: -0.0615
         Episode_Reward/dof_acc_l2: -0.1459
     Episode_Reward/action_rate_l2: -0.0620
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0078
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8629
Metrics/base_velocity/error_vel_xy: 0.4808
Metrics/base_velocity/error_vel_yaw: 0.3326
      Episode_Termination/time_out: 0.5404
  Episode_Termination/base_contact: 0.4596
--------------------------------------------------------------------------------
                   Total timesteps: 26025984
                    Iteration time: 2.54s
                      Time elapsed: 00:44:52
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1059/1400 [0m                     

                       Computation: 9823 steps/s (collection: 2.247s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7790
                       Mean reward: 10.93
               Mean episode length: 801.30
Episode_Reward/track_lin_vel_xy_exp: 0.6393
Episode_Reward/track_ang_vel_z_exp: 0.3673
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0671
         Episode_Reward/dof_acc_l2: -0.1447
     Episode_Reward/action_rate_l2: -0.0667
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0081
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8675
Metrics/base_velocity/error_vel_xy: 0.5209
Metrics/base_velocity/error_vel_yaw: 0.3425
      Episode_Termination/time_out: 0.5438
  Episode_Termination/base_contact: 0.4562
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.50s
                      Time elapsed: 00:44:55
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1060/1400 [0m                     

                       Computation: 9773 steps/s (collection: 2.257s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.7709
                       Mean reward: 11.32
               Mean episode length: 827.52
Episode_Reward/track_lin_vel_xy_exp: 0.5945
Episode_Reward/track_ang_vel_z_exp: 0.3446
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.0504
     Episode_Reward/dof_torques_l2: -0.0630
         Episode_Reward/dof_acc_l2: -0.1472
     Episode_Reward/action_rate_l2: -0.0643
      Episode_Reward/feet_air_time: -0.0104
 Episode_Reward/undesired_contacts: -0.0147
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8687
Metrics/base_velocity/error_vel_xy: 0.5192
Metrics/base_velocity/error_vel_yaw: 0.3502
      Episode_Termination/time_out: 0.5430
  Episode_Termination/base_contact: 0.4570
--------------------------------------------------------------------------------
                   Total timesteps: 26075136
                    Iteration time: 2.51s
                      Time elapsed: 00:44:57
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1061/1400 [0m                     

                       Computation: 9856 steps/s (collection: 2.238s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7554
                       Mean reward: 11.78
               Mean episode length: 863.60
Episode_Reward/track_lin_vel_xy_exp: 0.6761
Episode_Reward/track_ang_vel_z_exp: 0.3728
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0540
     Episode_Reward/dof_torques_l2: -0.0668
         Episode_Reward/dof_acc_l2: -0.1641
     Episode_Reward/action_rate_l2: -0.0688
      Episode_Reward/feet_air_time: -0.0108
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8718
Metrics/base_velocity/error_vel_xy: 0.4772
Metrics/base_velocity/error_vel_yaw: 0.3496
      Episode_Termination/time_out: 0.5455
  Episode_Termination/base_contact: 0.4545
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 2.49s
                      Time elapsed: 00:45:00
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1062/1400 [0m                     

                       Computation: 9759 steps/s (collection: 2.262s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 5.7545
                       Mean reward: 11.30
               Mean episode length: 836.75
Episode_Reward/track_lin_vel_xy_exp: 0.5692
Episode_Reward/track_ang_vel_z_exp: 0.3146
       Episode_Reward/lin_vel_z_l2: -0.0351
      Episode_Reward/ang_vel_xy_l2: -0.0460
     Episode_Reward/dof_torques_l2: -0.0545
         Episode_Reward/dof_acc_l2: -0.1261
     Episode_Reward/action_rate_l2: -0.0572
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8798
Metrics/base_velocity/error_vel_xy: 0.4120
Metrics/base_velocity/error_vel_yaw: 0.3098
      Episode_Termination/time_out: 0.5474
  Episode_Termination/base_contact: 0.4526
--------------------------------------------------------------------------------
                   Total timesteps: 26124288
                    Iteration time: 2.52s
                      Time elapsed: 00:45:02
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 1063/1400 [0m                     

                       Computation: 9691 steps/s (collection: 2.282s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 5.7522
                       Mean reward: 11.89
               Mean episode length: 839.54
Episode_Reward/track_lin_vel_xy_exp: 0.5718
Episode_Reward/track_ang_vel_z_exp: 0.3247
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.0486
     Episode_Reward/dof_torques_l2: -0.0586
         Episode_Reward/dof_acc_l2: -0.1316
     Episode_Reward/action_rate_l2: -0.0592
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8803
Metrics/base_velocity/error_vel_xy: 0.4491
Metrics/base_velocity/error_vel_yaw: 0.3064
      Episode_Termination/time_out: 0.5480
  Episode_Termination/base_contact: 0.4520
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.54s
                      Time elapsed: 00:45:05
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1064/1400 [0m                     

                       Computation: 9819 steps/s (collection: 2.240s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 5.7570
                       Mean reward: 11.47
               Mean episode length: 799.40
Episode_Reward/track_lin_vel_xy_exp: 0.6212
Episode_Reward/track_ang_vel_z_exp: 0.3417
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.0473
     Episode_Reward/dof_torques_l2: -0.0565
         Episode_Reward/dof_acc_l2: -0.1272
     Episode_Reward/action_rate_l2: -0.0614
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8940
Metrics/base_velocity/error_vel_xy: 0.4088
Metrics/base_velocity/error_vel_yaw: 0.2980
      Episode_Termination/time_out: 0.5532
  Episode_Termination/base_contact: 0.4468
--------------------------------------------------------------------------------
                   Total timesteps: 26173440
                    Iteration time: 2.50s
                      Time elapsed: 00:45:07
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1065/1400 [0m                     

                       Computation: 9810 steps/s (collection: 2.249s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.7555
                       Mean reward: 11.39
               Mean episode length: 800.08
Episode_Reward/track_lin_vel_xy_exp: 0.5944
Episode_Reward/track_ang_vel_z_exp: 0.3191
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0487
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.1403
     Episode_Reward/action_rate_l2: -0.0592
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0086
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8989
Metrics/base_velocity/error_vel_xy: 0.4202
Metrics/base_velocity/error_vel_yaw: 0.3399
      Episode_Termination/time_out: 0.5581
  Episode_Termination/base_contact: 0.4419
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 2.51s
                      Time elapsed: 00:45:10
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1066/1400 [0m                     

                       Computation: 9887 steps/s (collection: 2.222s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7565
                       Mean reward: 9.97
               Mean episode length: 719.59
Episode_Reward/track_lin_vel_xy_exp: 0.4107
Episode_Reward/track_ang_vel_z_exp: 0.2297
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0367
     Episode_Reward/dof_torques_l2: -0.0487
         Episode_Reward/dof_acc_l2: -0.1150
     Episode_Reward/action_rate_l2: -0.0457
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8906
Metrics/base_velocity/error_vel_xy: 0.3445
Metrics/base_velocity/error_vel_yaw: 0.2612
      Episode_Termination/time_out: 0.5552
  Episode_Termination/base_contact: 0.4448
--------------------------------------------------------------------------------
                   Total timesteps: 26222592
                    Iteration time: 2.49s
                      Time elapsed: 00:45:12
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1067/1400 [0m                     

                       Computation: 9961 steps/s (collection: 2.206s, learning 0.261s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7574
                       Mean reward: 9.51
               Mean episode length: 710.87
Episode_Reward/track_lin_vel_xy_exp: 0.4669
Episode_Reward/track_ang_vel_z_exp: 0.2844
       Episode_Reward/lin_vel_z_l2: -0.0344
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0509
         Episode_Reward/dof_acc_l2: -0.1286
     Episode_Reward/action_rate_l2: -0.0538
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0099
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8828
Metrics/base_velocity/error_vel_xy: 0.4787
Metrics/base_velocity/error_vel_yaw: 0.3034
      Episode_Termination/time_out: 0.5559
  Episode_Termination/base_contact: 0.4441
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.47s
                      Time elapsed: 00:45:15
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1068/1400 [0m                     

                       Computation: 9790 steps/s (collection: 2.254s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.7686
                       Mean reward: 8.83
               Mean episode length: 683.97
Episode_Reward/track_lin_vel_xy_exp: 0.4833
Episode_Reward/track_ang_vel_z_exp: 0.2703
       Episode_Reward/lin_vel_z_l2: -0.0289
      Episode_Reward/ang_vel_xy_l2: -0.0392
     Episode_Reward/dof_torques_l2: -0.0504
         Episode_Reward/dof_acc_l2: -0.1228
     Episode_Reward/action_rate_l2: -0.0504
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8882
Metrics/base_velocity/error_vel_xy: 0.3584
Metrics/base_velocity/error_vel_yaw: 0.2574
      Episode_Termination/time_out: 0.5527
  Episode_Termination/base_contact: 0.4473
--------------------------------------------------------------------------------
                   Total timesteps: 26271744
                    Iteration time: 2.51s
                      Time elapsed: 00:45:17
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1069/1400 [0m                     

                       Computation: 9691 steps/s (collection: 2.281s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 5.7769
                       Mean reward: 9.98
               Mean episode length: 746.85
Episode_Reward/track_lin_vel_xy_exp: 0.5445
Episode_Reward/track_ang_vel_z_exp: 0.3152
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.1281
     Episode_Reward/action_rate_l2: -0.0594
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0081
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8902
Metrics/base_velocity/error_vel_xy: 0.4948
Metrics/base_velocity/error_vel_yaw: 0.3428
      Episode_Termination/time_out: 0.5542
  Episode_Termination/base_contact: 0.4458
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 2.54s
                      Time elapsed: 00:45:20
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1070/1400 [0m                     

                       Computation: 9848 steps/s (collection: 2.242s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.7774
                       Mean reward: 10.33
               Mean episode length: 760.49
Episode_Reward/track_lin_vel_xy_exp: 0.5597
Episode_Reward/track_ang_vel_z_exp: 0.3118
       Episode_Reward/lin_vel_z_l2: -0.0355
      Episode_Reward/ang_vel_xy_l2: -0.0474
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1319
     Episode_Reward/action_rate_l2: -0.0572
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8890
Metrics/base_velocity/error_vel_xy: 0.4276
Metrics/base_velocity/error_vel_yaw: 0.3138
      Episode_Termination/time_out: 0.5576
  Episode_Termination/base_contact: 0.4424
--------------------------------------------------------------------------------
                   Total timesteps: 26320896
                    Iteration time: 2.50s
                      Time elapsed: 00:45:22
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1071/1400 [0m                     

                       Computation: 9901 steps/s (collection: 2.228s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 5.7750
                       Mean reward: 10.44
               Mean episode length: 778.77
Episode_Reward/track_lin_vel_xy_exp: 0.5828
Episode_Reward/track_ang_vel_z_exp: 0.3340
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.0526
     Episode_Reward/dof_torques_l2: -0.0620
         Episode_Reward/dof_acc_l2: -0.1607
     Episode_Reward/action_rate_l2: -0.0633
      Episode_Reward/feet_air_time: -0.0099
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8795
Metrics/base_velocity/error_vel_xy: 0.4869
Metrics/base_velocity/error_vel_yaw: 0.3320
      Episode_Termination/time_out: 0.5544
  Episode_Termination/base_contact: 0.4456
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.48s
                      Time elapsed: 00:45:25
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1072/1400 [0m                     

                       Computation: 9822 steps/s (collection: 2.240s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.7777
                       Mean reward: 10.39
               Mean episode length: 764.35
Episode_Reward/track_lin_vel_xy_exp: 0.6176
Episode_Reward/track_ang_vel_z_exp: 0.3451
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0520
     Episode_Reward/dof_torques_l2: -0.0617
         Episode_Reward/dof_acc_l2: -0.1565
     Episode_Reward/action_rate_l2: -0.0637
      Episode_Reward/feet_air_time: -0.0100
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8829
Metrics/base_velocity/error_vel_xy: 0.4346
Metrics/base_velocity/error_vel_yaw: 0.3026
      Episode_Termination/time_out: 0.5555
  Episode_Termination/base_contact: 0.4445
--------------------------------------------------------------------------------
                   Total timesteps: 26370048
                    Iteration time: 2.50s
                      Time elapsed: 00:45:27
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1073/1400 [0m                     

                       Computation: 9870 steps/s (collection: 2.232s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.7827
                       Mean reward: 10.43
               Mean episode length: 761.60
Episode_Reward/track_lin_vel_xy_exp: 0.5886
Episode_Reward/track_ang_vel_z_exp: 0.3406
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0631
         Episode_Reward/dof_acc_l2: -0.1512
     Episode_Reward/action_rate_l2: -0.0635
      Episode_Reward/feet_air_time: -0.0103
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8847
Metrics/base_velocity/error_vel_xy: 0.5121
Metrics/base_velocity/error_vel_yaw: 0.3469
      Episode_Termination/time_out: 0.5557
  Episode_Termination/base_contact: 0.4443
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 2.49s
                      Time elapsed: 00:45:30
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1074/1400 [0m                     

                       Computation: 9835 steps/s (collection: 2.245s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.7756
                       Mean reward: 11.07
               Mean episode length: 792.40
Episode_Reward/track_lin_vel_xy_exp: 0.6368
Episode_Reward/track_ang_vel_z_exp: 0.3466
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0606
         Episode_Reward/dof_acc_l2: -0.1416
     Episode_Reward/action_rate_l2: -0.0624
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8914
Metrics/base_velocity/error_vel_xy: 0.4180
Metrics/base_velocity/error_vel_yaw: 0.3249
      Episode_Termination/time_out: 0.5569
  Episode_Termination/base_contact: 0.4431
--------------------------------------------------------------------------------
                   Total timesteps: 26419200
                    Iteration time: 2.50s
                      Time elapsed: 00:45:32
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1075/1400 [0m                     

                       Computation: 9851 steps/s (collection: 2.238s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 5.7799
                       Mean reward: 11.80
               Mean episode length: 839.37
Episode_Reward/track_lin_vel_xy_exp: 0.7288
Episode_Reward/track_ang_vel_z_exp: 0.4055
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.0605
     Episode_Reward/dof_torques_l2: -0.0749
         Episode_Reward/dof_acc_l2: -0.1847
     Episode_Reward/action_rate_l2: -0.0733
      Episode_Reward/feet_air_time: -0.0108
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8974
Metrics/base_velocity/error_vel_xy: 0.5222
Metrics/base_velocity/error_vel_yaw: 0.3707
      Episode_Termination/time_out: 0.5567
  Episode_Termination/base_contact: 0.4433
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.49s
                      Time elapsed: 00:45:35
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1076/1400 [0m                     

                       Computation: 9764 steps/s (collection: 2.260s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 5.7706
                       Mean reward: 11.43
               Mean episode length: 828.23
Episode_Reward/track_lin_vel_xy_exp: 0.5334
Episode_Reward/track_ang_vel_z_exp: 0.3060
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0451
     Episode_Reward/dof_torques_l2: -0.0544
         Episode_Reward/dof_acc_l2: -0.1275
     Episode_Reward/action_rate_l2: -0.0559
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8960
Metrics/base_velocity/error_vel_xy: 0.4195
Metrics/base_velocity/error_vel_yaw: 0.2908
      Episode_Termination/time_out: 0.5579
  Episode_Termination/base_contact: 0.4421
--------------------------------------------------------------------------------
                   Total timesteps: 26468352
                    Iteration time: 2.52s
                      Time elapsed: 00:45:37
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1077/1400 [0m                     

                       Computation: 9778 steps/s (collection: 2.258s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7618
                       Mean reward: 10.97
               Mean episode length: 811.49
Episode_Reward/track_lin_vel_xy_exp: 0.4694
Episode_Reward/track_ang_vel_z_exp: 0.2652
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.0409
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.1194
     Episode_Reward/action_rate_l2: -0.0499
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8982
Metrics/base_velocity/error_vel_xy: 0.3792
Metrics/base_velocity/error_vel_yaw: 0.2689
      Episode_Termination/time_out: 0.5605
  Episode_Termination/base_contact: 0.4395
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 2.51s
                      Time elapsed: 00:45:40
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1078/1400 [0m                     

                       Computation: 9750 steps/s (collection: 2.266s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.7542
                       Mean reward: 10.64
               Mean episode length: 765.32
Episode_Reward/track_lin_vel_xy_exp: 0.5675
Episode_Reward/track_ang_vel_z_exp: 0.3150
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0603
         Episode_Reward/dof_acc_l2: -0.1364
     Episode_Reward/action_rate_l2: -0.0584
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9060
Metrics/base_velocity/error_vel_xy: 0.4154
Metrics/base_velocity/error_vel_yaw: 0.2983
      Episode_Termination/time_out: 0.5625
  Episode_Termination/base_contact: 0.4375
--------------------------------------------------------------------------------
                   Total timesteps: 26517504
                    Iteration time: 2.52s
                      Time elapsed: 00:45:42
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1079/1400 [0m                     

                       Computation: 9868 steps/s (collection: 2.228s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 5.7324
                       Mean reward: 10.76
               Mean episode length: 764.61
Episode_Reward/track_lin_vel_xy_exp: 0.4207
Episode_Reward/track_ang_vel_z_exp: 0.2376
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0393
     Episode_Reward/dof_torques_l2: -0.0425
         Episode_Reward/dof_acc_l2: -0.0985
     Episode_Reward/action_rate_l2: -0.0426
      Episode_Reward/feet_air_time: -0.0066
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9172
Metrics/base_velocity/error_vel_xy: 0.3275
Metrics/base_velocity/error_vel_yaw: 0.2313
      Episode_Termination/time_out: 0.5636
  Episode_Termination/base_contact: 0.4364
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.49s
                      Time elapsed: 00:45:45
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1080/1400 [0m                     

                       Computation: 9811 steps/s (collection: 2.249s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.7236
                       Mean reward: 10.35
               Mean episode length: 736.61
Episode_Reward/track_lin_vel_xy_exp: 0.4524
Episode_Reward/track_ang_vel_z_exp: 0.2547
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0377
     Episode_Reward/dof_torques_l2: -0.0455
         Episode_Reward/dof_acc_l2: -0.1064
     Episode_Reward/action_rate_l2: -0.0462
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0086
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9200
Metrics/base_velocity/error_vel_xy: 0.3546
Metrics/base_velocity/error_vel_yaw: 0.2509
      Episode_Termination/time_out: 0.5629
  Episode_Termination/base_contact: 0.4371
--------------------------------------------------------------------------------
                   Total timesteps: 26566656
                    Iteration time: 2.50s
                      Time elapsed: 00:45:47
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1081/1400 [0m                     

                       Computation: 9492 steps/s (collection: 2.332s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7112
                       Mean reward: 10.41
               Mean episode length: 756.47
Episode_Reward/track_lin_vel_xy_exp: 0.5784
Episode_Reward/track_ang_vel_z_exp: 0.3278
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1427
     Episode_Reward/action_rate_l2: -0.0604
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9276
Metrics/base_velocity/error_vel_xy: 0.4546
Metrics/base_velocity/error_vel_yaw: 0.3143
      Episode_Termination/time_out: 0.5636
  Episode_Termination/base_contact: 0.4364
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 2.59s
                      Time elapsed: 00:45:50
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1082/1400 [0m                     

                       Computation: 9646 steps/s (collection: 2.291s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.7135
                       Mean reward: 10.39
               Mean episode length: 744.05
Episode_Reward/track_lin_vel_xy_exp: 0.5947
Episode_Reward/track_ang_vel_z_exp: 0.3182
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.0434
     Episode_Reward/dof_torques_l2: -0.0536
         Episode_Reward/dof_acc_l2: -0.1209
     Episode_Reward/action_rate_l2: -0.0571
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9395
Metrics/base_velocity/error_vel_xy: 0.3778
Metrics/base_velocity/error_vel_yaw: 0.2931
      Episode_Termination/time_out: 0.5653
  Episode_Termination/base_contact: 0.4347
--------------------------------------------------------------------------------
                   Total timesteps: 26615808
                    Iteration time: 2.55s
                      Time elapsed: 00:45:52
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1083/1400 [0m                     

                       Computation: 9575 steps/s (collection: 2.309s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7075
                       Mean reward: 10.35
               Mean episode length: 761.71
Episode_Reward/track_lin_vel_xy_exp: 0.4855
Episode_Reward/track_ang_vel_z_exp: 0.2863
       Episode_Reward/lin_vel_z_l2: -0.0357
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0550
         Episode_Reward/dof_acc_l2: -0.1351
     Episode_Reward/action_rate_l2: -0.0552
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0279
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9420
Metrics/base_velocity/error_vel_xy: 0.5054
Metrics/base_velocity/error_vel_yaw: 0.3445
      Episode_Termination/time_out: 0.5622
  Episode_Termination/base_contact: 0.4378
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.57s
                      Time elapsed: 00:45:55
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1084/1400 [0m                     

                       Computation: 9625 steps/s (collection: 2.291s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 5.7012
                       Mean reward: 9.61
               Mean episode length: 756.79
Episode_Reward/track_lin_vel_xy_exp: 0.5331
Episode_Reward/track_ang_vel_z_exp: 0.3054
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.0550
     Episode_Reward/dof_torques_l2: -0.0572
         Episode_Reward/dof_acc_l2: -0.1513
     Episode_Reward/action_rate_l2: -0.0565
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0113
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9491
Metrics/base_velocity/error_vel_xy: 0.4785
Metrics/base_velocity/error_vel_yaw: 0.3295
      Episode_Termination/time_out: 0.5629
  Episode_Termination/base_contact: 0.4371
--------------------------------------------------------------------------------
                   Total timesteps: 26664960
                    Iteration time: 2.55s
                      Time elapsed: 00:45:58
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1085/1400 [0m                     

                       Computation: 9626 steps/s (collection: 2.295s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 5.6909
                       Mean reward: 9.01
               Mean episode length: 729.36
Episode_Reward/track_lin_vel_xy_exp: 0.5254
Episode_Reward/track_ang_vel_z_exp: 0.2992
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1380
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9542
Metrics/base_velocity/error_vel_xy: 0.4316
Metrics/base_velocity/error_vel_yaw: 0.3065
      Episode_Termination/time_out: 0.5649
  Episode_Termination/base_contact: 0.4351
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 2.55s
                      Time elapsed: 00:46:00
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1086/1400 [0m                     

                       Computation: 9675 steps/s (collection: 2.284s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.6763
                       Mean reward: 9.70
               Mean episode length: 752.62
Episode_Reward/track_lin_vel_xy_exp: 0.6068
Episode_Reward/track_ang_vel_z_exp: 0.3486
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.0545
     Episode_Reward/dof_torques_l2: -0.0632
         Episode_Reward/dof_acc_l2: -0.1451
     Episode_Reward/action_rate_l2: -0.0649
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0087
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9537
Metrics/base_velocity/error_vel_xy: 0.5294
Metrics/base_velocity/error_vel_yaw: 0.3617
      Episode_Termination/time_out: 0.5701
  Episode_Termination/base_contact: 0.4299
--------------------------------------------------------------------------------
                   Total timesteps: 26714112
                    Iteration time: 2.54s
                      Time elapsed: 00:46:03
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1087/1400 [0m                     

                       Computation: 9565 steps/s (collection: 2.305s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 5.6796
                       Mean reward: 10.16
               Mean episode length: 772.94
Episode_Reward/track_lin_vel_xy_exp: 0.6322
Episode_Reward/track_ang_vel_z_exp: 0.3400
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0489
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.1383
     Episode_Reward/action_rate_l2: -0.0629
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0083
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9544
Metrics/base_velocity/error_vel_xy: 0.4353
Metrics/base_velocity/error_vel_yaw: 0.3401
      Episode_Termination/time_out: 0.5699
  Episode_Termination/base_contact: 0.4301
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.57s
                      Time elapsed: 00:46:05
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1088/1400 [0m                     

                       Computation: 9532 steps/s (collection: 2.322s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0122
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.6850
                       Mean reward: 10.88
               Mean episode length: 805.59
Episode_Reward/track_lin_vel_xy_exp: 0.5749
Episode_Reward/track_ang_vel_z_exp: 0.3192
       Episode_Reward/lin_vel_z_l2: -0.0344
      Episode_Reward/ang_vel_xy_l2: -0.0462
     Episode_Reward/dof_torques_l2: -0.0578
         Episode_Reward/dof_acc_l2: -0.1360
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9589
Metrics/base_velocity/error_vel_xy: 0.4265
Metrics/base_velocity/error_vel_yaw: 0.3016
      Episode_Termination/time_out: 0.5716
  Episode_Termination/base_contact: 0.4284
--------------------------------------------------------------------------------
                   Total timesteps: 26763264
                    Iteration time: 2.58s
                      Time elapsed: 00:46:08
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1089/1400 [0m                     

                       Computation: 9581 steps/s (collection: 2.309s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.6769
                       Mean reward: 10.49
               Mean episode length: 771.45
Episode_Reward/track_lin_vel_xy_exp: 0.6005
Episode_Reward/track_ang_vel_z_exp: 0.3303
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0458
     Episode_Reward/dof_torques_l2: -0.0576
         Episode_Reward/dof_acc_l2: -0.1383
     Episode_Reward/action_rate_l2: -0.0597
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9626
Metrics/base_velocity/error_vel_xy: 0.4186
Metrics/base_velocity/error_vel_yaw: 0.3127
      Episode_Termination/time_out: 0.5774
  Episode_Termination/base_contact: 0.4226
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 2.57s
                      Time elapsed: 00:46:10
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1090/1400 [0m                     

                       Computation: 9632 steps/s (collection: 2.286s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.6702
                       Mean reward: 10.37
               Mean episode length: 744.45
Episode_Reward/track_lin_vel_xy_exp: 0.5920
Episode_Reward/track_ang_vel_z_exp: 0.3206
       Episode_Reward/lin_vel_z_l2: -0.0355
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1265
     Episode_Reward/action_rate_l2: -0.0570
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9552
Metrics/base_velocity/error_vel_xy: 0.3907
Metrics/base_velocity/error_vel_yaw: 0.2973
      Episode_Termination/time_out: 0.5806
  Episode_Termination/base_contact: 0.4194
--------------------------------------------------------------------------------
                   Total timesteps: 26812416
                    Iteration time: 2.55s
                      Time elapsed: 00:46:13
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1091/1400 [0m                     

                       Computation: 9627 steps/s (collection: 2.286s, learning 0.266s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.6719
                       Mean reward: 10.47
               Mean episode length: 745.26
Episode_Reward/track_lin_vel_xy_exp: 0.6161
Episode_Reward/track_ang_vel_z_exp: 0.3365
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1359
     Episode_Reward/action_rate_l2: -0.0602
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9525
Metrics/base_velocity/error_vel_xy: 0.4141
Metrics/base_velocity/error_vel_yaw: 0.3095
      Episode_Termination/time_out: 0.5858
  Episode_Termination/base_contact: 0.4142
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.55s
                      Time elapsed: 00:46:15
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1092/1400 [0m                     

                       Computation: 9763 steps/s (collection: 2.264s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 5.6842
                       Mean reward: 10.74
               Mean episode length: 771.49
Episode_Reward/track_lin_vel_xy_exp: 0.5987
Episode_Reward/track_ang_vel_z_exp: 0.3476
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0500
     Episode_Reward/dof_torques_l2: -0.0651
         Episode_Reward/dof_acc_l2: -0.1502
     Episode_Reward/action_rate_l2: -0.0635
      Episode_Reward/feet_air_time: -0.0104
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9533
Metrics/base_velocity/error_vel_xy: 0.5173
Metrics/base_velocity/error_vel_yaw: 0.3421
      Episode_Termination/time_out: 0.5889
  Episode_Termination/base_contact: 0.4111
--------------------------------------------------------------------------------
                   Total timesteps: 26861568
                    Iteration time: 2.52s
                      Time elapsed: 00:46:18
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1093/1400 [0m                     

                       Computation: 9811 steps/s (collection: 2.241s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.6925
                       Mean reward: 11.44
               Mean episode length: 801.00
Episode_Reward/track_lin_vel_xy_exp: 0.5761
Episode_Reward/track_ang_vel_z_exp: 0.3120
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0460
     Episode_Reward/dof_torques_l2: -0.0569
         Episode_Reward/dof_acc_l2: -0.1270
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9575
Metrics/base_velocity/error_vel_xy: 0.4050
Metrics/base_velocity/error_vel_yaw: 0.3120
      Episode_Termination/time_out: 0.5923
  Episode_Termination/base_contact: 0.4077
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 2.50s
                      Time elapsed: 00:46:20
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1094/1400 [0m                     

                       Computation: 9760 steps/s (collection: 2.263s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.6915
                       Mean reward: 10.17
               Mean episode length: 751.98
Episode_Reward/track_lin_vel_xy_exp: 0.5396
Episode_Reward/track_ang_vel_z_exp: 0.3090
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.0473
     Episode_Reward/dof_torques_l2: -0.0597
         Episode_Reward/dof_acc_l2: -0.1447
     Episode_Reward/action_rate_l2: -0.0578
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9621
Metrics/base_velocity/error_vel_xy: 0.4438
Metrics/base_velocity/error_vel_yaw: 0.3013
      Episode_Termination/time_out: 0.5940
  Episode_Termination/base_contact: 0.4060
--------------------------------------------------------------------------------
                   Total timesteps: 26910720
                    Iteration time: 2.52s
                      Time elapsed: 00:46:23
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1095/1400 [0m                     

                       Computation: 9751 steps/s (collection: 2.257s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.6920
                       Mean reward: 9.67
               Mean episode length: 722.38
Episode_Reward/track_lin_vel_xy_exp: 0.4939
Episode_Reward/track_ang_vel_z_exp: 0.2874
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.0478
     Episode_Reward/dof_torques_l2: -0.0575
         Episode_Reward/dof_acc_l2: -0.1337
     Episode_Reward/action_rate_l2: -0.0537
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0106
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9600
Metrics/base_velocity/error_vel_xy: 0.4777
Metrics/base_velocity/error_vel_yaw: 0.3336
      Episode_Termination/time_out: 0.5939
  Episode_Termination/base_contact: 0.4061
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.52s
                      Time elapsed: 00:46:25
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1096/1400 [0m                     

                       Computation: 9801 steps/s (collection: 2.253s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.6991
                       Mean reward: 9.67
               Mean episode length: 729.64
Episode_Reward/track_lin_vel_xy_exp: 0.5409
Episode_Reward/track_ang_vel_z_exp: 0.2996
       Episode_Reward/lin_vel_z_l2: -0.0321
      Episode_Reward/ang_vel_xy_l2: -0.0424
     Episode_Reward/dof_torques_l2: -0.0517
         Episode_Reward/dof_acc_l2: -0.1188
     Episode_Reward/action_rate_l2: -0.0537
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9691
Metrics/base_velocity/error_vel_xy: 0.3963
Metrics/base_velocity/error_vel_yaw: 0.2836
      Episode_Termination/time_out: 0.5990
  Episode_Termination/base_contact: 0.4010
--------------------------------------------------------------------------------
                   Total timesteps: 26959872
                    Iteration time: 2.51s
                      Time elapsed: 00:46:28
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1097/1400 [0m                     

                       Computation: 9694 steps/s (collection: 2.279s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.6904
                       Mean reward: 10.67
               Mean episode length: 789.71
Episode_Reward/track_lin_vel_xy_exp: 0.5800
Episode_Reward/track_ang_vel_z_exp: 0.3423
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.0535
     Episode_Reward/dof_torques_l2: -0.0615
         Episode_Reward/dof_acc_l2: -0.1492
     Episode_Reward/action_rate_l2: -0.0628
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9704
Metrics/base_velocity/error_vel_xy: 0.5360
Metrics/base_velocity/error_vel_yaw: 0.3433
      Episode_Termination/time_out: 0.6028
  Episode_Termination/base_contact: 0.3972
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 2.54s
                      Time elapsed: 00:46:31
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1098/1400 [0m                     

                       Computation: 9789 steps/s (collection: 2.255s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.6857
                       Mean reward: 11.22
               Mean episode length: 789.18
Episode_Reward/track_lin_vel_xy_exp: 0.6236
Episode_Reward/track_ang_vel_z_exp: 0.3417
       Episode_Reward/lin_vel_z_l2: -0.0438
      Episode_Reward/ang_vel_xy_l2: -0.0518
     Episode_Reward/dof_torques_l2: -0.0620
         Episode_Reward/dof_acc_l2: -0.1644
     Episode_Reward/action_rate_l2: -0.0620
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9786
Metrics/base_velocity/error_vel_xy: 0.4365
Metrics/base_velocity/error_vel_yaw: 0.3289
      Episode_Termination/time_out: 0.6050
  Episode_Termination/base_contact: 0.3950
--------------------------------------------------------------------------------
                   Total timesteps: 27009024
                    Iteration time: 2.51s
                      Time elapsed: 00:46:33
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1099/1400 [0m                     

                       Computation: 9785 steps/s (collection: 2.255s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0188
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.6799
                       Mean reward: 10.63
               Mean episode length: 779.72
Episode_Reward/track_lin_vel_xy_exp: 0.5965
Episode_Reward/track_ang_vel_z_exp: 0.3301
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.0519
     Episode_Reward/dof_torques_l2: -0.0595
         Episode_Reward/dof_acc_l2: -0.1466
     Episode_Reward/action_rate_l2: -0.0601
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9875
Metrics/base_velocity/error_vel_xy: 0.4374
Metrics/base_velocity/error_vel_yaw: 0.3262
      Episode_Termination/time_out: 0.6104
  Episode_Termination/base_contact: 0.3905
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.51s
                      Time elapsed: 00:46:36
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1100/1400 [0m                     

                       Computation: 9771 steps/s (collection: 2.260s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.6677
                       Mean reward: 10.43
               Mean episode length: 757.48
Episode_Reward/track_lin_vel_xy_exp: 0.5392
Episode_Reward/track_ang_vel_z_exp: 0.3021
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0485
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.1349
     Episode_Reward/action_rate_l2: -0.0562
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9972
Metrics/base_velocity/error_vel_xy: 0.4610
Metrics/base_velocity/error_vel_yaw: 0.3507
      Episode_Termination/time_out: 0.6100
  Episode_Termination/base_contact: 0.3910
--------------------------------------------------------------------------------
                   Total timesteps: 27058176
                    Iteration time: 2.52s
                      Time elapsed: 00:46:38
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1101/1400 [0m                     

                       Computation: 9897 steps/s (collection: 2.222s, learning 0.261s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 5.6657
                       Mean reward: 9.94
               Mean episode length: 721.58
Episode_Reward/track_lin_vel_xy_exp: 0.5713
Episode_Reward/track_ang_vel_z_exp: 0.3083
       Episode_Reward/lin_vel_z_l2: -0.0324
      Episode_Reward/ang_vel_xy_l2: -0.0423
     Episode_Reward/dof_torques_l2: -0.0550
         Episode_Reward/dof_acc_l2: -0.1291
     Episode_Reward/action_rate_l2: -0.0565
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0071
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0010
Metrics/base_velocity/error_vel_xy: 0.3902
Metrics/base_velocity/error_vel_yaw: 0.3059
      Episode_Termination/time_out: 0.6077
  Episode_Termination/base_contact: 0.3933
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 2.48s
                      Time elapsed: 00:46:41
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1102/1400 [0m                     

                       Computation: 9795 steps/s (collection: 2.243s, learning 0.266s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 5.6663
                       Mean reward: 10.07
               Mean episode length: 734.18
Episode_Reward/track_lin_vel_xy_exp: 0.5908
Episode_Reward/track_ang_vel_z_exp: 0.3346
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0500
     Episode_Reward/dof_torques_l2: -0.0616
         Episode_Reward/dof_acc_l2: -0.1424
     Episode_Reward/action_rate_l2: -0.0602
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0023
Metrics/base_velocity/error_vel_xy: 0.4563
Metrics/base_velocity/error_vel_yaw: 0.3227
      Episode_Termination/time_out: 0.6057
  Episode_Termination/base_contact: 0.3953
--------------------------------------------------------------------------------
                   Total timesteps: 27107328
                    Iteration time: 2.51s
                      Time elapsed: 00:46:43
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1103/1400 [0m                     

                       Computation: 9603 steps/s (collection: 2.305s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.6653
                       Mean reward: 9.69
               Mean episode length: 719.05
Episode_Reward/track_lin_vel_xy_exp: 0.5228
Episode_Reward/track_ang_vel_z_exp: 0.2983
       Episode_Reward/lin_vel_z_l2: -0.0372
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.0585
         Episode_Reward/dof_acc_l2: -0.1384
     Episode_Reward/action_rate_l2: -0.0564
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0011
Metrics/base_velocity/error_vel_xy: 0.4705
Metrics/base_velocity/error_vel_yaw: 0.3394
      Episode_Termination/time_out: 0.6049
  Episode_Termination/base_contact: 0.3961
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.56s
                      Time elapsed: 00:46:46
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1104/1400 [0m                     

                       Computation: 9678 steps/s (collection: 2.276s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 5.6554
                       Mean reward: 10.40
               Mean episode length: 760.28
Episode_Reward/track_lin_vel_xy_exp: 0.6427
Episode_Reward/track_ang_vel_z_exp: 0.3485
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.0514
     Episode_Reward/dof_torques_l2: -0.0599
         Episode_Reward/dof_acc_l2: -0.1483
     Episode_Reward/action_rate_l2: -0.0626
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0035
Metrics/base_velocity/error_vel_xy: 0.4425
Metrics/base_velocity/error_vel_yaw: 0.3416
      Episode_Termination/time_out: 0.6043
  Episode_Termination/base_contact: 0.3967
--------------------------------------------------------------------------------
                   Total timesteps: 27156480
                    Iteration time: 2.54s
                      Time elapsed: 00:46:48
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1105/1400 [0m                     

                       Computation: 9620 steps/s (collection: 2.298s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 5.6614
                       Mean reward: 10.94
               Mean episode length: 775.37
Episode_Reward/track_lin_vel_xy_exp: 0.6561
Episode_Reward/track_ang_vel_z_exp: 0.3601
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0511
     Episode_Reward/dof_torques_l2: -0.0620
         Episode_Reward/dof_acc_l2: -0.1499
     Episode_Reward/action_rate_l2: -0.0639
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9992
Metrics/base_velocity/error_vel_xy: 0.4515
Metrics/base_velocity/error_vel_yaw: 0.3335
      Episode_Termination/time_out: 0.6027
  Episode_Termination/base_contact: 0.3983
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 2.55s
                      Time elapsed: 00:46:51
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1106/1400 [0m                     

                       Computation: 9595 steps/s (collection: 2.299s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.6611
                       Mean reward: 10.87
               Mean episode length: 780.05
Episode_Reward/track_lin_vel_xy_exp: 0.5683
Episode_Reward/track_ang_vel_z_exp: 0.3164
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0462
     Episode_Reward/dof_torques_l2: -0.0609
         Episode_Reward/dof_acc_l2: -0.1391
     Episode_Reward/action_rate_l2: -0.0589
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9921
Metrics/base_velocity/error_vel_xy: 0.4750
Metrics/base_velocity/error_vel_yaw: 0.3544
      Episode_Termination/time_out: 0.5985
  Episode_Termination/base_contact: 0.4025
--------------------------------------------------------------------------------
                   Total timesteps: 27205632
                    Iteration time: 2.56s
                      Time elapsed: 00:46:53
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1107/1400 [0m                     

                       Computation: 9687 steps/s (collection: 2.280s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.6673
                       Mean reward: 10.37
               Mean episode length: 771.27
Episode_Reward/track_lin_vel_xy_exp: 0.5180
Episode_Reward/track_ang_vel_z_exp: 0.3016
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0468
     Episode_Reward/dof_torques_l2: -0.0555
         Episode_Reward/dof_acc_l2: -0.1224
     Episode_Reward/action_rate_l2: -0.0541
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9942
Metrics/base_velocity/error_vel_xy: 0.4823
Metrics/base_velocity/error_vel_yaw: 0.3181
      Episode_Termination/time_out: 0.5950
  Episode_Termination/base_contact: 0.4060
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.54s
                      Time elapsed: 00:46:56
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1108/1400 [0m                     

                       Computation: 9835 steps/s (collection: 2.244s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.6631
                       Mean reward: 9.93
               Mean episode length: 756.94
Episode_Reward/track_lin_vel_xy_exp: 0.5641
Episode_Reward/track_ang_vel_z_exp: 0.3072
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0463
     Episode_Reward/dof_torques_l2: -0.0578
         Episode_Reward/dof_acc_l2: -0.1313
     Episode_Reward/action_rate_l2: -0.0559
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0049
Metrics/base_velocity/error_vel_xy: 0.4088
Metrics/base_velocity/error_vel_yaw: 0.3076
      Episode_Termination/time_out: 0.5979
  Episode_Termination/base_contact: 0.4030
--------------------------------------------------------------------------------
                   Total timesteps: 27254784
                    Iteration time: 2.50s
                      Time elapsed: 00:46:58
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1109/1400 [0m                     

                       Computation: 9650 steps/s (collection: 2.293s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 5.6850
                       Mean reward: 9.98
               Mean episode length: 759.89
Episode_Reward/track_lin_vel_xy_exp: 0.5869
Episode_Reward/track_ang_vel_z_exp: 0.3245
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0502
     Episode_Reward/dof_torques_l2: -0.0606
         Episode_Reward/dof_acc_l2: -0.1445
     Episode_Reward/action_rate_l2: -0.0600
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0068
Metrics/base_velocity/error_vel_xy: 0.4587
Metrics/base_velocity/error_vel_yaw: 0.3475
      Episode_Termination/time_out: 0.5935
  Episode_Termination/base_contact: 0.4075
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 2.55s
                      Time elapsed: 00:47:01
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1110/1400 [0m                     

                       Computation: 9869 steps/s (collection: 2.233s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.6939
                       Mean reward: 9.71
               Mean episode length: 744.22
Episode_Reward/track_lin_vel_xy_exp: 0.5477
Episode_Reward/track_ang_vel_z_exp: 0.3008
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0574
         Episode_Reward/dof_acc_l2: -0.1386
     Episode_Reward/action_rate_l2: -0.0557
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0142
Metrics/base_velocity/error_vel_xy: 0.4118
Metrics/base_velocity/error_vel_yaw: 0.3162
      Episode_Termination/time_out: 0.5940
  Episode_Termination/base_contact: 0.4070
--------------------------------------------------------------------------------
                   Total timesteps: 27303936
                    Iteration time: 2.49s
                      Time elapsed: 00:47:03
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1111/1400 [0m                     

                       Computation: 9769 steps/s (collection: 2.252s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 5.6874
                       Mean reward: 10.84
               Mean episode length: 785.99
Episode_Reward/track_lin_vel_xy_exp: 0.6034
Episode_Reward/track_ang_vel_z_exp: 0.3256
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0580
         Episode_Reward/dof_acc_l2: -0.1423
     Episode_Reward/action_rate_l2: -0.0589
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0253
Metrics/base_velocity/error_vel_xy: 0.4149
Metrics/base_velocity/error_vel_yaw: 0.3258
      Episode_Termination/time_out: 0.5959
  Episode_Termination/base_contact: 0.4051
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.52s
                      Time elapsed: 00:47:06
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1112/1400 [0m                     

                       Computation: 9894 steps/s (collection: 2.229s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.6809
                       Mean reward: 11.29
               Mean episode length: 812.72
Episode_Reward/track_lin_vel_xy_exp: 0.6006
Episode_Reward/track_ang_vel_z_exp: 0.3259
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0584
         Episode_Reward/dof_acc_l2: -0.1442
     Episode_Reward/action_rate_l2: -0.0591
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0340
Metrics/base_velocity/error_vel_xy: 0.4141
Metrics/base_velocity/error_vel_yaw: 0.3195
      Episode_Termination/time_out: 0.5983
  Episode_Termination/base_contact: 0.4026
--------------------------------------------------------------------------------
                   Total timesteps: 27353088
                    Iteration time: 2.48s
                      Time elapsed: 00:47:08
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1113/1400 [0m                     

                       Computation: 9853 steps/s (collection: 2.236s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.6781
                       Mean reward: 11.11
               Mean episode length: 793.19
Episode_Reward/track_lin_vel_xy_exp: 0.5543
Episode_Reward/track_ang_vel_z_exp: 0.2975
       Episode_Reward/lin_vel_z_l2: -0.0338
      Episode_Reward/ang_vel_xy_l2: -0.0462
     Episode_Reward/dof_torques_l2: -0.0550
         Episode_Reward/dof_acc_l2: -0.1282
     Episode_Reward/action_rate_l2: -0.0538
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0419
Metrics/base_velocity/error_vel_xy: 0.3710
Metrics/base_velocity/error_vel_yaw: 0.2952
      Episode_Termination/time_out: 0.6007
  Episode_Termination/base_contact: 0.4003
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 2.49s
                      Time elapsed: 00:47:11
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1114/1400 [0m                     

                       Computation: 9866 steps/s (collection: 2.236s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.6871
                       Mean reward: 10.46
               Mean episode length: 749.39
Episode_Reward/track_lin_vel_xy_exp: 0.4864
Episode_Reward/track_ang_vel_z_exp: 0.2680
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0422
     Episode_Reward/dof_torques_l2: -0.0498
         Episode_Reward/dof_acc_l2: -0.1158
     Episode_Reward/action_rate_l2: -0.0491
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0072
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0444
Metrics/base_velocity/error_vel_xy: 0.3706
Metrics/base_velocity/error_vel_yaw: 0.2780
      Episode_Termination/time_out: 0.5985
  Episode_Termination/base_contact: 0.4025
--------------------------------------------------------------------------------
                   Total timesteps: 27402240
                    Iteration time: 2.49s
                      Time elapsed: 00:47:13
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1115/1400 [0m                     

                       Computation: 9841 steps/s (collection: 2.242s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.6958
                       Mean reward: 9.64
               Mean episode length: 696.19
Episode_Reward/track_lin_vel_xy_exp: 0.5535
Episode_Reward/track_ang_vel_z_exp: 0.3064
       Episode_Reward/lin_vel_z_l2: -0.0317
      Episode_Reward/ang_vel_xy_l2: -0.0449
     Episode_Reward/dof_torques_l2: -0.0519
         Episode_Reward/dof_acc_l2: -0.1146
     Episode_Reward/action_rate_l2: -0.0540
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0487
Metrics/base_velocity/error_vel_xy: 0.3936
Metrics/base_velocity/error_vel_yaw: 0.2929
      Episode_Termination/time_out: 0.5957
  Episode_Termination/base_contact: 0.4053
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.50s
                      Time elapsed: 00:47:16
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1116/1400 [0m                     

                       Computation: 9878 steps/s (collection: 2.231s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.7047
                       Mean reward: 9.52
               Mean episode length: 708.71
Episode_Reward/track_lin_vel_xy_exp: 0.5480
Episode_Reward/track_ang_vel_z_exp: 0.3011
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0546
         Episode_Reward/dof_acc_l2: -0.1335
     Episode_Reward/action_rate_l2: -0.0547
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0078
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0488
Metrics/base_velocity/error_vel_xy: 0.4132
Metrics/base_velocity/error_vel_yaw: 0.3175
      Episode_Termination/time_out: 0.5938
  Episode_Termination/base_contact: 0.4072
--------------------------------------------------------------------------------
                   Total timesteps: 27451392
                    Iteration time: 2.49s
                      Time elapsed: 00:47:18
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1117/1400 [0m                     

                       Computation: 9802 steps/s (collection: 2.250s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.7148
                       Mean reward: 9.87
               Mean episode length: 713.71
Episode_Reward/track_lin_vel_xy_exp: 0.5892
Episode_Reward/track_ang_vel_z_exp: 0.3165
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.0475
     Episode_Reward/dof_torques_l2: -0.0563
         Episode_Reward/dof_acc_l2: -0.1337
     Episode_Reward/action_rate_l2: -0.0576
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0527
Metrics/base_velocity/error_vel_xy: 0.3841
Metrics/base_velocity/error_vel_yaw: 0.2948
      Episode_Termination/time_out: 0.5883
  Episode_Termination/base_contact: 0.4127
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 2.51s
                      Time elapsed: 00:47:21
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1118/1400 [0m                     

                       Computation: 9798 steps/s (collection: 2.253s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7196
                       Mean reward: 9.87
               Mean episode length: 727.83
Episode_Reward/track_lin_vel_xy_exp: 0.5364
Episode_Reward/track_ang_vel_z_exp: 0.2942
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0545
         Episode_Reward/dof_acc_l2: -0.1333
     Episode_Reward/action_rate_l2: -0.0546
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0067
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0522
Metrics/base_velocity/error_vel_xy: 0.4262
Metrics/base_velocity/error_vel_yaw: 0.3341
      Episode_Termination/time_out: 0.5854
  Episode_Termination/base_contact: 0.4156
--------------------------------------------------------------------------------
                   Total timesteps: 27500544
                    Iteration time: 2.51s
                      Time elapsed: 00:47:23
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1119/1400 [0m                     

                       Computation: 9697 steps/s (collection: 2.278s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.7066
                       Mean reward: 10.65
               Mean episode length: 770.88
Episode_Reward/track_lin_vel_xy_exp: 0.5789
Episode_Reward/track_ang_vel_z_exp: 0.3195
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0497
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.1415
     Episode_Reward/action_rate_l2: -0.0587
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0591
Metrics/base_velocity/error_vel_xy: 0.4358
Metrics/base_velocity/error_vel_yaw: 0.3213
      Episode_Termination/time_out: 0.5884
  Episode_Termination/base_contact: 0.4126
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.53s
                      Time elapsed: 00:47:26
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1120/1400 [0m                     

                       Computation: 9814 steps/s (collection: 2.248s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7087
                       Mean reward: 10.92
               Mean episode length: 807.09
Episode_Reward/track_lin_vel_xy_exp: 0.5902
Episode_Reward/track_ang_vel_z_exp: 0.3268
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0499
     Episode_Reward/dof_torques_l2: -0.0616
         Episode_Reward/dof_acc_l2: -0.1438
     Episode_Reward/action_rate_l2: -0.0602
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0670
Metrics/base_velocity/error_vel_xy: 0.4531
Metrics/base_velocity/error_vel_yaw: 0.3402
      Episode_Termination/time_out: 0.5898
  Episode_Termination/base_contact: 0.4112
--------------------------------------------------------------------------------
                   Total timesteps: 27549696
                    Iteration time: 2.50s
                      Time elapsed: 00:47:28
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1121/1400 [0m                     

                       Computation: 9724 steps/s (collection: 2.263s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7166
                       Mean reward: 10.93
               Mean episode length: 791.63
Episode_Reward/track_lin_vel_xy_exp: 0.4608
Episode_Reward/track_ang_vel_z_exp: 0.2414
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.0421
     Episode_Reward/dof_torques_l2: -0.0450
         Episode_Reward/dof_acc_l2: -0.1093
     Episode_Reward/action_rate_l2: -0.0453
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0642
Metrics/base_velocity/error_vel_xy: 0.3285
Metrics/base_velocity/error_vel_yaw: 0.2815
      Episode_Termination/time_out: 0.5929
  Episode_Termination/base_contact: 0.4081
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 2.53s
                      Time elapsed: 00:47:31
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1122/1400 [0m                     

                       Computation: 9635 steps/s (collection: 2.296s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.7165
                       Mean reward: 10.21
               Mean episode length: 756.34
Episode_Reward/track_lin_vel_xy_exp: 0.5856
Episode_Reward/track_ang_vel_z_exp: 0.3180
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.0465
     Episode_Reward/dof_torques_l2: -0.0598
         Episode_Reward/dof_acc_l2: -0.1323
     Episode_Reward/action_rate_l2: -0.0577
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0599
Metrics/base_velocity/error_vel_xy: 0.4084
Metrics/base_velocity/error_vel_yaw: 0.3039
      Episode_Termination/time_out: 0.5917
  Episode_Termination/base_contact: 0.4093
--------------------------------------------------------------------------------
                   Total timesteps: 27598848
                    Iteration time: 2.55s
                      Time elapsed: 00:47:33
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1123/1400 [0m                     

                       Computation: 9605 steps/s (collection: 2.298s, learning 0.261s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.6998
                       Mean reward: 10.33
               Mean episode length: 752.07
Episode_Reward/track_lin_vel_xy_exp: 0.6644
Episode_Reward/track_ang_vel_z_exp: 0.3585
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0499
     Episode_Reward/dof_torques_l2: -0.0598
         Episode_Reward/dof_acc_l2: -0.1428
     Episode_Reward/action_rate_l2: -0.0626
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0570
Metrics/base_velocity/error_vel_xy: 0.4192
Metrics/base_velocity/error_vel_yaw: 0.3205
      Episode_Termination/time_out: 0.5912
  Episode_Termination/base_contact: 0.4098
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.56s
                      Time elapsed: 00:47:36
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1124/1400 [0m                     

                       Computation: 9735 steps/s (collection: 2.262s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.6956
                       Mean reward: 10.15
               Mean episode length: 746.49
Episode_Reward/track_lin_vel_xy_exp: 0.5743
Episode_Reward/track_ang_vel_z_exp: 0.3184
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0569
         Episode_Reward/dof_acc_l2: -0.1245
     Episode_Reward/action_rate_l2: -0.0568
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0086
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0568
Metrics/base_velocity/error_vel_xy: 0.4232
Metrics/base_velocity/error_vel_yaw: 0.3137
      Episode_Termination/time_out: 0.5918
  Episode_Termination/base_contact: 0.4092
--------------------------------------------------------------------------------
                   Total timesteps: 27648000
                    Iteration time: 2.52s
                      Time elapsed: 00:47:39
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1125/1400 [0m                     

                       Computation: 9663 steps/s (collection: 2.287s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.6990
                       Mean reward: 10.18
               Mean episode length: 764.51
Episode_Reward/track_lin_vel_xy_exp: 0.5269
Episode_Reward/track_ang_vel_z_exp: 0.3000
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0580
         Episode_Reward/dof_acc_l2: -0.1346
     Episode_Reward/action_rate_l2: -0.0564
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0097
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0573
Metrics/base_velocity/error_vel_xy: 0.4729
Metrics/base_velocity/error_vel_yaw: 0.3422
      Episode_Termination/time_out: 0.5917
  Episode_Termination/base_contact: 0.4093
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 2.54s
                      Time elapsed: 00:47:41
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1126/1400 [0m                     

                       Computation: 9741 steps/s (collection: 2.268s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7181
                       Mean reward: 10.75
               Mean episode length: 791.50
Episode_Reward/track_lin_vel_xy_exp: 0.4951
Episode_Reward/track_ang_vel_z_exp: 0.3049
       Episode_Reward/lin_vel_z_l2: -0.0378
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0581
         Episode_Reward/dof_acc_l2: -0.1266
     Episode_Reward/action_rate_l2: -0.0548
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0528
Metrics/base_velocity/error_vel_xy: 0.5211
Metrics/base_velocity/error_vel_yaw: 0.3211
      Episode_Termination/time_out: 0.5981
  Episode_Termination/base_contact: 0.4028
--------------------------------------------------------------------------------
                   Total timesteps: 27697152
                    Iteration time: 2.52s
                      Time elapsed: 00:47:44
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1127/1400 [0m                     

                       Computation: 9683 steps/s (collection: 2.285s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0127
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7239
                       Mean reward: 10.19
               Mean episode length: 794.41
Episode_Reward/track_lin_vel_xy_exp: 0.5404
Episode_Reward/track_ang_vel_z_exp: 0.3088
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0503
     Episode_Reward/dof_torques_l2: -0.0625
         Episode_Reward/dof_acc_l2: -0.1466
     Episode_Reward/action_rate_l2: -0.0591
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0086
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0656
Metrics/base_velocity/error_vel_xy: 0.4932
Metrics/base_velocity/error_vel_yaw: 0.3539
      Episode_Termination/time_out: 0.5983
  Episode_Termination/base_contact: 0.4027
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.54s
                      Time elapsed: 00:47:46
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1128/1400 [0m                     

                       Computation: 9861 steps/s (collection: 2.237s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.7230
                       Mean reward: 10.54
               Mean episode length: 807.99
Episode_Reward/track_lin_vel_xy_exp: 0.5470
Episode_Reward/track_ang_vel_z_exp: 0.3081
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0498
     Episode_Reward/dof_torques_l2: -0.0565
         Episode_Reward/dof_acc_l2: -0.1342
     Episode_Reward/action_rate_l2: -0.0564
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0702
Metrics/base_velocity/error_vel_xy: 0.4822
Metrics/base_velocity/error_vel_yaw: 0.3400
      Episode_Termination/time_out: 0.5984
  Episode_Termination/base_contact: 0.4025
--------------------------------------------------------------------------------
                   Total timesteps: 27746304
                    Iteration time: 2.49s
                      Time elapsed: 00:47:49
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1129/1400 [0m                     

                       Computation: 9805 steps/s (collection: 2.250s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.7193
                       Mean reward: 10.45
               Mean episode length: 796.59
Episode_Reward/track_lin_vel_xy_exp: 0.5105
Episode_Reward/track_ang_vel_z_exp: 0.2846
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1210
     Episode_Reward/action_rate_l2: -0.0531
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0192
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0769
Metrics/base_velocity/error_vel_xy: 0.4127
Metrics/base_velocity/error_vel_yaw: 0.3157
      Episode_Termination/time_out: 0.5927
  Episode_Termination/base_contact: 0.4082
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 2.51s
                      Time elapsed: 00:47:51
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1130/1400 [0m                     

                       Computation: 9891 steps/s (collection: 2.232s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 5.7175
                       Mean reward: 10.40
               Mean episode length: 782.57
Episode_Reward/track_lin_vel_xy_exp: 0.5730
Episode_Reward/track_ang_vel_z_exp: 0.3231
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0584
         Episode_Reward/dof_acc_l2: -0.1350
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0878
Metrics/base_velocity/error_vel_xy: 0.4445
Metrics/base_velocity/error_vel_yaw: 0.3164
      Episode_Termination/time_out: 0.5897
  Episode_Termination/base_contact: 0.4113
--------------------------------------------------------------------------------
                   Total timesteps: 27795456
                    Iteration time: 2.48s
                      Time elapsed: 00:47:54
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1131/1400 [0m                     

                       Computation: 9991 steps/s (collection: 2.203s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 5.7258
                       Mean reward: 10.12
               Mean episode length: 751.67
Episode_Reward/track_lin_vel_xy_exp: 0.4704
Episode_Reward/track_ang_vel_z_exp: 0.2610
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0416
     Episode_Reward/dof_torques_l2: -0.0472
         Episode_Reward/dof_acc_l2: -0.1117
     Episode_Reward/action_rate_l2: -0.0481
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0883
Metrics/base_velocity/error_vel_xy: 0.3648
Metrics/base_velocity/error_vel_yaw: 0.2751
      Episode_Termination/time_out: 0.5909
  Episode_Termination/base_contact: 0.4100
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.46s
                      Time elapsed: 00:47:56
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 1132/1400 [0m                     

                       Computation: 9825 steps/s (collection: 2.245s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7211
                       Mean reward: 9.11
               Mean episode length: 706.60
Episode_Reward/track_lin_vel_xy_exp: 0.4665
Episode_Reward/track_ang_vel_z_exp: 0.2626
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0437
     Episode_Reward/dof_torques_l2: -0.0528
         Episode_Reward/dof_acc_l2: -0.1202
     Episode_Reward/action_rate_l2: -0.0499
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0103
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0747
Metrics/base_velocity/error_vel_xy: 0.4121
Metrics/base_velocity/error_vel_yaw: 0.3144
      Episode_Termination/time_out: 0.5890
  Episode_Termination/base_contact: 0.4120
--------------------------------------------------------------------------------
                   Total timesteps: 27844608
                    Iteration time: 2.50s
                      Time elapsed: 00:47:59
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1133/1400 [0m                     

                       Computation: 9899 steps/s (collection: 2.220s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.7169
                       Mean reward: 9.05
               Mean episode length: 691.41
Episode_Reward/track_lin_vel_xy_exp: 0.5362
Episode_Reward/track_ang_vel_z_exp: 0.2878
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0533
         Episode_Reward/dof_acc_l2: -0.1297
     Episode_Reward/action_rate_l2: -0.0540
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0766
Metrics/base_velocity/error_vel_xy: 0.4032
Metrics/base_velocity/error_vel_yaw: 0.3214
      Episode_Termination/time_out: 0.5872
  Episode_Termination/base_contact: 0.4138
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 2.48s
                      Time elapsed: 00:48:01
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1134/1400 [0m                     

                       Computation: 9770 steps/s (collection: 2.261s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 5.7196
                       Mean reward: 9.31
               Mean episode length: 698.92
Episode_Reward/track_lin_vel_xy_exp: 0.5426
Episode_Reward/track_ang_vel_z_exp: 0.2956
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1274
     Episode_Reward/action_rate_l2: -0.0541
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0815
Metrics/base_velocity/error_vel_xy: 0.4100
Metrics/base_velocity/error_vel_yaw: 0.3152
      Episode_Termination/time_out: 0.5851
  Episode_Termination/base_contact: 0.4159
--------------------------------------------------------------------------------
                   Total timesteps: 27893760
                    Iteration time: 2.52s
                      Time elapsed: 00:48:04
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1135/1400 [0m                     

                       Computation: 9795 steps/s (collection: 2.252s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7237
                       Mean reward: 10.24
               Mean episode length: 740.69
Episode_Reward/track_lin_vel_xy_exp: 0.5775
Episode_Reward/track_ang_vel_z_exp: 0.3175
       Episode_Reward/lin_vel_z_l2: -0.0335
      Episode_Reward/ang_vel_xy_l2: -0.0463
     Episode_Reward/dof_torques_l2: -0.0567
         Episode_Reward/dof_acc_l2: -0.1290
     Episode_Reward/action_rate_l2: -0.0567
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0848
Metrics/base_velocity/error_vel_xy: 0.4055
Metrics/base_velocity/error_vel_yaw: 0.3033
      Episode_Termination/time_out: 0.5862
  Episode_Termination/base_contact: 0.4148
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.51s
                      Time elapsed: 00:48:06
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1136/1400 [0m                     

                       Computation: 9951 steps/s (collection: 2.205s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 5.7381
                       Mean reward: 11.03
               Mean episode length: 783.12
Episode_Reward/track_lin_vel_xy_exp: 0.6155
Episode_Reward/track_ang_vel_z_exp: 0.3372
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0536
     Episode_Reward/dof_torques_l2: -0.0650
         Episode_Reward/dof_acc_l2: -0.1449
     Episode_Reward/action_rate_l2: -0.0621
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0112
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0918
Metrics/base_velocity/error_vel_xy: 0.4542
Metrics/base_velocity/error_vel_yaw: 0.3491
      Episode_Termination/time_out: 0.5902
  Episode_Termination/base_contact: 0.4108
--------------------------------------------------------------------------------
                   Total timesteps: 27942912
                    Iteration time: 2.47s
                      Time elapsed: 00:48:09
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1137/1400 [0m                     

                       Computation: 9886 steps/s (collection: 2.220s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.7611
                       Mean reward: 11.73
               Mean episode length: 818.81
Episode_Reward/track_lin_vel_xy_exp: 0.5974
Episode_Reward/track_ang_vel_z_exp: 0.3323
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.0516
     Episode_Reward/dof_torques_l2: -0.0598
         Episode_Reward/dof_acc_l2: -0.1474
     Episode_Reward/action_rate_l2: -0.0610
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0990
Metrics/base_velocity/error_vel_xy: 0.4413
Metrics/base_velocity/error_vel_yaw: 0.3204
      Episode_Termination/time_out: 0.5889
  Episode_Termination/base_contact: 0.4121
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 2.49s
                      Time elapsed: 00:48:11
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1138/1400 [0m                     

                       Computation: 10008 steps/s (collection: 2.193s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.7615
                       Mean reward: 12.00
               Mean episode length: 832.40
Episode_Reward/track_lin_vel_xy_exp: 0.6651
Episode_Reward/track_ang_vel_z_exp: 0.3669
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0514
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1362
     Episode_Reward/action_rate_l2: -0.0641
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1013
Metrics/base_velocity/error_vel_xy: 0.4527
Metrics/base_velocity/error_vel_yaw: 0.3284
      Episode_Termination/time_out: 0.5886
  Episode_Termination/base_contact: 0.4124
--------------------------------------------------------------------------------
                   Total timesteps: 27992064
                    Iteration time: 2.46s
                      Time elapsed: 00:48:14
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1139/1400 [0m                     

                       Computation: 9733 steps/s (collection: 2.272s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.7579
                       Mean reward: 11.80
               Mean episode length: 842.33
Episode_Reward/track_lin_vel_xy_exp: 0.6844
Episode_Reward/track_ang_vel_z_exp: 0.3704
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.0538
     Episode_Reward/dof_torques_l2: -0.0653
         Episode_Reward/dof_acc_l2: -0.1565
     Episode_Reward/action_rate_l2: -0.0662
      Episode_Reward/feet_air_time: -0.0103
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1042
Metrics/base_velocity/error_vel_xy: 0.4671
Metrics/base_velocity/error_vel_yaw: 0.3650
      Episode_Termination/time_out: 0.5924
  Episode_Termination/base_contact: 0.4086
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.52s
                      Time elapsed: 00:48:16
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1140/1400 [0m                     

                       Computation: 9626 steps/s (collection: 2.296s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.7597
                       Mean reward: 11.62
               Mean episode length: 854.00
Episode_Reward/track_lin_vel_xy_exp: 0.6265
Episode_Reward/track_ang_vel_z_exp: 0.3468
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.0536
     Episode_Reward/dof_torques_l2: -0.0646
         Episode_Reward/dof_acc_l2: -0.1522
     Episode_Reward/action_rate_l2: -0.0648
      Episode_Reward/feet_air_time: -0.0099
 Episode_Reward/undesired_contacts: -0.0066
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1117
Metrics/base_velocity/error_vel_xy: 0.4845
Metrics/base_velocity/error_vel_yaw: 0.3641
      Episode_Termination/time_out: 0.5941
  Episode_Termination/base_contact: 0.4066
--------------------------------------------------------------------------------
                   Total timesteps: 28041216
                    Iteration time: 2.55s
                      Time elapsed: 00:48:19
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1141/1400 [0m                     

                       Computation: 9533 steps/s (collection: 2.322s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 5.7596
                       Mean reward: 10.36
               Mean episode length: 828.69
Episode_Reward/track_lin_vel_xy_exp: 0.5521
Episode_Reward/track_ang_vel_z_exp: 0.3111
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.0561
     Episode_Reward/dof_torques_l2: -0.0617
         Episode_Reward/dof_acc_l2: -0.1573
     Episode_Reward/action_rate_l2: -0.0599
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0093
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1236
Metrics/base_velocity/error_vel_xy: 0.5133
Metrics/base_velocity/error_vel_yaw: 0.3730
      Episode_Termination/time_out: 0.5990
  Episode_Termination/base_contact: 0.4010
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 2.58s
                      Time elapsed: 00:48:21
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1142/1400 [0m                     

                       Computation: 9523 steps/s (collection: 2.317s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0176
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 5.7613
                       Mean reward: 10.87
               Mean episode length: 818.14
Episode_Reward/track_lin_vel_xy_exp: 0.6315
Episode_Reward/track_ang_vel_z_exp: 0.3375
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0484
     Episode_Reward/dof_torques_l2: -0.0604
         Episode_Reward/dof_acc_l2: -0.1370
     Episode_Reward/action_rate_l2: -0.0615
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0261
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1371
Metrics/base_velocity/error_vel_xy: 0.4461
Metrics/base_velocity/error_vel_yaw: 0.3578
      Episode_Termination/time_out: 0.6037
  Episode_Termination/base_contact: 0.3963
--------------------------------------------------------------------------------
                   Total timesteps: 28090368
                    Iteration time: 2.58s
                      Time elapsed: 00:48:24
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1143/1400 [0m                     

                       Computation: 9616 steps/s (collection: 2.297s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.7671
                       Mean reward: 10.92
               Mean episode length: 792.29
Episode_Reward/track_lin_vel_xy_exp: 0.5867
Episode_Reward/track_ang_vel_z_exp: 0.3252
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0476
     Episode_Reward/dof_torques_l2: -0.0541
         Episode_Reward/dof_acc_l2: -0.1259
     Episode_Reward/action_rate_l2: -0.0583
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1418
Metrics/base_velocity/error_vel_xy: 0.4253
Metrics/base_velocity/error_vel_yaw: 0.3105
      Episode_Termination/time_out: 0.6096
  Episode_Termination/base_contact: 0.3904
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.56s
                      Time elapsed: 00:48:26
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1144/1400 [0m                     

                       Computation: 9619 steps/s (collection: 2.299s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.7717
                       Mean reward: 10.04
               Mean episode length: 732.86
Episode_Reward/track_lin_vel_xy_exp: 0.3961
Episode_Reward/track_ang_vel_z_exp: 0.2393
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.0411
     Episode_Reward/dof_torques_l2: -0.0542
         Episode_Reward/dof_acc_l2: -0.1240
     Episode_Reward/action_rate_l2: -0.0485
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1390
Metrics/base_velocity/error_vel_xy: 0.4763
Metrics/base_velocity/error_vel_yaw: 0.3237
      Episode_Termination/time_out: 0.6052
  Episode_Termination/base_contact: 0.3948
--------------------------------------------------------------------------------
                   Total timesteps: 28139520
                    Iteration time: 2.55s
                      Time elapsed: 00:48:29
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1145/1400 [0m                     

                       Computation: 9514 steps/s (collection: 2.323s, learning 0.260s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.7666
                       Mean reward: 9.46
               Mean episode length: 703.87
Episode_Reward/track_lin_vel_xy_exp: 0.6025
Episode_Reward/track_ang_vel_z_exp: 0.3255
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0506
     Episode_Reward/dof_torques_l2: -0.0572
         Episode_Reward/dof_acc_l2: -0.1329
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1373
Metrics/base_velocity/error_vel_xy: 0.3979
Metrics/base_velocity/error_vel_yaw: 0.3110
      Episode_Termination/time_out: 0.6025
  Episode_Termination/base_contact: 0.3975
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 2.58s
                      Time elapsed: 00:48:31
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1146/1400 [0m                     

                       Computation: 9673 steps/s (collection: 2.284s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.7615
                       Mean reward: 9.59
               Mean episode length: 714.69
Episode_Reward/track_lin_vel_xy_exp: 0.6606
Episode_Reward/track_ang_vel_z_exp: 0.3561
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0539
     Episode_Reward/dof_torques_l2: -0.0610
         Episode_Reward/dof_acc_l2: -0.1427
     Episode_Reward/action_rate_l2: -0.0635
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1502
Metrics/base_velocity/error_vel_xy: 0.4533
Metrics/base_velocity/error_vel_yaw: 0.3474
      Episode_Termination/time_out: 0.6017
  Episode_Termination/base_contact: 0.3983
--------------------------------------------------------------------------------
                   Total timesteps: 28188672
                    Iteration time: 2.54s
                      Time elapsed: 00:48:34
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1147/1400 [0m                     

                       Computation: 9649 steps/s (collection: 2.289s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 5.7718
                       Mean reward: 11.22
               Mean episode length: 779.96
Episode_Reward/track_lin_vel_xy_exp: 0.5240
Episode_Reward/track_ang_vel_z_exp: 0.2812
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0479
         Episode_Reward/dof_acc_l2: -0.1148
     Episode_Reward/action_rate_l2: -0.0502
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1533
Metrics/base_velocity/error_vel_xy: 0.3455
Metrics/base_velocity/error_vel_yaw: 0.2635
      Episode_Termination/time_out: 0.6007
  Episode_Termination/base_contact: 0.3993
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.55s
                      Time elapsed: 00:48:37
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1148/1400 [0m                     

                       Computation: 9396 steps/s (collection: 2.360s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.7747
                       Mean reward: 10.94
               Mean episode length: 800.19
Episode_Reward/track_lin_vel_xy_exp: 0.5034
Episode_Reward/track_ang_vel_z_exp: 0.2873
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0463
     Episode_Reward/dof_torques_l2: -0.0555
         Episode_Reward/dof_acc_l2: -0.1354
     Episode_Reward/action_rate_l2: -0.0554
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0150
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1524
Metrics/base_velocity/error_vel_xy: 0.4602
Metrics/base_velocity/error_vel_yaw: 0.3337
      Episode_Termination/time_out: 0.6032
  Episode_Termination/base_contact: 0.3968
--------------------------------------------------------------------------------
                   Total timesteps: 28237824
                    Iteration time: 2.62s
                      Time elapsed: 00:48:39
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1149/1400 [0m                     

                       Computation: 9618 steps/s (collection: 2.298s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.7670
                       Mean reward: 10.31
               Mean episode length: 769.60
Episode_Reward/track_lin_vel_xy_exp: 0.5250
Episode_Reward/track_ang_vel_z_exp: 0.2856
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0431
     Episode_Reward/dof_torques_l2: -0.0499
         Episode_Reward/dof_acc_l2: -0.1138
     Episode_Reward/action_rate_l2: -0.0518
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1505
Metrics/base_velocity/error_vel_xy: 0.3755
Metrics/base_velocity/error_vel_yaw: 0.2902
      Episode_Termination/time_out: 0.6019
  Episode_Termination/base_contact: 0.3981
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 2.56s
                      Time elapsed: 00:48:42
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1150/1400 [0m                     

                       Computation: 9547 steps/s (collection: 2.321s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.7734
                       Mean reward: 10.06
               Mean episode length: 769.68
Episode_Reward/track_lin_vel_xy_exp: 0.5708
Episode_Reward/track_ang_vel_z_exp: 0.3159
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0504
     Episode_Reward/dof_torques_l2: -0.0562
         Episode_Reward/dof_acc_l2: -0.1377
     Episode_Reward/action_rate_l2: -0.0578
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1510
Metrics/base_velocity/error_vel_xy: 0.4326
Metrics/base_velocity/error_vel_yaw: 0.3221
      Episode_Termination/time_out: 0.6066
  Episode_Termination/base_contact: 0.3934
--------------------------------------------------------------------------------
                   Total timesteps: 28286976
                    Iteration time: 2.57s
                      Time elapsed: 00:48:44
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1151/1400 [0m                     

                       Computation: 9593 steps/s (collection: 2.305s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7721
                       Mean reward: 11.31
               Mean episode length: 811.85
Episode_Reward/track_lin_vel_xy_exp: 0.6385
Episode_Reward/track_ang_vel_z_exp: 0.3517
       Episode_Reward/lin_vel_z_l2: -0.0389
      Episode_Reward/ang_vel_xy_l2: -0.0527
     Episode_Reward/dof_torques_l2: -0.0628
         Episode_Reward/dof_acc_l2: -0.1432
     Episode_Reward/action_rate_l2: -0.0632
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1653
Metrics/base_velocity/error_vel_xy: 0.4475
Metrics/base_velocity/error_vel_yaw: 0.3303
      Episode_Termination/time_out: 0.6125
  Episode_Termination/base_contact: 0.3875
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.56s
                      Time elapsed: 00:48:47
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1152/1400 [0m                     

                       Computation: 9626 steps/s (collection: 2.298s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.7617
                       Mean reward: 11.56
               Mean episode length: 829.50
Episode_Reward/track_lin_vel_xy_exp: 0.6533
Episode_Reward/track_ang_vel_z_exp: 0.3561
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1387
     Episode_Reward/action_rate_l2: -0.0629
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1746
Metrics/base_velocity/error_vel_xy: 0.4408
Metrics/base_velocity/error_vel_yaw: 0.3263
      Episode_Termination/time_out: 0.6178
  Episode_Termination/base_contact: 0.3822
--------------------------------------------------------------------------------
                   Total timesteps: 28336128
                    Iteration time: 2.55s
                      Time elapsed: 00:48:49
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1153/1400 [0m                     

                       Computation: 9650 steps/s (collection: 2.284s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.7689
                       Mean reward: 11.12
               Mean episode length: 799.49
Episode_Reward/track_lin_vel_xy_exp: 0.4514
Episode_Reward/track_ang_vel_z_exp: 0.2526
       Episode_Reward/lin_vel_z_l2: -0.0335
      Episode_Reward/ang_vel_xy_l2: -0.0393
     Episode_Reward/dof_torques_l2: -0.0457
         Episode_Reward/dof_acc_l2: -0.1167
     Episode_Reward/action_rate_l2: -0.0477
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1802
Metrics/base_velocity/error_vel_xy: 0.3805
Metrics/base_velocity/error_vel_yaw: 0.2712
      Episode_Termination/time_out: 0.6215
  Episode_Termination/base_contact: 0.3785
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 2.55s
                      Time elapsed: 00:48:52
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1154/1400 [0m                     

                       Computation: 9769 steps/s (collection: 2.252s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.7577
                       Mean reward: 11.05
               Mean episode length: 807.31
Episode_Reward/track_lin_vel_xy_exp: 0.5846
Episode_Reward/track_ang_vel_z_exp: 0.3329
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0627
         Episode_Reward/dof_acc_l2: -0.1701
     Episode_Reward/action_rate_l2: -0.0627
      Episode_Reward/feet_air_time: -0.0099
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1848
Metrics/base_velocity/error_vel_xy: 0.4829
Metrics/base_velocity/error_vel_yaw: 0.3345
      Episode_Termination/time_out: 0.6226
  Episode_Termination/base_contact: 0.3774
--------------------------------------------------------------------------------
                   Total timesteps: 28385280
                    Iteration time: 2.52s
                      Time elapsed: 00:48:54
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1155/1400 [0m                     

                       Computation: 9778 steps/s (collection: 2.253s, learning 0.260s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 5.7518
                       Mean reward: 10.39
               Mean episode length: 767.71
Episode_Reward/track_lin_vel_xy_exp: 0.5123
Episode_Reward/track_ang_vel_z_exp: 0.2792
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0451
     Episode_Reward/dof_torques_l2: -0.0519
         Episode_Reward/dof_acc_l2: -0.1311
     Episode_Reward/action_rate_l2: -0.0514
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1925
Metrics/base_velocity/error_vel_xy: 0.3687
Metrics/base_velocity/error_vel_yaw: 0.2809
      Episode_Termination/time_out: 0.6202
  Episode_Termination/base_contact: 0.3798
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.51s
                      Time elapsed: 00:48:57
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1156/1400 [0m                     

                       Computation: 9512 steps/s (collection: 2.329s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 5.7500
                       Mean reward: 9.72
               Mean episode length: 758.76
Episode_Reward/track_lin_vel_xy_exp: 0.5793
Episode_Reward/track_ang_vel_z_exp: 0.3294
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.0511
     Episode_Reward/dof_torques_l2: -0.0619
         Episode_Reward/dof_acc_l2: -0.1452
     Episode_Reward/action_rate_l2: -0.0618
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0087
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1986
Metrics/base_velocity/error_vel_xy: 0.4831
Metrics/base_velocity/error_vel_yaw: 0.3503
      Episode_Termination/time_out: 0.6225
  Episode_Termination/base_contact: 0.3775
--------------------------------------------------------------------------------
                   Total timesteps: 28434432
                    Iteration time: 2.58s
                      Time elapsed: 00:49:00
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1157/1400 [0m                     

                       Computation: 9596 steps/s (collection: 2.304s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.7450
                       Mean reward: 10.54
               Mean episode length: 801.07
Episode_Reward/track_lin_vel_xy_exp: 0.6424
Episode_Reward/track_ang_vel_z_exp: 0.3510
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.0563
     Episode_Reward/dof_torques_l2: -0.0649
         Episode_Reward/dof_acc_l2: -0.1593
     Episode_Reward/action_rate_l2: -0.0649
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2105
Metrics/base_velocity/error_vel_xy: 0.4626
Metrics/base_velocity/error_vel_yaw: 0.3504
      Episode_Termination/time_out: 0.6227
  Episode_Termination/base_contact: 0.3773
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 2.56s
                      Time elapsed: 00:49:02
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1158/1400 [0m                     

                       Computation: 9584 steps/s (collection: 2.307s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.7456
                       Mean reward: 11.32
               Mean episode length: 832.63
Episode_Reward/track_lin_vel_xy_exp: 0.5733
Episode_Reward/track_ang_vel_z_exp: 0.3154
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0489
     Episode_Reward/dof_torques_l2: -0.0546
         Episode_Reward/dof_acc_l2: -0.1260
     Episode_Reward/action_rate_l2: -0.0564
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2166
Metrics/base_velocity/error_vel_xy: 0.4159
Metrics/base_velocity/error_vel_yaw: 0.3114
      Episode_Termination/time_out: 0.6293
  Episode_Termination/base_contact: 0.3707
--------------------------------------------------------------------------------
                   Total timesteps: 28483584
                    Iteration time: 2.56s
                      Time elapsed: 00:49:05
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1159/1400 [0m                     

                       Computation: 9529 steps/s (collection: 2.316s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.7441
                       Mean reward: 11.34
               Mean episode length: 821.87
Episode_Reward/track_lin_vel_xy_exp: 0.5945
Episode_Reward/track_ang_vel_z_exp: 0.3348
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.0562
     Episode_Reward/dof_torques_l2: -0.0638
         Episode_Reward/dof_acc_l2: -0.1550
     Episode_Reward/action_rate_l2: -0.0620
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2159
Metrics/base_velocity/error_vel_xy: 0.4976
Metrics/base_velocity/error_vel_yaw: 0.3592
      Episode_Termination/time_out: 0.6385
  Episode_Termination/base_contact: 0.3615
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.58s
                      Time elapsed: 00:49:07
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1160/1400 [0m                     

                       Computation: 9654 steps/s (collection: 2.289s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 5.7539
                       Mean reward: 11.12
               Mean episode length: 806.83
Episode_Reward/track_lin_vel_xy_exp: 0.6079
Episode_Reward/track_ang_vel_z_exp: 0.3288
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.0506
     Episode_Reward/dof_torques_l2: -0.0605
         Episode_Reward/dof_acc_l2: -0.1420
     Episode_Reward/action_rate_l2: -0.0602
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2242
Metrics/base_velocity/error_vel_xy: 0.4188
Metrics/base_velocity/error_vel_yaw: 0.3252
      Episode_Termination/time_out: 0.6405
  Episode_Termination/base_contact: 0.3595
--------------------------------------------------------------------------------
                   Total timesteps: 28532736
                    Iteration time: 2.55s
                      Time elapsed: 00:49:10
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1161/1400 [0m                     

                       Computation: 9560 steps/s (collection: 2.314s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.7720
                       Mean reward: 10.72
               Mean episode length: 788.65
Episode_Reward/track_lin_vel_xy_exp: 0.5498
Episode_Reward/track_ang_vel_z_exp: 0.3008
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0551
         Episode_Reward/dof_acc_l2: -0.1298
     Episode_Reward/action_rate_l2: -0.0555
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0108
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2206
Metrics/base_velocity/error_vel_xy: 0.4040
Metrics/base_velocity/error_vel_yaw: 0.3095
      Episode_Termination/time_out: 0.6403
  Episode_Termination/base_contact: 0.3597
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 2.57s
                      Time elapsed: 00:49:12
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1162/1400 [0m                     

                       Computation: 9425 steps/s (collection: 2.351s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.7780
                       Mean reward: 10.25
               Mean episode length: 746.64
Episode_Reward/track_lin_vel_xy_exp: 0.4998
Episode_Reward/track_ang_vel_z_exp: 0.2789
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0539
         Episode_Reward/dof_acc_l2: -0.1312
     Episode_Reward/action_rate_l2: -0.0532
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0089
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2033
Metrics/base_velocity/error_vel_xy: 0.4299
Metrics/base_velocity/error_vel_yaw: 0.3257
      Episode_Termination/time_out: 0.6361
  Episode_Termination/base_contact: 0.3639
--------------------------------------------------------------------------------
                   Total timesteps: 28581888
                    Iteration time: 2.61s
                      Time elapsed: 00:49:15
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1163/1400 [0m                     

                       Computation: 9488 steps/s (collection: 2.326s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7768
                       Mean reward: 10.05
               Mean episode length: 739.43
Episode_Reward/track_lin_vel_xy_exp: 0.5552
Episode_Reward/track_ang_vel_z_exp: 0.3057
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0530
         Episode_Reward/dof_acc_l2: -0.1302
     Episode_Reward/action_rate_l2: -0.0551
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2109
Metrics/base_velocity/error_vel_xy: 0.3903
Metrics/base_velocity/error_vel_yaw: 0.2828
      Episode_Termination/time_out: 0.6344
  Episode_Termination/base_contact: 0.3656
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.59s
                      Time elapsed: 00:49:18
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1164/1400 [0m                     

                       Computation: 9539 steps/s (collection: 2.319s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.7946
                       Mean reward: 10.46
               Mean episode length: 758.99
Episode_Reward/track_lin_vel_xy_exp: 0.6051
Episode_Reward/track_ang_vel_z_exp: 0.3337
       Episode_Reward/lin_vel_z_l2: -0.0412
      Episode_Reward/ang_vel_xy_l2: -0.0495
     Episode_Reward/dof_torques_l2: -0.0581
         Episode_Reward/dof_acc_l2: -0.1346
     Episode_Reward/action_rate_l2: -0.0604
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2075
Metrics/base_velocity/error_vel_xy: 0.4394
Metrics/base_velocity/error_vel_yaw: 0.3307
      Episode_Termination/time_out: 0.6337
  Episode_Termination/base_contact: 0.3663
--------------------------------------------------------------------------------
                   Total timesteps: 28631040
                    Iteration time: 2.58s
                      Time elapsed: 00:49:20
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1165/1400 [0m                     

                       Computation: 9589 steps/s (collection: 2.299s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 5.8032
                       Mean reward: 11.25
               Mean episode length: 784.06
Episode_Reward/track_lin_vel_xy_exp: 0.6407
Episode_Reward/track_ang_vel_z_exp: 0.3353
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0563
         Episode_Reward/dof_acc_l2: -0.1341
     Episode_Reward/action_rate_l2: -0.0594
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0071
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2098
Metrics/base_velocity/error_vel_xy: 0.3751
Metrics/base_velocity/error_vel_yaw: 0.3113
      Episode_Termination/time_out: 0.6351
  Episode_Termination/base_contact: 0.3649
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 2.56s
                      Time elapsed: 00:49:23
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1166/1400 [0m                     

                       Computation: 9558 steps/s (collection: 2.315s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 5.7978
                       Mean reward: 11.30
               Mean episode length: 785.71
Episode_Reward/track_lin_vel_xy_exp: 0.5565
Episode_Reward/track_ang_vel_z_exp: 0.3048
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0539
         Episode_Reward/dof_acc_l2: -0.1307
     Episode_Reward/action_rate_l2: -0.0554
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2124
Metrics/base_velocity/error_vel_xy: 0.3999
Metrics/base_velocity/error_vel_yaw: 0.3041
      Episode_Termination/time_out: 0.6355
  Episode_Termination/base_contact: 0.3645
--------------------------------------------------------------------------------
                   Total timesteps: 28680192
                    Iteration time: 2.57s
                      Time elapsed: 00:49:25
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1167/1400 [0m                     

                       Computation: 9622 steps/s (collection: 2.298s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 5.7919
                       Mean reward: 11.37
               Mean episode length: 788.19
Episode_Reward/track_lin_vel_xy_exp: 0.5372
Episode_Reward/track_ang_vel_z_exp: 0.2933
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0456
     Episode_Reward/dof_torques_l2: -0.0536
         Episode_Reward/dof_acc_l2: -0.1236
     Episode_Reward/action_rate_l2: -0.0537
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0093
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2178
Metrics/base_velocity/error_vel_xy: 0.3927
Metrics/base_velocity/error_vel_yaw: 0.3028
      Episode_Termination/time_out: 0.6330
  Episode_Termination/base_contact: 0.3670
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.55s
                      Time elapsed: 00:49:28
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1168/1400 [0m                     

                       Computation: 9451 steps/s (collection: 2.343s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0184
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.8018
                       Mean reward: 10.18
               Mean episode length: 724.44
Episode_Reward/track_lin_vel_xy_exp: 0.4329
Episode_Reward/track_ang_vel_z_exp: 0.2409
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0419
     Episode_Reward/dof_torques_l2: -0.0459
         Episode_Reward/dof_acc_l2: -0.1143
     Episode_Reward/action_rate_l2: -0.0450
      Episode_Reward/feet_air_time: -0.0068
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2247
Metrics/base_velocity/error_vel_xy: 0.3593
Metrics/base_velocity/error_vel_yaw: 0.2695
      Episode_Termination/time_out: 0.6292
  Episode_Termination/base_contact: 0.3708
--------------------------------------------------------------------------------
                   Total timesteps: 28729344
                    Iteration time: 2.60s
                      Time elapsed: 00:49:30
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1169/1400 [0m                     

                       Computation: 9453 steps/s (collection: 2.343s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.8050
                       Mean reward: 9.99
               Mean episode length: 744.60
Episode_Reward/track_lin_vel_xy_exp: 0.6064
Episode_Reward/track_ang_vel_z_exp: 0.3283
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0596
         Episode_Reward/dof_acc_l2: -0.1453
     Episode_Reward/action_rate_l2: -0.0603
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2339
Metrics/base_velocity/error_vel_xy: 0.4422
Metrics/base_velocity/error_vel_yaw: 0.3407
      Episode_Termination/time_out: 0.6276
  Episode_Termination/base_contact: 0.3724
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 2.60s
                      Time elapsed: 00:49:33
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1170/1400 [0m                     

                       Computation: 9538 steps/s (collection: 2.318s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7938
                       Mean reward: 9.69
               Mean episode length: 759.66
Episode_Reward/track_lin_vel_xy_exp: 0.5743
Episode_Reward/track_ang_vel_z_exp: 0.3310
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.0571
     Episode_Reward/dof_torques_l2: -0.0612
         Episode_Reward/dof_acc_l2: -0.1529
     Episode_Reward/action_rate_l2: -0.0612
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0084
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2450
Metrics/base_velocity/error_vel_xy: 0.5056
Metrics/base_velocity/error_vel_yaw: 0.3499
      Episode_Termination/time_out: 0.6319
  Episode_Termination/base_contact: 0.3681
--------------------------------------------------------------------------------
                   Total timesteps: 28778496
                    Iteration time: 2.58s
                      Time elapsed: 00:49:36
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1171/1400 [0m                     

                       Computation: 9624 steps/s (collection: 2.298s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7700
                       Mean reward: 10.02
               Mean episode length: 781.43
Episode_Reward/track_lin_vel_xy_exp: 0.5168
Episode_Reward/track_ang_vel_z_exp: 0.2915
       Episode_Reward/lin_vel_z_l2: -0.0367
      Episode_Reward/ang_vel_xy_l2: -0.0461
     Episode_Reward/dof_torques_l2: -0.0543
         Episode_Reward/dof_acc_l2: -0.1411
     Episode_Reward/action_rate_l2: -0.0545
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2559
Metrics/base_velocity/error_vel_xy: 0.4305
Metrics/base_velocity/error_vel_yaw: 0.3164
      Episode_Termination/time_out: 0.6376
  Episode_Termination/base_contact: 0.3624
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.55s
                      Time elapsed: 00:49:38
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1172/1400 [0m                     

                       Computation: 9573 steps/s (collection: 2.311s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 5.7527
                       Mean reward: 10.01
               Mean episode length: 776.57
Episode_Reward/track_lin_vel_xy_exp: 0.5408
Episode_Reward/track_ang_vel_z_exp: 0.3012
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0541
         Episode_Reward/dof_acc_l2: -0.1272
     Episode_Reward/action_rate_l2: -0.0548
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2577
Metrics/base_velocity/error_vel_xy: 0.3997
Metrics/base_velocity/error_vel_yaw: 0.2907
      Episode_Termination/time_out: 0.6375
  Episode_Termination/base_contact: 0.3625
--------------------------------------------------------------------------------
                   Total timesteps: 28827648
                    Iteration time: 2.57s
                      Time elapsed: 00:49:41
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1173/1400 [0m                     

                       Computation: 9550 steps/s (collection: 2.308s, learning 0.266s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.7437
                       Mean reward: 10.14
               Mean episode length: 768.43
Episode_Reward/track_lin_vel_xy_exp: 0.5706
Episode_Reward/track_ang_vel_z_exp: 0.3098
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1429
     Episode_Reward/action_rate_l2: -0.0572
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2585
Metrics/base_velocity/error_vel_xy: 0.4087
Metrics/base_velocity/error_vel_yaw: 0.3158
      Episode_Termination/time_out: 0.6357
  Episode_Termination/base_contact: 0.3643
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 2.57s
                      Time elapsed: 00:49:43
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1174/1400 [0m                     

                       Computation: 9708 steps/s (collection: 2.267s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.7286
                       Mean reward: 9.84
               Mean episode length: 730.66
Episode_Reward/track_lin_vel_xy_exp: 0.4746
Episode_Reward/track_ang_vel_z_exp: 0.2623
       Episode_Reward/lin_vel_z_l2: -0.0353
      Episode_Reward/ang_vel_xy_l2: -0.0420
     Episode_Reward/dof_torques_l2: -0.0492
         Episode_Reward/dof_acc_l2: -0.1175
     Episode_Reward/action_rate_l2: -0.0482
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2604
Metrics/base_velocity/error_vel_xy: 0.3686
Metrics/base_velocity/error_vel_yaw: 0.2841
      Episode_Termination/time_out: 0.6375
  Episode_Termination/base_contact: 0.3625
--------------------------------------------------------------------------------
                   Total timesteps: 28876800
                    Iteration time: 2.53s
                      Time elapsed: 00:49:46
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1175/1400 [0m                     

                       Computation: 9759 steps/s (collection: 2.257s, learning 0.261s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 5.7244
                       Mean reward: 10.24
               Mean episode length: 746.29
Episode_Reward/track_lin_vel_xy_exp: 0.6160
Episode_Reward/track_ang_vel_z_exp: 0.3373
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0604
         Episode_Reward/dof_acc_l2: -0.1504
     Episode_Reward/action_rate_l2: -0.0605
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2569
Metrics/base_velocity/error_vel_xy: 0.4198
Metrics/base_velocity/error_vel_yaw: 0.3193
      Episode_Termination/time_out: 0.6373
  Episode_Termination/base_contact: 0.3627
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.52s
                      Time elapsed: 00:49:48
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1176/1400 [0m                     

                       Computation: 9940 steps/s (collection: 2.210s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7365
                       Mean reward: 11.27
               Mean episode length: 778.77
Episode_Reward/track_lin_vel_xy_exp: 0.7137
Episode_Reward/track_ang_vel_z_exp: 0.3720
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0496
     Episode_Reward/dof_torques_l2: -0.0598
         Episode_Reward/dof_acc_l2: -0.1386
     Episode_Reward/action_rate_l2: -0.0645
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2561
Metrics/base_velocity/error_vel_xy: 0.3737
Metrics/base_velocity/error_vel_yaw: 0.3072
      Episode_Termination/time_out: 0.6401
  Episode_Termination/base_contact: 0.3599
--------------------------------------------------------------------------------
                   Total timesteps: 28925952
                    Iteration time: 2.47s
                      Time elapsed: 00:49:51
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1177/1400 [0m                     

                       Computation: 9798 steps/s (collection: 2.252s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7506
                       Mean reward: 10.76
               Mean episode length: 762.43
Episode_Reward/track_lin_vel_xy_exp: 0.6614
Episode_Reward/track_ang_vel_z_exp: 0.3580
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.0550
     Episode_Reward/dof_torques_l2: -0.0635
         Episode_Reward/dof_acc_l2: -0.1556
     Episode_Reward/action_rate_l2: -0.0656
      Episode_Reward/feet_air_time: -0.0099
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2620
Metrics/base_velocity/error_vel_xy: 0.4771
Metrics/base_velocity/error_vel_yaw: 0.3686
      Episode_Termination/time_out: 0.6427
  Episode_Termination/base_contact: 0.3573
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 2.51s
                      Time elapsed: 00:49:53
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1178/1400 [0m                     

                       Computation: 9828 steps/s (collection: 2.244s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7452
                       Mean reward: 11.40
               Mean episode length: 793.18
Episode_Reward/track_lin_vel_xy_exp: 0.5854
Episode_Reward/track_ang_vel_z_exp: 0.3162
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0564
         Episode_Reward/dof_acc_l2: -0.1297
     Episode_Reward/action_rate_l2: -0.0573
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2625
Metrics/base_velocity/error_vel_xy: 0.3967
Metrics/base_velocity/error_vel_yaw: 0.3004
      Episode_Termination/time_out: 0.6393
  Episode_Termination/base_contact: 0.3607
--------------------------------------------------------------------------------
                   Total timesteps: 28975104
                    Iteration time: 2.50s
                      Time elapsed: 00:49:56
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 1179/1400 [0m                     

                       Computation: 9805 steps/s (collection: 2.242s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7310
                       Mean reward: 11.18
               Mean episode length: 801.48
Episode_Reward/track_lin_vel_xy_exp: 0.5181
Episode_Reward/track_ang_vel_z_exp: 0.2925
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0573
         Episode_Reward/dof_acc_l2: -0.1443
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0103
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2669
Metrics/base_velocity/error_vel_xy: 0.4546
Metrics/base_velocity/error_vel_yaw: 0.3369
      Episode_Termination/time_out: 0.6385
  Episode_Termination/base_contact: 0.3615
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.51s
                      Time elapsed: 00:49:58
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1180/1400 [0m                     

                       Computation: 9624 steps/s (collection: 2.297s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7229
                       Mean reward: 10.95
               Mean episode length: 793.15
Episode_Reward/track_lin_vel_xy_exp: 0.6346
Episode_Reward/track_ang_vel_z_exp: 0.3433
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0472
     Episode_Reward/dof_torques_l2: -0.0559
         Episode_Reward/dof_acc_l2: -0.1248
     Episode_Reward/action_rate_l2: -0.0591
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0102
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2738
Metrics/base_velocity/error_vel_xy: 0.3830
Metrics/base_velocity/error_vel_yaw: 0.2854
      Episode_Termination/time_out: 0.6349
  Episode_Termination/base_contact: 0.3651
--------------------------------------------------------------------------------
                   Total timesteps: 29024256
                    Iteration time: 2.55s
                      Time elapsed: 00:50:01
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1181/1400 [0m                     

                       Computation: 9462 steps/s (collection: 2.333s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.7087
                       Mean reward: 10.53
               Mean episode length: 801.75
Episode_Reward/track_lin_vel_xy_exp: 0.5929
Episode_Reward/track_ang_vel_z_exp: 0.3310
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.1468
     Episode_Reward/action_rate_l2: -0.0614
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0139
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2748
Metrics/base_velocity/error_vel_xy: 0.4825
Metrics/base_velocity/error_vel_yaw: 0.3523
      Episode_Termination/time_out: 0.6335
  Episode_Termination/base_contact: 0.3665
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 2.60s
                      Time elapsed: 00:50:03
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1182/1400 [0m                     

                       Computation: 9643 steps/s (collection: 2.293s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.6984
                       Mean reward: 10.80
               Mean episode length: 812.84
Episode_Reward/track_lin_vel_xy_exp: 0.6398
Episode_Reward/track_ang_vel_z_exp: 0.3522
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0544
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1569
     Episode_Reward/action_rate_l2: -0.0638
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2820
Metrics/base_velocity/error_vel_xy: 0.4649
Metrics/base_velocity/error_vel_yaw: 0.3513
      Episode_Termination/time_out: 0.6331
  Episode_Termination/base_contact: 0.3669
--------------------------------------------------------------------------------
                   Total timesteps: 29073408
                    Iteration time: 2.55s
                      Time elapsed: 00:50:06
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1183/1400 [0m                     

                       Computation: 9580 steps/s (collection: 2.306s, learning 0.259s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.6978
                       Mean reward: 10.87
               Mean episode length: 818.43
Episode_Reward/track_lin_vel_xy_exp: 0.6350
Episode_Reward/track_ang_vel_z_exp: 0.3537
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.0541
     Episode_Reward/dof_torques_l2: -0.0611
         Episode_Reward/dof_acc_l2: -0.1535
     Episode_Reward/action_rate_l2: -0.0638
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2888
Metrics/base_velocity/error_vel_xy: 0.4807
Metrics/base_velocity/error_vel_yaw: 0.3552
      Episode_Termination/time_out: 0.6342
  Episode_Termination/base_contact: 0.3658
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.57s
                      Time elapsed: 00:50:09
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1184/1400 [0m                     

                       Computation: 9480 steps/s (collection: 2.335s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 5.7056
                       Mean reward: 10.93
               Mean episode length: 813.11
Episode_Reward/track_lin_vel_xy_exp: 0.6099
Episode_Reward/track_ang_vel_z_exp: 0.3383
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0497
     Episode_Reward/dof_torques_l2: -0.0590
         Episode_Reward/dof_acc_l2: -0.1461
     Episode_Reward/action_rate_l2: -0.0609
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2957
Metrics/base_velocity/error_vel_xy: 0.4696
Metrics/base_velocity/error_vel_yaw: 0.3448
      Episode_Termination/time_out: 0.6338
  Episode_Termination/base_contact: 0.3662
--------------------------------------------------------------------------------
                   Total timesteps: 29122560
                    Iteration time: 2.59s
                      Time elapsed: 00:50:11
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1185/1400 [0m                     

                       Computation: 9475 steps/s (collection: 2.338s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7244
                       Mean reward: 10.73
               Mean episode length: 808.68
Episode_Reward/track_lin_vel_xy_exp: 0.5133
Episode_Reward/track_ang_vel_z_exp: 0.2997
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0532
     Episode_Reward/dof_torques_l2: -0.0590
         Episode_Reward/dof_acc_l2: -0.1441
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0147
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3019
Metrics/base_velocity/error_vel_xy: 0.4893
Metrics/base_velocity/error_vel_yaw: 0.3313
      Episode_Termination/time_out: 0.6327
  Episode_Termination/base_contact: 0.3673
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 2.59s
                      Time elapsed: 00:50:14
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1186/1400 [0m                     

                       Computation: 9665 steps/s (collection: 2.284s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 5.7463
                       Mean reward: 11.16
               Mean episode length: 830.84
Episode_Reward/track_lin_vel_xy_exp: 0.7029
Episode_Reward/track_ang_vel_z_exp: 0.3729
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0519
     Episode_Reward/dof_torques_l2: -0.0584
         Episode_Reward/dof_acc_l2: -0.1389
     Episode_Reward/action_rate_l2: -0.0651
      Episode_Reward/feet_air_time: -0.0099
 Episode_Reward/undesired_contacts: -0.0072
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3081
Metrics/base_velocity/error_vel_xy: 0.4164
Metrics/base_velocity/error_vel_yaw: 0.3323
      Episode_Termination/time_out: 0.6410
  Episode_Termination/base_contact: 0.3590
--------------------------------------------------------------------------------
                   Total timesteps: 29171712
                    Iteration time: 2.54s
                      Time elapsed: 00:50:16
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1187/1400 [0m                     

                       Computation: 9495 steps/s (collection: 2.324s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 5.7554
                       Mean reward: 11.29
               Mean episode length: 819.22
Episode_Reward/track_lin_vel_xy_exp: 0.6093
Episode_Reward/track_ang_vel_z_exp: 0.3272
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0502
     Episode_Reward/dof_torques_l2: -0.0605
         Episode_Reward/dof_acc_l2: -0.1394
     Episode_Reward/action_rate_l2: -0.0588
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3067
Metrics/base_velocity/error_vel_xy: 0.3999
Metrics/base_velocity/error_vel_yaw: 0.3189
      Episode_Termination/time_out: 0.6392
  Episode_Termination/base_contact: 0.3608
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.59s
                      Time elapsed: 00:50:19
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1188/1400 [0m                     

                       Computation: 9435 steps/s (collection: 2.341s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.7548
                       Mean reward: 12.30
               Mean episode length: 835.86
Episode_Reward/track_lin_vel_xy_exp: 0.6368
Episode_Reward/track_ang_vel_z_exp: 0.3434
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.0514
     Episode_Reward/dof_torques_l2: -0.0596
         Episode_Reward/dof_acc_l2: -0.1535
     Episode_Reward/action_rate_l2: -0.0618
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2994
Metrics/base_velocity/error_vel_xy: 0.4265
Metrics/base_velocity/error_vel_yaw: 0.3302
      Episode_Termination/time_out: 0.6375
  Episode_Termination/base_contact: 0.3625
--------------------------------------------------------------------------------
                   Total timesteps: 29220864
                    Iteration time: 2.60s
                      Time elapsed: 00:50:22
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1189/1400 [0m                     

                       Computation: 9725 steps/s (collection: 2.264s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.7394
                       Mean reward: 11.80
               Mean episode length: 800.60
Episode_Reward/track_lin_vel_xy_exp: 0.5852
Episode_Reward/track_ang_vel_z_exp: 0.3123
       Episode_Reward/lin_vel_z_l2: -0.0340
      Episode_Reward/ang_vel_xy_l2: -0.0443
     Episode_Reward/dof_torques_l2: -0.0529
         Episode_Reward/dof_acc_l2: -0.1262
     Episode_Reward/action_rate_l2: -0.0554
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3009
Metrics/base_velocity/error_vel_xy: 0.3754
Metrics/base_velocity/error_vel_yaw: 0.3037
      Episode_Termination/time_out: 0.6370
  Episode_Termination/base_contact: 0.3630
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 2.53s
                      Time elapsed: 00:50:24
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1190/1400 [0m                     

                       Computation: 9699 steps/s (collection: 2.276s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.7302
                       Mean reward: 11.95
               Mean episode length: 815.21
Episode_Reward/track_lin_vel_xy_exp: 0.5506
Episode_Reward/track_ang_vel_z_exp: 0.3031
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.0499
     Episode_Reward/dof_torques_l2: -0.0549
         Episode_Reward/dof_acc_l2: -0.1341
     Episode_Reward/action_rate_l2: -0.0551
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0107
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3049
Metrics/base_velocity/error_vel_xy: 0.4165
Metrics/base_velocity/error_vel_yaw: 0.3114
      Episode_Termination/time_out: 0.6382
  Episode_Termination/base_contact: 0.3618
--------------------------------------------------------------------------------
                   Total timesteps: 29270016
                    Iteration time: 2.53s
                      Time elapsed: 00:50:27
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1191/1400 [0m                     

                       Computation: 9879 steps/s (collection: 2.220s, learning 0.268s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7379
                       Mean reward: 11.51
               Mean episode length: 810.17
Episode_Reward/track_lin_vel_xy_exp: 0.6106
Episode_Reward/track_ang_vel_z_exp: 0.3304
       Episode_Reward/lin_vel_z_l2: -0.0438
      Episode_Reward/ang_vel_xy_l2: -0.0530
     Episode_Reward/dof_torques_l2: -0.0671
         Episode_Reward/dof_acc_l2: -0.1722
     Episode_Reward/action_rate_l2: -0.0632
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0150
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3121
Metrics/base_velocity/error_vel_xy: 0.5092
Metrics/base_velocity/error_vel_yaw: 0.4420
      Episode_Termination/time_out: 0.6401
  Episode_Termination/base_contact: 0.3599
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.49s
                      Time elapsed: 00:50:29
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 1192/1400 [0m                     

                       Computation: 9692 steps/s (collection: 2.273s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7474
                       Mean reward: 11.04
               Mean episode length: 809.15
Episode_Reward/track_lin_vel_xy_exp: 0.6048
Episode_Reward/track_ang_vel_z_exp: 0.3407
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.0536
     Episode_Reward/dof_torques_l2: -0.0622
         Episode_Reward/dof_acc_l2: -0.1572
     Episode_Reward/action_rate_l2: -0.0625
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0085
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3188
Metrics/base_velocity/error_vel_xy: 0.4867
Metrics/base_velocity/error_vel_yaw: 0.3448
      Episode_Termination/time_out: 0.6409
  Episode_Termination/base_contact: 0.3591
--------------------------------------------------------------------------------
                   Total timesteps: 29319168
                    Iteration time: 2.54s
                      Time elapsed: 00:50:32
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1193/1400 [0m                     

                       Computation: 9666 steps/s (collection: 2.284s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.7435
                       Mean reward: 10.27
               Mean episode length: 779.77
Episode_Reward/track_lin_vel_xy_exp: 0.5243
Episode_Reward/track_ang_vel_z_exp: 0.3001
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0513
     Episode_Reward/dof_torques_l2: -0.0576
         Episode_Reward/dof_acc_l2: -0.1437
     Episode_Reward/action_rate_l2: -0.0562
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0066
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3238
Metrics/base_velocity/error_vel_xy: 0.4786
Metrics/base_velocity/error_vel_yaw: 0.3373
      Episode_Termination/time_out: 0.6364
  Episode_Termination/base_contact: 0.3636
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 2.54s
                      Time elapsed: 00:50:34
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1194/1400 [0m                     

                       Computation: 9741 steps/s (collection: 2.268s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.7426
                       Mean reward: 10.51
               Mean episode length: 776.11
Episode_Reward/track_lin_vel_xy_exp: 0.6590
Episode_Reward/track_ang_vel_z_exp: 0.3579
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.0541
     Episode_Reward/dof_torques_l2: -0.0692
         Episode_Reward/dof_acc_l2: -0.1705
     Episode_Reward/action_rate_l2: -0.0667
      Episode_Reward/feet_air_time: -0.0100
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3328
Metrics/base_velocity/error_vel_xy: 0.4610
Metrics/base_velocity/error_vel_yaw: 0.3559
      Episode_Termination/time_out: 0.6300
  Episode_Termination/base_contact: 0.3700
--------------------------------------------------------------------------------
                   Total timesteps: 29368320
                    Iteration time: 2.52s
                      Time elapsed: 00:50:37
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1195/1400 [0m                     

                       Computation: 9878 steps/s (collection: 2.229s, learning 0.259s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 5.7579
                       Mean reward: 10.23
               Mean episode length: 771.94
Episode_Reward/track_lin_vel_xy_exp: 0.5101
Episode_Reward/track_ang_vel_z_exp: 0.2902
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0505
     Episode_Reward/dof_torques_l2: -0.0590
         Episode_Reward/dof_acc_l2: -0.1435
     Episode_Reward/action_rate_l2: -0.0552
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0101
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3406
Metrics/base_velocity/error_vel_xy: 0.4574
Metrics/base_velocity/error_vel_yaw: 0.3461
      Episode_Termination/time_out: 0.6294
  Episode_Termination/base_contact: 0.3706
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.49s
                      Time elapsed: 00:50:39
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1196/1400 [0m                     

                       Computation: 9778 steps/s (collection: 2.258s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.7628
                       Mean reward: 9.94
               Mean episode length: 767.30
Episode_Reward/track_lin_vel_xy_exp: 0.4902
Episode_Reward/track_ang_vel_z_exp: 0.2891
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.0502
     Episode_Reward/dof_torques_l2: -0.0569
         Episode_Reward/dof_acc_l2: -0.1451
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0086
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3418
Metrics/base_velocity/error_vel_xy: 0.5008
Metrics/base_velocity/error_vel_yaw: 0.3310
      Episode_Termination/time_out: 0.6272
  Episode_Termination/base_contact: 0.3728
--------------------------------------------------------------------------------
                   Total timesteps: 29417472
                    Iteration time: 2.51s
                      Time elapsed: 00:50:42
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1197/1400 [0m                     

                       Computation: 9709 steps/s (collection: 2.274s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 5.7597
                       Mean reward: 9.80
               Mean episode length: 764.70
Episode_Reward/track_lin_vel_xy_exp: 0.5582
Episode_Reward/track_ang_vel_z_exp: 0.3226
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.0521
     Episode_Reward/dof_torques_l2: -0.0646
         Episode_Reward/dof_acc_l2: -0.1521
     Episode_Reward/action_rate_l2: -0.0614
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0120
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3406
Metrics/base_velocity/error_vel_xy: 0.5163
Metrics/base_velocity/error_vel_yaw: 0.3570
      Episode_Termination/time_out: 0.6282
  Episode_Termination/base_contact: 0.3718
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 2.53s
                      Time elapsed: 00:50:44
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1198/1400 [0m                     

                       Computation: 9824 steps/s (collection: 2.242s, learning 0.260s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7590
                       Mean reward: 8.93
               Mean episode length: 731.07
Episode_Reward/track_lin_vel_xy_exp: 0.5560
Episode_Reward/track_ang_vel_z_exp: 0.3200
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0632
         Episode_Reward/dof_acc_l2: -0.1418
     Episode_Reward/action_rate_l2: -0.0604
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3318
Metrics/base_velocity/error_vel_xy: 0.5065
Metrics/base_velocity/error_vel_yaw: 0.3546
      Episode_Termination/time_out: 0.6258
  Episode_Termination/base_contact: 0.3742
--------------------------------------------------------------------------------
                   Total timesteps: 29466624
                    Iteration time: 2.50s
                      Time elapsed: 00:50:47
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1199/1400 [0m                     

                       Computation: 9714 steps/s (collection: 2.273s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.7642
                       Mean reward: 9.89
               Mean episode length: 750.89
Episode_Reward/track_lin_vel_xy_exp: 0.5937
Episode_Reward/track_ang_vel_z_exp: 0.3293
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0556
         Episode_Reward/dof_acc_l2: -0.1353
     Episode_Reward/action_rate_l2: -0.0581
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3290
Metrics/base_velocity/error_vel_xy: 0.4169
Metrics/base_velocity/error_vel_yaw: 0.3025
      Episode_Termination/time_out: 0.6257
  Episode_Termination/base_contact: 0.3743
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.53s
                      Time elapsed: 00:50:49
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1200/1400 [0m                     

                       Computation: 9758 steps/s (collection: 2.256s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 5.7660
                       Mean reward: 10.69
               Mean episode length: 792.66
Episode_Reward/track_lin_vel_xy_exp: 0.5772
Episode_Reward/track_ang_vel_z_exp: 0.3257
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.0489
     Episode_Reward/dof_torques_l2: -0.0573
         Episode_Reward/dof_acc_l2: -0.1321
     Episode_Reward/action_rate_l2: -0.0592
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3241
Metrics/base_velocity/error_vel_xy: 0.4431
Metrics/base_velocity/error_vel_yaw: 0.3134
      Episode_Termination/time_out: 0.6255
  Episode_Termination/base_contact: 0.3745
--------------------------------------------------------------------------------
                   Total timesteps: 29515776
                    Iteration time: 2.52s
                      Time elapsed: 00:50:52
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1201/1400 [0m                     

                       Computation: 9803 steps/s (collection: 2.249s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.7669
                       Mean reward: 11.08
               Mean episode length: 794.93
Episode_Reward/track_lin_vel_xy_exp: 0.6040
Episode_Reward/track_ang_vel_z_exp: 0.3199
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0484
     Episode_Reward/dof_torques_l2: -0.0525
         Episode_Reward/dof_acc_l2: -0.1262
     Episode_Reward/action_rate_l2: -0.0564
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0087
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3337
Metrics/base_velocity/error_vel_xy: 0.3855
Metrics/base_velocity/error_vel_yaw: 0.3112
      Episode_Termination/time_out: 0.6246
  Episode_Termination/base_contact: 0.3754
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 2.51s
                      Time elapsed: 00:50:54
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1202/1400 [0m                     

                       Computation: 9870 steps/s (collection: 2.230s, learning 0.260s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.7707
                       Mean reward: 10.96
               Mean episode length: 806.40
Episode_Reward/track_lin_vel_xy_exp: 0.6292
Episode_Reward/track_ang_vel_z_exp: 0.3668
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.0577
     Episode_Reward/dof_torques_l2: -0.0681
         Episode_Reward/dof_acc_l2: -0.1641
     Episode_Reward/action_rate_l2: -0.0667
      Episode_Reward/feet_air_time: -0.0101
 Episode_Reward/undesired_contacts: -0.0110
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3398
Metrics/base_velocity/error_vel_xy: 0.5594
Metrics/base_velocity/error_vel_yaw: 0.3655
      Episode_Termination/time_out: 0.6275
  Episode_Termination/base_contact: 0.3725
--------------------------------------------------------------------------------
                   Total timesteps: 29564928
                    Iteration time: 2.49s
                      Time elapsed: 00:50:57
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1203/1400 [0m                     

                       Computation: 9739 steps/s (collection: 2.261s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0210
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 5.7662
                       Mean reward: 11.08
               Mean episode length: 799.53
Episode_Reward/track_lin_vel_xy_exp: 0.5964
Episode_Reward/track_ang_vel_z_exp: 0.3269
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.0502
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.1404
     Episode_Reward/action_rate_l2: -0.0587
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0075
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3536
Metrics/base_velocity/error_vel_xy: 0.4361
Metrics/base_velocity/error_vel_yaw: 0.3250
      Episode_Termination/time_out: 0.6308
  Episode_Termination/base_contact: 0.3692
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.52s
                      Time elapsed: 00:50:59
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1204/1400 [0m                     

                       Computation: 9788 steps/s (collection: 2.254s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7625
                       Mean reward: 10.85
               Mean episode length: 802.68
Episode_Reward/track_lin_vel_xy_exp: 0.5609
Episode_Reward/track_ang_vel_z_exp: 0.3128
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.0509
     Episode_Reward/dof_torques_l2: -0.0555
         Episode_Reward/dof_acc_l2: -0.1464
     Episode_Reward/action_rate_l2: -0.0572
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3546
Metrics/base_velocity/error_vel_xy: 0.4510
Metrics/base_velocity/error_vel_yaw: 0.3362
      Episode_Termination/time_out: 0.6320
  Episode_Termination/base_contact: 0.3680
--------------------------------------------------------------------------------
                   Total timesteps: 29614080
                    Iteration time: 2.51s
                      Time elapsed: 00:51:02
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1205/1400 [0m                     

                       Computation: 9791 steps/s (collection: 2.254s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 5.7577
                       Mean reward: 9.97
               Mean episode length: 750.08
Episode_Reward/track_lin_vel_xy_exp: 0.5026
Episode_Reward/track_ang_vel_z_exp: 0.2904
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.1321
     Episode_Reward/action_rate_l2: -0.0559
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0180
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3608
Metrics/base_velocity/error_vel_xy: 0.5179
Metrics/base_velocity/error_vel_yaw: 0.3538
      Episode_Termination/time_out: 0.6331
  Episode_Termination/base_contact: 0.3669
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 2.51s
                      Time elapsed: 00:51:04
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1206/1400 [0m                     

                       Computation: 9814 steps/s (collection: 2.246s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 5.7477
                       Mean reward: 9.83
               Mean episode length: 746.95
Episode_Reward/track_lin_vel_xy_exp: 0.5060
Episode_Reward/track_ang_vel_z_exp: 0.2833
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.0501
     Episode_Reward/dof_torques_l2: -0.0536
         Episode_Reward/dof_acc_l2: -0.1385
     Episode_Reward/action_rate_l2: -0.0522
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3599
Metrics/base_velocity/error_vel_xy: 0.4099
Metrics/base_velocity/error_vel_yaw: 0.2972
      Episode_Termination/time_out: 0.6299
  Episode_Termination/base_contact: 0.3701
--------------------------------------------------------------------------------
                   Total timesteps: 29663232
                    Iteration time: 2.50s
                      Time elapsed: 00:51:07
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1207/1400 [0m                     

                       Computation: 9795 steps/s (collection: 2.245s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.7397
                       Mean reward: 10.69
               Mean episode length: 781.48
Episode_Reward/track_lin_vel_xy_exp: 0.6442
Episode_Reward/track_ang_vel_z_exp: 0.3441
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.0534
     Episode_Reward/dof_torques_l2: -0.0623
         Episode_Reward/dof_acc_l2: -0.1521
     Episode_Reward/action_rate_l2: -0.0633
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3671
Metrics/base_velocity/error_vel_xy: 0.4407
Metrics/base_velocity/error_vel_yaw: 0.3565
      Episode_Termination/time_out: 0.6292
  Episode_Termination/base_contact: 0.3708
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.51s
                      Time elapsed: 00:51:09
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1208/1400 [0m                     

                       Computation: 9748 steps/s (collection: 2.262s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 5.7446
                       Mean reward: 10.80
               Mean episode length: 799.83
Episode_Reward/track_lin_vel_xy_exp: 0.5616
Episode_Reward/track_ang_vel_z_exp: 0.3149
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0495
     Episode_Reward/dof_torques_l2: -0.0603
         Episode_Reward/dof_acc_l2: -0.1354
     Episode_Reward/action_rate_l2: -0.0582
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0067
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3795
Metrics/base_velocity/error_vel_xy: 0.4507
Metrics/base_velocity/error_vel_yaw: 0.3337
      Episode_Termination/time_out: 0.6293
  Episode_Termination/base_contact: 0.3707
--------------------------------------------------------------------------------
                   Total timesteps: 29712384
                    Iteration time: 2.52s
                      Time elapsed: 00:51:12
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1209/1400 [0m                     

                       Computation: 9827 steps/s (collection: 2.247s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 5.7466
                       Mean reward: 10.67
               Mean episode length: 781.73
Episode_Reward/track_lin_vel_xy_exp: 0.6376
Episode_Reward/track_ang_vel_z_exp: 0.3497
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0610
         Episode_Reward/dof_acc_l2: -0.1439
     Episode_Reward/action_rate_l2: -0.0625
      Episode_Reward/feet_air_time: -0.0101
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3802
Metrics/base_velocity/error_vel_xy: 0.4598
Metrics/base_velocity/error_vel_yaw: 0.3464
      Episode_Termination/time_out: 0.6297
  Episode_Termination/base_contact: 0.3703
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 2.50s
                      Time elapsed: 00:51:14
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1210/1400 [0m                     

                       Computation: 9878 steps/s (collection: 2.225s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.7496
                       Mean reward: 10.96
               Mean episode length: 794.86
Episode_Reward/track_lin_vel_xy_exp: 0.5681
Episode_Reward/track_ang_vel_z_exp: 0.3087
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0578
         Episode_Reward/dof_acc_l2: -0.1350
     Episode_Reward/action_rate_l2: -0.0566
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3733
Metrics/base_velocity/error_vel_xy: 0.4221
Metrics/base_velocity/error_vel_yaw: 0.3382
      Episode_Termination/time_out: 0.6316
  Episode_Termination/base_contact: 0.3684
--------------------------------------------------------------------------------
                   Total timesteps: 29761536
                    Iteration time: 2.49s
                      Time elapsed: 00:51:17
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1211/1400 [0m                     

                       Computation: 9778 steps/s (collection: 2.257s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.7541
                       Mean reward: 10.93
               Mean episode length: 805.37
Episode_Reward/track_lin_vel_xy_exp: 0.6064
Episode_Reward/track_ang_vel_z_exp: 0.3424
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0527
     Episode_Reward/dof_torques_l2: -0.0669
         Episode_Reward/dof_acc_l2: -0.1493
     Episode_Reward/action_rate_l2: -0.0636
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3644
Metrics/base_velocity/error_vel_xy: 0.5072
Metrics/base_velocity/error_vel_yaw: 0.3677
      Episode_Termination/time_out: 0.6355
  Episode_Termination/base_contact: 0.3645
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.51s
                      Time elapsed: 00:51:19
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1212/1400 [0m                     

                       Computation: 9795 steps/s (collection: 2.244s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.7637
                       Mean reward: 10.44
               Mean episode length: 786.54
Episode_Reward/track_lin_vel_xy_exp: 0.5185
Episode_Reward/track_ang_vel_z_exp: 0.2949
       Episode_Reward/lin_vel_z_l2: -0.0367
      Episode_Reward/ang_vel_xy_l2: -0.0478
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1290
     Episode_Reward/action_rate_l2: -0.0548
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0218
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3606
Metrics/base_velocity/error_vel_xy: 0.4611
Metrics/base_velocity/error_vel_yaw: 0.3379
      Episode_Termination/time_out: 0.6340
  Episode_Termination/base_contact: 0.3660
--------------------------------------------------------------------------------
                   Total timesteps: 29810688
                    Iteration time: 2.51s
                      Time elapsed: 00:51:22
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1213/1400 [0m                     

                       Computation: 9780 steps/s (collection: 2.250s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0183
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7585
                       Mean reward: 9.98
               Mean episode length: 776.84
Episode_Reward/track_lin_vel_xy_exp: 0.5254
Episode_Reward/track_ang_vel_z_exp: 0.2930
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0561
         Episode_Reward/dof_acc_l2: -0.1324
     Episode_Reward/action_rate_l2: -0.0545
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3568
Metrics/base_velocity/error_vel_xy: 0.4282
Metrics/base_velocity/error_vel_yaw: 0.3152
      Episode_Termination/time_out: 0.6324
  Episode_Termination/base_contact: 0.3676
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 2.51s
                      Time elapsed: 00:51:24
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1214/1400 [0m                     

                       Computation: 9662 steps/s (collection: 2.289s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.7466
                       Mean reward: 9.43
               Mean episode length: 753.55
Episode_Reward/track_lin_vel_xy_exp: 0.5619
Episode_Reward/track_ang_vel_z_exp: 0.3229
       Episode_Reward/lin_vel_z_l2: -0.0466
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0638
         Episode_Reward/dof_acc_l2: -0.1605
     Episode_Reward/action_rate_l2: -0.0610
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3572
Metrics/base_velocity/error_vel_xy: 0.4836
Metrics/base_velocity/error_vel_yaw: 0.3344
      Episode_Termination/time_out: 0.6321
  Episode_Termination/base_contact: 0.3679
--------------------------------------------------------------------------------
                   Total timesteps: 29859840
                    Iteration time: 2.54s
                      Time elapsed: 00:51:27
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1215/1400 [0m                     

                       Computation: 9663 steps/s (collection: 2.287s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7387
                       Mean reward: 9.38
               Mean episode length: 747.89
Episode_Reward/track_lin_vel_xy_exp: 0.5596
Episode_Reward/track_ang_vel_z_exp: 0.3038
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.0531
     Episode_Reward/dof_torques_l2: -0.0594
         Episode_Reward/dof_acc_l2: -0.1552
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0073
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3681
Metrics/base_velocity/error_vel_xy: 0.4370
Metrics/base_velocity/error_vel_yaw: 0.3484
      Episode_Termination/time_out: 0.6314
  Episode_Termination/base_contact: 0.3686
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.54s
                      Time elapsed: 00:51:29
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1216/1400 [0m                     

                       Computation: 9826 steps/s (collection: 2.243s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.7331
                       Mean reward: 9.82
               Mean episode length: 772.74
Episode_Reward/track_lin_vel_xy_exp: 0.5554
Episode_Reward/track_ang_vel_z_exp: 0.3122
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.0505
     Episode_Reward/dof_torques_l2: -0.0586
         Episode_Reward/dof_acc_l2: -0.1403
     Episode_Reward/action_rate_l2: -0.0575
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0081
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3630
Metrics/base_velocity/error_vel_xy: 0.4513
Metrics/base_velocity/error_vel_yaw: 0.3286
      Episode_Termination/time_out: 0.6274
  Episode_Termination/base_contact: 0.3726
--------------------------------------------------------------------------------
                   Total timesteps: 29908992
                    Iteration time: 2.50s
                      Time elapsed: 00:51:32
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1217/1400 [0m                     

                       Computation: 9695 steps/s (collection: 2.279s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 5.7319
                       Mean reward: 10.00
               Mean episode length: 774.31
Episode_Reward/track_lin_vel_xy_exp: 0.5202
Episode_Reward/track_ang_vel_z_exp: 0.2903
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0468
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1360
     Episode_Reward/action_rate_l2: -0.0545
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0184
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3526
Metrics/base_velocity/error_vel_xy: 0.4306
Metrics/base_velocity/error_vel_yaw: 0.3257
      Episode_Termination/time_out: 0.6313
  Episode_Termination/base_contact: 0.3687
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 2.53s
                      Time elapsed: 00:51:34
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1218/1400 [0m                     

                       Computation: 9747 steps/s (collection: 2.259s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 5.7261
                       Mean reward: 9.30
               Mean episode length: 742.57
Episode_Reward/track_lin_vel_xy_exp: 0.4802
Episode_Reward/track_ang_vel_z_exp: 0.2833
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.0504
     Episode_Reward/dof_torques_l2: -0.0572
         Episode_Reward/dof_acc_l2: -0.1416
     Episode_Reward/action_rate_l2: -0.0545
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0111
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3504
Metrics/base_velocity/error_vel_xy: 0.5021
Metrics/base_velocity/error_vel_yaw: 0.3353
      Episode_Termination/time_out: 0.6282
  Episode_Termination/base_contact: 0.3718
--------------------------------------------------------------------------------
                   Total timesteps: 29958144
                    Iteration time: 2.52s
                      Time elapsed: 00:51:37
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1219/1400 [0m                     

                       Computation: 9739 steps/s (collection: 2.267s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 5.7212
                       Mean reward: 10.15
               Mean episode length: 764.69
Episode_Reward/track_lin_vel_xy_exp: 0.6456
Episode_Reward/track_ang_vel_z_exp: 0.3513
       Episode_Reward/lin_vel_z_l2: -0.0378
      Episode_Reward/ang_vel_xy_l2: -0.0536
     Episode_Reward/dof_torques_l2: -0.0599
         Episode_Reward/dof_acc_l2: -0.1420
     Episode_Reward/action_rate_l2: -0.0631
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3423
Metrics/base_velocity/error_vel_xy: 0.4300
Metrics/base_velocity/error_vel_yaw: 0.3269
      Episode_Termination/time_out: 0.6265
  Episode_Termination/base_contact: 0.3735
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.52s
                      Time elapsed: 00:51:40
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1220/1400 [0m                     

                       Computation: 9728 steps/s (collection: 2.271s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 5.7195
                       Mean reward: 9.57
               Mean episode length: 748.14
Episode_Reward/track_lin_vel_xy_exp: 0.5385
Episode_Reward/track_ang_vel_z_exp: 0.2975
       Episode_Reward/lin_vel_z_l2: -0.0372
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0555
         Episode_Reward/dof_acc_l2: -0.1249
     Episode_Reward/action_rate_l2: -0.0558
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0118
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3406
Metrics/base_velocity/error_vel_xy: 0.4164
Metrics/base_velocity/error_vel_yaw: 0.3174
      Episode_Termination/time_out: 0.6237
  Episode_Termination/base_contact: 0.3763
--------------------------------------------------------------------------------
                   Total timesteps: 30007296
                    Iteration time: 2.53s
                      Time elapsed: 00:51:42
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1221/1400 [0m                     

                       Computation: 9671 steps/s (collection: 2.275s, learning 0.266s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0176
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.7169
                       Mean reward: 10.31
               Mean episode length: 749.36
Episode_Reward/track_lin_vel_xy_exp: 0.5218
Episode_Reward/track_ang_vel_z_exp: 0.2844
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0523
         Episode_Reward/dof_acc_l2: -0.1234
     Episode_Reward/action_rate_l2: -0.0526
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3354
Metrics/base_velocity/error_vel_xy: 0.3866
Metrics/base_velocity/error_vel_yaw: 0.2980
      Episode_Termination/time_out: 0.6211
  Episode_Termination/base_contact: 0.3789
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 2.54s
                      Time elapsed: 00:51:45
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1222/1400 [0m                     

                       Computation: 9654 steps/s (collection: 2.289s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.7164
                       Mean reward: 10.65
               Mean episode length: 773.72
Episode_Reward/track_lin_vel_xy_exp: 0.5764
Episode_Reward/track_ang_vel_z_exp: 0.3210
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0606
         Episode_Reward/dof_acc_l2: -0.1406
     Episode_Reward/action_rate_l2: -0.0593
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3445
Metrics/base_velocity/error_vel_xy: 0.4367
Metrics/base_velocity/error_vel_yaw: 0.3217
      Episode_Termination/time_out: 0.6223
  Episode_Termination/base_contact: 0.3777
--------------------------------------------------------------------------------
                   Total timesteps: 30056448
                    Iteration time: 2.55s
                      Time elapsed: 00:51:47
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1223/1400 [0m                     

                       Computation: 9510 steps/s (collection: 2.325s, learning 0.259s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7154
                       Mean reward: 10.68
               Mean episode length: 797.15
Episode_Reward/track_lin_vel_xy_exp: 0.5628
Episode_Reward/track_ang_vel_z_exp: 0.3228
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.0525
     Episode_Reward/dof_torques_l2: -0.0624
         Episode_Reward/dof_acc_l2: -0.1360
     Episode_Reward/action_rate_l2: -0.0609
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3435
Metrics/base_velocity/error_vel_xy: 0.5221
Metrics/base_velocity/error_vel_yaw: 0.3815
      Episode_Termination/time_out: 0.6228
  Episode_Termination/base_contact: 0.3772
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.58s
                      Time elapsed: 00:51:50
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1224/1400 [0m                     

                       Computation: 9705 steps/s (collection: 2.268s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.7188
                       Mean reward: 10.71
               Mean episode length: 805.19
Episode_Reward/track_lin_vel_xy_exp: 0.5968
Episode_Reward/track_ang_vel_z_exp: 0.3186
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.0470
     Episode_Reward/dof_torques_l2: -0.0535
         Episode_Reward/dof_acc_l2: -0.1275
     Episode_Reward/action_rate_l2: -0.0573
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3271
Metrics/base_velocity/error_vel_xy: 0.3858
Metrics/base_velocity/error_vel_yaw: 0.3052
      Episode_Termination/time_out: 0.6230
  Episode_Termination/base_contact: 0.3770
--------------------------------------------------------------------------------
                   Total timesteps: 30105600
                    Iteration time: 2.53s
                      Time elapsed: 00:51:52
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1225/1400 [0m                     

                       Computation: 9600 steps/s (collection: 2.304s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 5.7204
                       Mean reward: 10.21
               Mean episode length: 805.74
Episode_Reward/track_lin_vel_xy_exp: 0.5466
Episode_Reward/track_ang_vel_z_exp: 0.3061
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0535
     Episode_Reward/dof_torques_l2: -0.0577
         Episode_Reward/dof_acc_l2: -0.1450
     Episode_Reward/action_rate_l2: -0.0565
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0074
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3159
Metrics/base_velocity/error_vel_xy: 0.4538
Metrics/base_velocity/error_vel_yaw: 0.3416
      Episode_Termination/time_out: 0.6254
  Episode_Termination/base_contact: 0.3746
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 2.56s
                      Time elapsed: 00:51:55
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1226/1400 [0m                     

                       Computation: 9605 steps/s (collection: 2.305s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.7212
                       Mean reward: 10.16
               Mean episode length: 782.19
Episode_Reward/track_lin_vel_xy_exp: 0.5082
Episode_Reward/track_ang_vel_z_exp: 0.2706
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0433
     Episode_Reward/dof_torques_l2: -0.0478
         Episode_Reward/dof_acc_l2: -0.1157
     Episode_Reward/action_rate_l2: -0.0506
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0086
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3085
Metrics/base_velocity/error_vel_xy: 0.3624
Metrics/base_velocity/error_vel_yaw: 0.2998
      Episode_Termination/time_out: 0.6207
  Episode_Termination/base_contact: 0.3793
--------------------------------------------------------------------------------
                   Total timesteps: 30154752
                    Iteration time: 2.56s
                      Time elapsed: 00:51:57
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1227/1400 [0m                     

                       Computation: 9641 steps/s (collection: 2.292s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.7168
                       Mean reward: 10.32
               Mean episode length: 773.81
Episode_Reward/track_lin_vel_xy_exp: 0.5103
Episode_Reward/track_ang_vel_z_exp: 0.2807
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0451
     Episode_Reward/dof_torques_l2: -0.0540
         Episode_Reward/dof_acc_l2: -0.1222
     Episode_Reward/action_rate_l2: -0.0538
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0069
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3118
Metrics/base_velocity/error_vel_xy: 0.4071
Metrics/base_velocity/error_vel_yaw: 0.3178
      Episode_Termination/time_out: 0.6211
  Episode_Termination/base_contact: 0.3789
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.55s
                      Time elapsed: 00:52:00
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1228/1400 [0m                     

                       Computation: 9795 steps/s (collection: 2.246s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7130
                       Mean reward: 10.17
               Mean episode length: 756.78
Episode_Reward/track_lin_vel_xy_exp: 0.5658
Episode_Reward/track_ang_vel_z_exp: 0.3109
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0567
         Episode_Reward/dof_acc_l2: -0.1379
     Episode_Reward/action_rate_l2: -0.0571
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0074
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3140
Metrics/base_velocity/error_vel_xy: 0.4171
Metrics/base_velocity/error_vel_yaw: 0.3212
      Episode_Termination/time_out: 0.6190
  Episode_Termination/base_contact: 0.3810
--------------------------------------------------------------------------------
                   Total timesteps: 30203904
                    Iteration time: 2.51s
                      Time elapsed: 00:52:02
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1229/1400 [0m                     

                       Computation: 9488 steps/s (collection: 2.333s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.7185
                       Mean reward: 11.26
               Mean episode length: 797.90
Episode_Reward/track_lin_vel_xy_exp: 0.6255
Episode_Reward/track_ang_vel_z_exp: 0.3473
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.0524
     Episode_Reward/dof_torques_l2: -0.0625
         Episode_Reward/dof_acc_l2: -0.1467
     Episode_Reward/action_rate_l2: -0.0644
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3217
Metrics/base_velocity/error_vel_xy: 0.4830
Metrics/base_velocity/error_vel_yaw: 0.3590
      Episode_Termination/time_out: 0.6252
  Episode_Termination/base_contact: 0.3748
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 2.59s
                      Time elapsed: 00:52:05
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1230/1400 [0m                     

                       Computation: 9521 steps/s (collection: 2.324s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.7238
                       Mean reward: 11.45
               Mean episode length: 804.56
Episode_Reward/track_lin_vel_xy_exp: 0.5892
Episode_Reward/track_ang_vel_z_exp: 0.3208
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0497
     Episode_Reward/dof_torques_l2: -0.0580
         Episode_Reward/dof_acc_l2: -0.1391
     Episode_Reward/action_rate_l2: -0.0589
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0105
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3299
Metrics/base_velocity/error_vel_xy: 0.4118
Metrics/base_velocity/error_vel_yaw: 0.3158
      Episode_Termination/time_out: 0.6264
  Episode_Termination/base_contact: 0.3736
--------------------------------------------------------------------------------
                   Total timesteps: 30253056
                    Iteration time: 2.58s
                      Time elapsed: 00:52:08
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1231/1400 [0m                     

                       Computation: 9584 steps/s (collection: 2.299s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7301
                       Mean reward: 11.08
               Mean episode length: 794.47
Episode_Reward/track_lin_vel_xy_exp: 0.5377
Episode_Reward/track_ang_vel_z_exp: 0.3078
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0580
         Episode_Reward/dof_acc_l2: -0.1235
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3285
Metrics/base_velocity/error_vel_xy: 0.4558
Metrics/base_velocity/error_vel_yaw: 0.3113
      Episode_Termination/time_out: 0.6261
  Episode_Termination/base_contact: 0.3739
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.56s
                      Time elapsed: 00:52:10
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1232/1400 [0m                     

                       Computation: 9473 steps/s (collection: 2.338s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7262
                       Mean reward: 10.65
               Mean episode length: 787.43
Episode_Reward/track_lin_vel_xy_exp: 0.6118
Episode_Reward/track_ang_vel_z_exp: 0.3309
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.0552
     Episode_Reward/dof_torques_l2: -0.0644
         Episode_Reward/dof_acc_l2: -0.1486
     Episode_Reward/action_rate_l2: -0.0628
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0085
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3266
Metrics/base_velocity/error_vel_xy: 0.4688
Metrics/base_velocity/error_vel_yaw: 0.3777
      Episode_Termination/time_out: 0.6247
  Episode_Termination/base_contact: 0.3753
--------------------------------------------------------------------------------
                   Total timesteps: 30302208
                    Iteration time: 2.59s
                      Time elapsed: 00:52:13
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1233/1400 [0m                     

                       Computation: 9481 steps/s (collection: 2.335s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.7266
                       Mean reward: 10.44
               Mean episode length: 780.45
Episode_Reward/track_lin_vel_xy_exp: 0.5812
Episode_Reward/track_ang_vel_z_exp: 0.3204
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.1406
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3327
Metrics/base_velocity/error_vel_xy: 0.4351
Metrics/base_velocity/error_vel_yaw: 0.3228
      Episode_Termination/time_out: 0.6225
  Episode_Termination/base_contact: 0.3775
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 2.59s
                      Time elapsed: 00:52:15
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1234/1400 [0m                     

                       Computation: 9572 steps/s (collection: 2.312s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7297
                       Mean reward: 10.69
               Mean episode length: 808.76
Episode_Reward/track_lin_vel_xy_exp: 0.6184
Episode_Reward/track_ang_vel_z_exp: 0.3484
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.0555
     Episode_Reward/dof_torques_l2: -0.0664
         Episode_Reward/dof_acc_l2: -0.1513
     Episode_Reward/action_rate_l2: -0.0635
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0083
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3344
Metrics/base_velocity/error_vel_xy: 0.5116
Metrics/base_velocity/error_vel_yaw: 0.3695
      Episode_Termination/time_out: 0.6217
  Episode_Termination/base_contact: 0.3783
--------------------------------------------------------------------------------
                   Total timesteps: 30351360
                    Iteration time: 2.57s
                      Time elapsed: 00:52:18
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1235/1400 [0m                     

                       Computation: 9444 steps/s (collection: 2.346s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7514
                       Mean reward: 10.38
               Mean episode length: 782.39
Episode_Reward/track_lin_vel_xy_exp: 0.5060
Episode_Reward/track_ang_vel_z_exp: 0.2833
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0438
     Episode_Reward/dof_torques_l2: -0.0532
         Episode_Reward/dof_acc_l2: -0.1254
     Episode_Reward/action_rate_l2: -0.0528
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3390
Metrics/base_velocity/error_vel_xy: 0.4110
Metrics/base_velocity/error_vel_yaw: 0.2987
      Episode_Termination/time_out: 0.6243
  Episode_Termination/base_contact: 0.3757
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.60s
                      Time elapsed: 00:52:21
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1236/1400 [0m                     

                       Computation: 9560 steps/s (collection: 2.307s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7643
                       Mean reward: 10.94
               Mean episode length: 803.43
Episode_Reward/track_lin_vel_xy_exp: 0.6721
Episode_Reward/track_ang_vel_z_exp: 0.3689
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.0550
     Episode_Reward/dof_torques_l2: -0.0657
         Episode_Reward/dof_acc_l2: -0.1544
     Episode_Reward/action_rate_l2: -0.0672
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0076
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3385
Metrics/base_velocity/error_vel_xy: 0.4799
Metrics/base_velocity/error_vel_yaw: 0.3663
      Episode_Termination/time_out: 0.6229
  Episode_Termination/base_contact: 0.3771
--------------------------------------------------------------------------------
                   Total timesteps: 30400512
                    Iteration time: 2.57s
                      Time elapsed: 00:52:23
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1237/1400 [0m                     

                       Computation: 9502 steps/s (collection: 2.331s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.7686
                       Mean reward: 10.48
               Mean episode length: 757.48
Episode_Reward/track_lin_vel_xy_exp: 0.5208
Episode_Reward/track_ang_vel_z_exp: 0.2853
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0535
         Episode_Reward/dof_acc_l2: -0.1206
     Episode_Reward/action_rate_l2: -0.0531
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3440
Metrics/base_velocity/error_vel_xy: 0.4009
Metrics/base_velocity/error_vel_yaw: 0.3091
      Episode_Termination/time_out: 0.6240
  Episode_Termination/base_contact: 0.3760
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 2.59s
                      Time elapsed: 00:52:26
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1238/1400 [0m                     

                       Computation: 9600 steps/s (collection: 2.304s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7658
                       Mean reward: 10.17
               Mean episode length: 762.11
Episode_Reward/track_lin_vel_xy_exp: 0.4264
Episode_Reward/track_ang_vel_z_exp: 0.2446
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0437
     Episode_Reward/dof_torques_l2: -0.0496
         Episode_Reward/dof_acc_l2: -0.1165
     Episode_Reward/action_rate_l2: -0.0468
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0128
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3512
Metrics/base_velocity/error_vel_xy: 0.4099
Metrics/base_velocity/error_vel_yaw: 0.3022
      Episode_Termination/time_out: 0.6246
  Episode_Termination/base_contact: 0.3754
--------------------------------------------------------------------------------
                   Total timesteps: 30449664
                    Iteration time: 2.56s
                      Time elapsed: 00:52:28
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1239/1400 [0m                     

                       Computation: 9396 steps/s (collection: 2.359s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 5.7650
                       Mean reward: 10.22
               Mean episode length: 765.75
Episode_Reward/track_lin_vel_xy_exp: 0.6307
Episode_Reward/track_ang_vel_z_exp: 0.3437
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.0497
     Episode_Reward/dof_torques_l2: -0.0599
         Episode_Reward/dof_acc_l2: -0.1402
     Episode_Reward/action_rate_l2: -0.0613
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3604
Metrics/base_velocity/error_vel_xy: 0.4282
Metrics/base_velocity/error_vel_yaw: 0.3242
      Episode_Termination/time_out: 0.6254
  Episode_Termination/base_contact: 0.3746
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.62s
                      Time elapsed: 00:52:31
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1240/1400 [0m                     

                       Computation: 9627 steps/s (collection: 2.290s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 5.7551
                       Mean reward: 9.95
               Mean episode length: 737.75
Episode_Reward/track_lin_vel_xy_exp: 0.4940
Episode_Reward/track_ang_vel_z_exp: 0.2685
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.1151
     Episode_Reward/action_rate_l2: -0.0497
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3638
Metrics/base_velocity/error_vel_xy: 0.3762
Metrics/base_velocity/error_vel_yaw: 0.2984
      Episode_Termination/time_out: 0.6267
  Episode_Termination/base_contact: 0.3733
--------------------------------------------------------------------------------
                   Total timesteps: 30498816
                    Iteration time: 2.55s
                      Time elapsed: 00:52:33
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1241/1400 [0m                     

                       Computation: 9581 steps/s (collection: 2.309s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7468
                       Mean reward: 10.23
               Mean episode length: 740.30
Episode_Reward/track_lin_vel_xy_exp: 0.4971
Episode_Reward/track_ang_vel_z_exp: 0.2839
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.0463
     Episode_Reward/dof_torques_l2: -0.0505
         Episode_Reward/dof_acc_l2: -0.1187
     Episode_Reward/action_rate_l2: -0.0527
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3617
Metrics/base_velocity/error_vel_xy: 0.4205
Metrics/base_velocity/error_vel_yaw: 0.2928
      Episode_Termination/time_out: 0.6214
  Episode_Termination/base_contact: 0.3786
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 2.57s
                      Time elapsed: 00:52:36
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1242/1400 [0m                     

                       Computation: 9482 steps/s (collection: 2.336s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7391
                       Mean reward: 9.34
               Mean episode length: 720.18
Episode_Reward/track_lin_vel_xy_exp: 0.5602
Episode_Reward/track_ang_vel_z_exp: 0.3088
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0507
     Episode_Reward/dof_torques_l2: -0.0567
         Episode_Reward/dof_acc_l2: -0.1308
     Episode_Reward/action_rate_l2: -0.0576
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3521
Metrics/base_velocity/error_vel_xy: 0.4492
Metrics/base_velocity/error_vel_yaw: 0.3370
      Episode_Termination/time_out: 0.6229
  Episode_Termination/base_contact: 0.3771
--------------------------------------------------------------------------------
                   Total timesteps: 30547968
                    Iteration time: 2.59s
                      Time elapsed: 00:52:39
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1243/1400 [0m                     

                       Computation: 9566 steps/s (collection: 2.312s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.7407
                       Mean reward: 9.86
               Mean episode length: 750.92
Episode_Reward/track_lin_vel_xy_exp: 0.5372
Episode_Reward/track_ang_vel_z_exp: 0.2950
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0527
         Episode_Reward/dof_acc_l2: -0.1294
     Episode_Reward/action_rate_l2: -0.0538
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0067
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3608
Metrics/base_velocity/error_vel_xy: 0.3955
Metrics/base_velocity/error_vel_yaw: 0.2959
      Episode_Termination/time_out: 0.6223
  Episode_Termination/base_contact: 0.3777
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.57s
                      Time elapsed: 00:52:41
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1244/1400 [0m                     

                       Computation: 9608 steps/s (collection: 2.301s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.7437
                       Mean reward: 10.54
               Mean episode length: 773.89
Episode_Reward/track_lin_vel_xy_exp: 0.5982
Episode_Reward/track_ang_vel_z_exp: 0.3220
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0542
         Episode_Reward/dof_acc_l2: -0.1390
     Episode_Reward/action_rate_l2: -0.0582
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0088
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3674
Metrics/base_velocity/error_vel_xy: 0.3990
Metrics/base_velocity/error_vel_yaw: 0.3095
      Episode_Termination/time_out: 0.6182
  Episode_Termination/base_contact: 0.3818
--------------------------------------------------------------------------------
                   Total timesteps: 30597120
                    Iteration time: 2.56s
                      Time elapsed: 00:52:44
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1245/1400 [0m                     

                       Computation: 9503 steps/s (collection: 2.331s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7477
                       Mean reward: 11.34
               Mean episode length: 794.72
Episode_Reward/track_lin_vel_xy_exp: 0.5557
Episode_Reward/track_ang_vel_z_exp: 0.3100
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0468
     Episode_Reward/dof_torques_l2: -0.0539
         Episode_Reward/dof_acc_l2: -0.1283
     Episode_Reward/action_rate_l2: -0.0562
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3728
Metrics/base_velocity/error_vel_xy: 0.4177
Metrics/base_velocity/error_vel_yaw: 0.2993
      Episode_Termination/time_out: 0.6195
  Episode_Termination/base_contact: 0.3805
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 2.59s
                      Time elapsed: 00:52:46
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1246/1400 [0m                     

                       Computation: 9602 steps/s (collection: 2.295s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7593
                       Mean reward: 10.61
               Mean episode length: 757.02
Episode_Reward/track_lin_vel_xy_exp: 0.5038
Episode_Reward/track_ang_vel_z_exp: 0.2817
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.0463
     Episode_Reward/dof_torques_l2: -0.0515
         Episode_Reward/dof_acc_l2: -0.1301
     Episode_Reward/action_rate_l2: -0.0531
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0096
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3703
Metrics/base_velocity/error_vel_xy: 0.4312
Metrics/base_velocity/error_vel_yaw: 0.3222
      Episode_Termination/time_out: 0.6248
  Episode_Termination/base_contact: 0.3752
--------------------------------------------------------------------------------
                   Total timesteps: 30646272
                    Iteration time: 2.56s
                      Time elapsed: 00:52:49
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1247/1400 [0m                     

                       Computation: 9526 steps/s (collection: 2.327s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 5.7523
                       Mean reward: 10.23
               Mean episode length: 743.05
Episode_Reward/track_lin_vel_xy_exp: 0.5251
Episode_Reward/track_ang_vel_z_exp: 0.2977
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0534
         Episode_Reward/dof_acc_l2: -0.1302
     Episode_Reward/action_rate_l2: -0.0542
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3625
Metrics/base_velocity/error_vel_xy: 0.4358
Metrics/base_velocity/error_vel_yaw: 0.3093
      Episode_Termination/time_out: 0.6236
  Episode_Termination/base_contact: 0.3764
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.58s
                      Time elapsed: 00:52:51
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1248/1400 [0m                     

                       Computation: 9538 steps/s (collection: 2.320s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 5.7519
                       Mean reward: 10.63
               Mean episode length: 781.14
Episode_Reward/track_lin_vel_xy_exp: 0.6753
Episode_Reward/track_ang_vel_z_exp: 0.3734
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0629
         Episode_Reward/dof_acc_l2: -0.1485
     Episode_Reward/action_rate_l2: -0.0661
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3653
Metrics/base_velocity/error_vel_xy: 0.4699
Metrics/base_velocity/error_vel_yaw: 0.3371
      Episode_Termination/time_out: 0.6234
  Episode_Termination/base_contact: 0.3766
--------------------------------------------------------------------------------
                   Total timesteps: 30695424
                    Iteration time: 2.58s
                      Time elapsed: 00:52:54
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1249/1400 [0m                     

                       Computation: 9613 steps/s (collection: 2.293s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.7544
                       Mean reward: 11.32
               Mean episode length: 810.73
Episode_Reward/track_lin_vel_xy_exp: 0.5230
Episode_Reward/track_ang_vel_z_exp: 0.2950
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0543
         Episode_Reward/dof_acc_l2: -0.1352
     Episode_Reward/action_rate_l2: -0.0549
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3612
Metrics/base_velocity/error_vel_xy: 0.4532
Metrics/base_velocity/error_vel_yaw: 0.3264
      Episode_Termination/time_out: 0.6229
  Episode_Termination/base_contact: 0.3771
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 2.56s
                      Time elapsed: 00:52:57
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1250/1400 [0m                     

                       Computation: 9587 steps/s (collection: 2.302s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.7523
                       Mean reward: 11.31
               Mean episode length: 829.12
Episode_Reward/track_lin_vel_xy_exp: 0.5963
Episode_Reward/track_ang_vel_z_exp: 0.3367
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.0515
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1367
     Episode_Reward/action_rate_l2: -0.0614
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0097
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3628
Metrics/base_velocity/error_vel_xy: 0.4902
Metrics/base_velocity/error_vel_yaw: 0.3464
      Episode_Termination/time_out: 0.6235
  Episode_Termination/base_contact: 0.3765
--------------------------------------------------------------------------------
                   Total timesteps: 30744576
                    Iteration time: 2.56s
                      Time elapsed: 00:52:59
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1251/1400 [0m                     

                       Computation: 9910 steps/s (collection: 2.216s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.7541
                       Mean reward: 11.19
               Mean episode length: 792.57
Episode_Reward/track_lin_vel_xy_exp: 0.4798
Episode_Reward/track_ang_vel_z_exp: 0.2649
       Episode_Reward/lin_vel_z_l2: -0.0366
      Episode_Reward/ang_vel_xy_l2: -0.0418
     Episode_Reward/dof_torques_l2: -0.0456
         Episode_Reward/dof_acc_l2: -0.1114
     Episode_Reward/action_rate_l2: -0.0480
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3702
Metrics/base_velocity/error_vel_xy: 0.3626
Metrics/base_velocity/error_vel_yaw: 0.2692
      Episode_Termination/time_out: 0.6214
  Episode_Termination/base_contact: 0.3786
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.48s
                      Time elapsed: 00:53:02
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1252/1400 [0m                     

                       Computation: 9484 steps/s (collection: 2.329s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 5.7541
                       Mean reward: 10.78
               Mean episode length: 772.35
Episode_Reward/track_lin_vel_xy_exp: 0.5891
Episode_Reward/track_ang_vel_z_exp: 0.3238
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0475
     Episode_Reward/dof_torques_l2: -0.0603
         Episode_Reward/dof_acc_l2: -0.1478
     Episode_Reward/action_rate_l2: -0.0593
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3693
Metrics/base_velocity/error_vel_xy: 0.4280
Metrics/base_velocity/error_vel_yaw: 0.3161
      Episode_Termination/time_out: 0.6186
  Episode_Termination/base_contact: 0.3814
--------------------------------------------------------------------------------
                   Total timesteps: 30793728
                    Iteration time: 2.59s
                      Time elapsed: 00:53:04
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1253/1400 [0m                     

                       Computation: 9438 steps/s (collection: 2.347s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7595
                       Mean reward: 9.95
               Mean episode length: 743.30
Episode_Reward/track_lin_vel_xy_exp: 0.5683
Episode_Reward/track_ang_vel_z_exp: 0.3125
       Episode_Reward/lin_vel_z_l2: -0.0360
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0574
         Episode_Reward/dof_acc_l2: -0.1244
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0083
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3752
Metrics/base_velocity/error_vel_xy: 0.4445
Metrics/base_velocity/error_vel_yaw: 0.3314
      Episode_Termination/time_out: 0.6138
  Episode_Termination/base_contact: 0.3862
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 2.60s
                      Time elapsed: 00:53:07
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1254/1400 [0m                     

                       Computation: 9593 steps/s (collection: 2.307s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7820
                       Mean reward: 9.69
               Mean episode length: 727.78
Episode_Reward/track_lin_vel_xy_exp: 0.5482
Episode_Reward/track_ang_vel_z_exp: 0.3082
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0458
     Episode_Reward/dof_torques_l2: -0.0564
         Episode_Reward/dof_acc_l2: -0.1270
     Episode_Reward/action_rate_l2: -0.0563
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0108
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3694
Metrics/base_velocity/error_vel_xy: 0.4408
Metrics/base_velocity/error_vel_yaw: 0.3158
      Episode_Termination/time_out: 0.6135
  Episode_Termination/base_contact: 0.3865
--------------------------------------------------------------------------------
                   Total timesteps: 30842880
                    Iteration time: 2.56s
                      Time elapsed: 00:53:09
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1255/1400 [0m                     

                       Computation: 9554 steps/s (collection: 2.315s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.7918
                       Mean reward: 9.93
               Mean episode length: 739.83
Episode_Reward/track_lin_vel_xy_exp: 0.6123
Episode_Reward/track_ang_vel_z_exp: 0.3312
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0532
     Episode_Reward/dof_torques_l2: -0.0583
         Episode_Reward/dof_acc_l2: -0.1446
     Episode_Reward/action_rate_l2: -0.0599
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3708
Metrics/base_velocity/error_vel_xy: 0.4199
Metrics/base_velocity/error_vel_yaw: 0.3365
      Episode_Termination/time_out: 0.6072
  Episode_Termination/base_contact: 0.3928
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.57s
                      Time elapsed: 00:53:12
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1256/1400 [0m                     

                       Computation: 9530 steps/s (collection: 2.322s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 5.7951
                       Mean reward: 9.49
               Mean episode length: 718.56
Episode_Reward/track_lin_vel_xy_exp: 0.4256
Episode_Reward/track_ang_vel_z_exp: 0.2325
       Episode_Reward/lin_vel_z_l2: -0.0304
      Episode_Reward/ang_vel_xy_l2: -0.0392
     Episode_Reward/dof_torques_l2: -0.0455
         Episode_Reward/dof_acc_l2: -0.1057
     Episode_Reward/action_rate_l2: -0.0434
      Episode_Reward/feet_air_time: -0.0067
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3716
Metrics/base_velocity/error_vel_xy: 0.3359
Metrics/base_velocity/error_vel_yaw: 0.2608
      Episode_Termination/time_out: 0.6027
  Episode_Termination/base_contact: 0.3973
--------------------------------------------------------------------------------
                   Total timesteps: 30892032
                    Iteration time: 2.58s
                      Time elapsed: 00:53:14
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1257/1400 [0m                     

                       Computation: 9567 steps/s (collection: 2.310s, learning 0.259s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.7939
                       Mean reward: 10.02
               Mean episode length: 733.66
Episode_Reward/track_lin_vel_xy_exp: 0.4992
Episode_Reward/track_ang_vel_z_exp: 0.2761
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.0460
     Episode_Reward/dof_torques_l2: -0.0499
         Episode_Reward/dof_acc_l2: -0.1182
     Episode_Reward/action_rate_l2: -0.0501
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3794
Metrics/base_velocity/error_vel_xy: 0.3832
Metrics/base_velocity/error_vel_yaw: 0.2848
      Episode_Termination/time_out: 0.6031
  Episode_Termination/base_contact: 0.3969
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 2.57s
                      Time elapsed: 00:53:17
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1258/1400 [0m                     

                       Computation: 9528 steps/s (collection: 2.314s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7807
                       Mean reward: 9.88
               Mean episode length: 737.16
Episode_Reward/track_lin_vel_xy_exp: 0.5362
Episode_Reward/track_ang_vel_z_exp: 0.3072
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0586
         Episode_Reward/dof_acc_l2: -0.1341
     Episode_Reward/action_rate_l2: -0.0578
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3767
Metrics/base_velocity/error_vel_xy: 0.4626
Metrics/base_velocity/error_vel_yaw: 0.3233
      Episode_Termination/time_out: 0.6042
  Episode_Termination/base_contact: 0.3958
--------------------------------------------------------------------------------
                   Total timesteps: 30941184
                    Iteration time: 2.58s
                      Time elapsed: 00:53:20
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1259/1400 [0m                     

                       Computation: 9704 steps/s (collection: 2.268s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.7791
                       Mean reward: 9.99
               Mean episode length: 763.34
Episode_Reward/track_lin_vel_xy_exp: 0.5288
Episode_Reward/track_ang_vel_z_exp: 0.2906
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.0534
         Episode_Reward/dof_acc_l2: -0.1209
     Episode_Reward/action_rate_l2: -0.0537
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0109
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3877
Metrics/base_velocity/error_vel_xy: 0.4167
Metrics/base_velocity/error_vel_yaw: 0.3079
      Episode_Termination/time_out: 0.5986
  Episode_Termination/base_contact: 0.4014
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.53s
                      Time elapsed: 00:53:22
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1260/1400 [0m                     

                       Computation: 9690 steps/s (collection: 2.272s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7766
                       Mean reward: 10.26
               Mean episode length: 768.02
Episode_Reward/track_lin_vel_xy_exp: 0.5818
Episode_Reward/track_ang_vel_z_exp: 0.3163
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0496
     Episode_Reward/dof_torques_l2: -0.0608
         Episode_Reward/dof_acc_l2: -0.1310
     Episode_Reward/action_rate_l2: -0.0601
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3951
Metrics/base_velocity/error_vel_xy: 0.4509
Metrics/base_velocity/error_vel_yaw: 0.3501
      Episode_Termination/time_out: 0.5988
  Episode_Termination/base_contact: 0.4012
--------------------------------------------------------------------------------
                   Total timesteps: 30990336
                    Iteration time: 2.54s
                      Time elapsed: 00:53:25
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1261/1400 [0m                     

                       Computation: 9480 steps/s (collection: 2.329s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7836
                       Mean reward: 9.70
               Mean episode length: 722.84
Episode_Reward/track_lin_vel_xy_exp: 0.4808
Episode_Reward/track_ang_vel_z_exp: 0.2666
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.1145
     Episode_Reward/action_rate_l2: -0.0491
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3913
Metrics/base_velocity/error_vel_xy: 0.3757
Metrics/base_velocity/error_vel_yaw: 0.2854
      Episode_Termination/time_out: 0.5982
  Episode_Termination/base_contact: 0.4018
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 2.59s
                      Time elapsed: 00:53:27
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1262/1400 [0m                     

                       Computation: 9607 steps/s (collection: 2.294s, learning 0.264s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 5.7903
                       Mean reward: 9.59
               Mean episode length: 717.57
Episode_Reward/track_lin_vel_xy_exp: 0.5437
Episode_Reward/track_ang_vel_z_exp: 0.2951
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0477
     Episode_Reward/dof_torques_l2: -0.0559
         Episode_Reward/dof_acc_l2: -0.1372
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0100
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3948
Metrics/base_velocity/error_vel_xy: 0.4107
Metrics/base_velocity/error_vel_yaw: 0.3246
      Episode_Termination/time_out: 0.5974
  Episode_Termination/base_contact: 0.4026
--------------------------------------------------------------------------------
                   Total timesteps: 31039488
                    Iteration time: 2.56s
                      Time elapsed: 00:53:30
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1263/1400 [0m                     

                       Computation: 9889 steps/s (collection: 2.229s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.7871
                       Mean reward: 9.89
               Mean episode length: 738.37
Episode_Reward/track_lin_vel_xy_exp: 0.5937
Episode_Reward/track_ang_vel_z_exp: 0.3499
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0510
     Episode_Reward/dof_torques_l2: -0.0664
         Episode_Reward/dof_acc_l2: -0.1296
     Episode_Reward/action_rate_l2: -0.0642
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0315
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4031
Metrics/base_velocity/error_vel_xy: 0.5627
Metrics/base_velocity/error_vel_yaw: 0.3840
      Episode_Termination/time_out: 0.6001
  Episode_Termination/base_contact: 0.3999
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.49s
                      Time elapsed: 00:53:32
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1264/1400 [0m                     

                       Computation: 9693 steps/s (collection: 2.270s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7733
                       Mean reward: 10.43
               Mean episode length: 775.20
Episode_Reward/track_lin_vel_xy_exp: 0.5863
Episode_Reward/track_ang_vel_z_exp: 0.3229
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.0510
     Episode_Reward/dof_torques_l2: -0.0585
         Episode_Reward/dof_acc_l2: -0.1412
     Episode_Reward/action_rate_l2: -0.0595
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0199
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3990
Metrics/base_velocity/error_vel_xy: 0.4585
Metrics/base_velocity/error_vel_yaw: 0.3500
      Episode_Termination/time_out: 0.5945
  Episode_Termination/base_contact: 0.4055
--------------------------------------------------------------------------------
                   Total timesteps: 31088640
                    Iteration time: 2.54s
                      Time elapsed: 00:53:35
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1265/1400 [0m                     

                       Computation: 9879 steps/s (collection: 2.226s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.7670
                       Mean reward: 10.61
               Mean episode length: 778.70
Episode_Reward/track_lin_vel_xy_exp: 0.4779
Episode_Reward/track_ang_vel_z_exp: 0.2619
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0425
     Episode_Reward/dof_torques_l2: -0.0472
         Episode_Reward/dof_acc_l2: -0.1158
     Episode_Reward/action_rate_l2: -0.0481
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4029
Metrics/base_velocity/error_vel_xy: 0.3605
Metrics/base_velocity/error_vel_yaw: 0.2802
      Episode_Termination/time_out: 0.5926
  Episode_Termination/base_contact: 0.4074
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 2.49s
                      Time elapsed: 00:53:37
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1266/1400 [0m                     

                       Computation: 9766 steps/s (collection: 2.259s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7552
                       Mean reward: 9.46
               Mean episode length: 688.73
Episode_Reward/track_lin_vel_xy_exp: 0.4410
Episode_Reward/track_ang_vel_z_exp: 0.2329
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0375
     Episode_Reward/dof_torques_l2: -0.0359
         Episode_Reward/dof_acc_l2: -0.0899
     Episode_Reward/action_rate_l2: -0.0407
      Episode_Reward/feet_air_time: -0.0063
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4074
Metrics/base_velocity/error_vel_xy: 0.2741
Metrics/base_velocity/error_vel_yaw: 0.2231
      Episode_Termination/time_out: 0.5865
  Episode_Termination/base_contact: 0.4135
--------------------------------------------------------------------------------
                   Total timesteps: 31137792
                    Iteration time: 2.52s
                      Time elapsed: 00:53:40
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1267/1400 [0m                     

                       Computation: 9822 steps/s (collection: 2.239s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7508
                       Mean reward: 9.72
               Mean episode length: 707.73
Episode_Reward/track_lin_vel_xy_exp: 0.5675
Episode_Reward/track_ang_vel_z_exp: 0.3093
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0598
         Episode_Reward/dof_acc_l2: -0.1421
     Episode_Reward/action_rate_l2: -0.0586
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4093
Metrics/base_velocity/error_vel_xy: 0.4400
Metrics/base_velocity/error_vel_yaw: 0.3383
      Episode_Termination/time_out: 0.5849
  Episode_Termination/base_contact: 0.4151
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.50s
                      Time elapsed: 00:53:42
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1268/1400 [0m                     

                       Computation: 9798 steps/s (collection: 2.252s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 5.7437
                       Mean reward: 9.46
               Mean episode length: 699.38
Episode_Reward/track_lin_vel_xy_exp: 0.5577
Episode_Reward/track_ang_vel_z_exp: 0.3016
       Episode_Reward/lin_vel_z_l2: -0.0359
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.1334
     Episode_Reward/action_rate_l2: -0.0576
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4139
Metrics/base_velocity/error_vel_xy: 0.4312
Metrics/base_velocity/error_vel_yaw: 0.3473
      Episode_Termination/time_out: 0.5902
  Episode_Termination/base_contact: 0.4098
--------------------------------------------------------------------------------
                   Total timesteps: 31186944
                    Iteration time: 2.51s
                      Time elapsed: 00:53:45
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1269/1400 [0m                     

                       Computation: 9845 steps/s (collection: 2.239s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7434
                       Mean reward: 9.78
               Mean episode length: 733.28
Episode_Reward/track_lin_vel_xy_exp: 0.5111
Episode_Reward/track_ang_vel_z_exp: 0.2883
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0436
     Episode_Reward/dof_torques_l2: -0.0526
         Episode_Reward/dof_acc_l2: -0.1218
     Episode_Reward/action_rate_l2: -0.0532
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4159
Metrics/base_velocity/error_vel_xy: 0.4179
Metrics/base_velocity/error_vel_yaw: 0.2992
      Episode_Termination/time_out: 0.5878
  Episode_Termination/base_contact: 0.4122
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 2.50s
                      Time elapsed: 00:53:47
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1270/1400 [0m                     

                       Computation: 9875 steps/s (collection: 2.232s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7417
                       Mean reward: 10.43
               Mean episode length: 752.72
Episode_Reward/track_lin_vel_xy_exp: 0.4711
Episode_Reward/track_ang_vel_z_exp: 0.2669
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0396
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.1111
     Episode_Reward/action_rate_l2: -0.0494
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4115
Metrics/base_velocity/error_vel_xy: 0.3867
Metrics/base_velocity/error_vel_yaw: 0.2738
      Episode_Termination/time_out: 0.5852
  Episode_Termination/base_contact: 0.4148
--------------------------------------------------------------------------------
                   Total timesteps: 31236096
                    Iteration time: 2.49s
                      Time elapsed: 00:53:50
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1271/1400 [0m                     

                       Computation: 9777 steps/s (collection: 2.259s, learning 0.254s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0176
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.7400
                       Mean reward: 10.66
               Mean episode length: 756.64
Episode_Reward/track_lin_vel_xy_exp: 0.5643
Episode_Reward/track_ang_vel_z_exp: 0.3040
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0527
         Episode_Reward/dof_acc_l2: -0.1257
     Episode_Reward/action_rate_l2: -0.0553
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4017
Metrics/base_velocity/error_vel_xy: 0.3819
Metrics/base_velocity/error_vel_yaw: 0.3018
      Episode_Termination/time_out: 0.5819
  Episode_Termination/base_contact: 0.4181
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.51s
                      Time elapsed: 00:53:52
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1272/1400 [0m                     

                       Computation: 9882 steps/s (collection: 2.229s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7511
                       Mean reward: 10.98
               Mean episode length: 776.68
Episode_Reward/track_lin_vel_xy_exp: 0.5996
Episode_Reward/track_ang_vel_z_exp: 0.3336
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.0510
     Episode_Reward/dof_torques_l2: -0.0586
         Episode_Reward/dof_acc_l2: -0.1451
     Episode_Reward/action_rate_l2: -0.0612
      Episode_Reward/feet_air_time: -0.0100
 Episode_Reward/undesired_contacts: -0.0064
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3905
Metrics/base_velocity/error_vel_xy: 0.4657
Metrics/base_velocity/error_vel_yaw: 0.3318
      Episode_Termination/time_out: 0.5823
  Episode_Termination/base_contact: 0.4177
--------------------------------------------------------------------------------
                   Total timesteps: 31285248
                    Iteration time: 2.49s
                      Time elapsed: 00:53:55
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1273/1400 [0m                     

                       Computation: 9854 steps/s (collection: 2.238s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.7698
                       Mean reward: 10.80
               Mean episode length: 783.67
Episode_Reward/track_lin_vel_xy_exp: 0.4899
Episode_Reward/track_ang_vel_z_exp: 0.2832
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0472
     Episode_Reward/dof_torques_l2: -0.0534
         Episode_Reward/dof_acc_l2: -0.1231
     Episode_Reward/action_rate_l2: -0.0532
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3924
Metrics/base_velocity/error_vel_xy: 0.4540
Metrics/base_velocity/error_vel_yaw: 0.3118
      Episode_Termination/time_out: 0.5798
  Episode_Termination/base_contact: 0.4202
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 2.49s
                      Time elapsed: 00:53:57
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1274/1400 [0m                     

                       Computation: 9696 steps/s (collection: 2.277s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.7765
                       Mean reward: 10.76
               Mean episode length: 777.37
Episode_Reward/track_lin_vel_xy_exp: 0.5769
Episode_Reward/track_ang_vel_z_exp: 0.3070
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.0438
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.1261
     Episode_Reward/action_rate_l2: -0.0548
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3977
Metrics/base_velocity/error_vel_xy: 0.3622
Metrics/base_velocity/error_vel_yaw: 0.2872
      Episode_Termination/time_out: 0.5787
  Episode_Termination/base_contact: 0.4213
--------------------------------------------------------------------------------
                   Total timesteps: 31334400
                    Iteration time: 2.53s
                      Time elapsed: 00:54:00
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1275/1400 [0m                     

                       Computation: 9836 steps/s (collection: 2.244s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 5.7778
                       Mean reward: 10.85
               Mean episode length: 790.07
Episode_Reward/track_lin_vel_xy_exp: 0.5601
Episode_Reward/track_ang_vel_z_exp: 0.3054
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0493
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1372
     Episode_Reward/action_rate_l2: -0.0573
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3966
Metrics/base_velocity/error_vel_xy: 0.4381
Metrics/base_velocity/error_vel_yaw: 0.3466
      Episode_Termination/time_out: 0.5824
  Episode_Termination/base_contact: 0.4176
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.50s
                      Time elapsed: 00:54:02
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1276/1400 [0m                     

                       Computation: 9741 steps/s (collection: 2.265s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 5.7804
                       Mean reward: 11.16
               Mean episode length: 793.37
Episode_Reward/track_lin_vel_xy_exp: 0.5415
Episode_Reward/track_ang_vel_z_exp: 0.3098
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0559
         Episode_Reward/dof_acc_l2: -0.1346
     Episode_Reward/action_rate_l2: -0.0572
      Episode_Reward/feet_air_time: -0.0080
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3936
Metrics/base_velocity/error_vel_xy: 0.4557
Metrics/base_velocity/error_vel_yaw: 0.3135
      Episode_Termination/time_out: 0.5837
  Episode_Termination/base_contact: 0.4163
--------------------------------------------------------------------------------
                   Total timesteps: 31383552
                    Iteration time: 2.52s
                      Time elapsed: 00:54:05
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1277/1400 [0m                     

                       Computation: 9829 steps/s (collection: 2.237s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.7833
                       Mean reward: 10.93
               Mean episode length: 790.28
Episode_Reward/track_lin_vel_xy_exp: 0.6276
Episode_Reward/track_ang_vel_z_exp: 0.3418
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.0541
     Episode_Reward/dof_torques_l2: -0.0642
         Episode_Reward/dof_acc_l2: -0.1521
     Episode_Reward/action_rate_l2: -0.0630
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3926
Metrics/base_velocity/error_vel_xy: 0.4472
Metrics/base_velocity/error_vel_yaw: 0.3390
      Episode_Termination/time_out: 0.5786
  Episode_Termination/base_contact: 0.4214
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 2.50s
                      Time elapsed: 00:54:07
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1278/1400 [0m                     

                       Computation: 9808 steps/s (collection: 2.249s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.7872
                       Mean reward: 11.17
               Mean episode length: 809.05
Episode_Reward/track_lin_vel_xy_exp: 0.6633
Episode_Reward/track_ang_vel_z_exp: 0.3558
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0610
         Episode_Reward/dof_acc_l2: -0.1361
     Episode_Reward/action_rate_l2: -0.0636
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3833
Metrics/base_velocity/error_vel_xy: 0.4308
Metrics/base_velocity/error_vel_yaw: 0.3460
      Episode_Termination/time_out: 0.5793
  Episode_Termination/base_contact: 0.4207
--------------------------------------------------------------------------------
                   Total timesteps: 31432704
                    Iteration time: 2.51s
                      Time elapsed: 00:54:10
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1279/1400 [0m                     

                       Computation: 9769 steps/s (collection: 2.261s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 5.7946
                       Mean reward: 10.93
               Mean episode length: 787.30
Episode_Reward/track_lin_vel_xy_exp: 0.5040
Episode_Reward/track_ang_vel_z_exp: 0.2831
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0415
     Episode_Reward/dof_torques_l2: -0.0494
         Episode_Reward/dof_acc_l2: -0.1153
     Episode_Reward/action_rate_l2: -0.0518
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3809
Metrics/base_velocity/error_vel_xy: 0.4029
Metrics/base_velocity/error_vel_yaw: 0.2975
      Episode_Termination/time_out: 0.5795
  Episode_Termination/base_contact: 0.4205
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.52s
                      Time elapsed: 00:54:12
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1280/1400 [0m                     

                       Computation: 9757 steps/s (collection: 2.263s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 5.7983
                       Mean reward: 11.22
               Mean episode length: 800.48
Episode_Reward/track_lin_vel_xy_exp: 0.5893
Episode_Reward/track_ang_vel_z_exp: 0.3228
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.0488
     Episode_Reward/dof_torques_l2: -0.0616
         Episode_Reward/dof_acc_l2: -0.1437
     Episode_Reward/action_rate_l2: -0.0617
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3787
Metrics/base_velocity/error_vel_xy: 0.4529
Metrics/base_velocity/error_vel_yaw: 0.3562
      Episode_Termination/time_out: 0.5794
  Episode_Termination/base_contact: 0.4206
--------------------------------------------------------------------------------
                   Total timesteps: 31481856
                    Iteration time: 2.52s
                      Time elapsed: 00:54:15
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1281/1400 [0m                     

                       Computation: 9803 steps/s (collection: 2.250s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.8019
                       Mean reward: 11.21
               Mean episode length: 802.89
Episode_Reward/track_lin_vel_xy_exp: 0.7184
Episode_Reward/track_ang_vel_z_exp: 0.3842
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.0554
     Episode_Reward/dof_torques_l2: -0.0619
         Episode_Reward/dof_acc_l2: -0.1483
     Episode_Reward/action_rate_l2: -0.0678
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3695
Metrics/base_velocity/error_vel_xy: 0.4436
Metrics/base_velocity/error_vel_yaw: 0.3504
      Episode_Termination/time_out: 0.5835
  Episode_Termination/base_contact: 0.4165
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 2.51s
                      Time elapsed: 00:54:17
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1282/1400 [0m                     

                       Computation: 9700 steps/s (collection: 2.279s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.8092
                       Mean reward: 11.40
               Mean episode length: 813.69
Episode_Reward/track_lin_vel_xy_exp: 0.5737
Episode_Reward/track_ang_vel_z_exp: 0.3099
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0505
         Episode_Reward/dof_acc_l2: -0.1210
     Episode_Reward/action_rate_l2: -0.0557
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3675
Metrics/base_velocity/error_vel_xy: 0.3964
Metrics/base_velocity/error_vel_yaw: 0.3079
      Episode_Termination/time_out: 0.5850
  Episode_Termination/base_contact: 0.4150
--------------------------------------------------------------------------------
                   Total timesteps: 31531008
                    Iteration time: 2.53s
                      Time elapsed: 00:54:20
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1283/1400 [0m                     

                       Computation: 9751 steps/s (collection: 2.263s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 5.8096
                       Mean reward: 11.25
               Mean episode length: 797.91
Episode_Reward/track_lin_vel_xy_exp: 0.6135
Episode_Reward/track_ang_vel_z_exp: 0.3351
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0478
     Episode_Reward/dof_torques_l2: -0.0567
         Episode_Reward/dof_acc_l2: -0.1355
     Episode_Reward/action_rate_l2: -0.0610
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3683
Metrics/base_velocity/error_vel_xy: 0.4336
Metrics/base_velocity/error_vel_yaw: 0.3254
      Episode_Termination/time_out: 0.5871
  Episode_Termination/base_contact: 0.4129
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.52s
                      Time elapsed: 00:54:23
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1284/1400 [0m                     

                       Computation: 9807 steps/s (collection: 2.245s, learning 0.261s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0205
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.8063
                       Mean reward: 11.25
               Mean episode length: 807.36
Episode_Reward/track_lin_vel_xy_exp: 0.5588
Episode_Reward/track_ang_vel_z_exp: 0.3058
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1338
     Episode_Reward/action_rate_l2: -0.0578
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0108
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3889
Metrics/base_velocity/error_vel_xy: 0.4382
Metrics/base_velocity/error_vel_yaw: 0.3384
      Episode_Termination/time_out: 0.5870
  Episode_Termination/base_contact: 0.4130
--------------------------------------------------------------------------------
                   Total timesteps: 31580160
                    Iteration time: 2.51s
                      Time elapsed: 00:54:25
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1285/1400 [0m                     

                       Computation: 9810 steps/s (collection: 2.250s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.8059
                       Mean reward: 11.35
               Mean episode length: 805.61
Episode_Reward/track_lin_vel_xy_exp: 0.6280
Episode_Reward/track_ang_vel_z_exp: 0.3420
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.1346
     Episode_Reward/action_rate_l2: -0.0629
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3969
Metrics/base_velocity/error_vel_xy: 0.4513
Metrics/base_velocity/error_vel_yaw: 0.3457
      Episode_Termination/time_out: 0.5877
  Episode_Termination/base_contact: 0.4123
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 2.51s
                      Time elapsed: 00:54:28
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1286/1400 [0m                     

                       Computation: 9618 steps/s (collection: 2.290s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.8103
                       Mean reward: 11.55
               Mean episode length: 809.83
Episode_Reward/track_lin_vel_xy_exp: 0.6559
Episode_Reward/track_ang_vel_z_exp: 0.3554
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0530
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.1316
     Episode_Reward/action_rate_l2: -0.0631
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3990
Metrics/base_velocity/error_vel_xy: 0.4465
Metrics/base_velocity/error_vel_yaw: 0.3399
      Episode_Termination/time_out: 0.5897
  Episode_Termination/base_contact: 0.4103
--------------------------------------------------------------------------------
                   Total timesteps: 31629312
                    Iteration time: 2.56s
                      Time elapsed: 00:54:30
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1287/1400 [0m                     

                       Computation: 9496 steps/s (collection: 2.324s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0202
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.8188
                       Mean reward: 11.23
               Mean episode length: 785.76
Episode_Reward/track_lin_vel_xy_exp: 0.5488
Episode_Reward/track_ang_vel_z_exp: 0.2970
       Episode_Reward/lin_vel_z_l2: -0.0382
      Episode_Reward/ang_vel_xy_l2: -0.0475
     Episode_Reward/dof_torques_l2: -0.0544
         Episode_Reward/dof_acc_l2: -0.1359
     Episode_Reward/action_rate_l2: -0.0569
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4111
Metrics/base_velocity/error_vel_xy: 0.4125
Metrics/base_velocity/error_vel_yaw: 0.3173
      Episode_Termination/time_out: 0.5912
  Episode_Termination/base_contact: 0.4088
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.59s
                      Time elapsed: 00:54:33
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1288/1400 [0m                     

                       Computation: 9492 steps/s (collection: 2.334s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.8166
                       Mean reward: 10.92
               Mean episode length: 786.96
Episode_Reward/track_lin_vel_xy_exp: 0.5639
Episode_Reward/track_ang_vel_z_exp: 0.3104
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0479
     Episode_Reward/dof_torques_l2: -0.0567
         Episode_Reward/dof_acc_l2: -0.1408
     Episode_Reward/action_rate_l2: -0.0581
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4175
Metrics/base_velocity/error_vel_xy: 0.4163
Metrics/base_velocity/error_vel_yaw: 0.3167
      Episode_Termination/time_out: 0.5941
  Episode_Termination/base_contact: 0.4059
--------------------------------------------------------------------------------
                   Total timesteps: 31678464
                    Iteration time: 2.59s
                      Time elapsed: 00:54:35
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1289/1400 [0m                     

                       Computation: 9438 steps/s (collection: 2.346s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0186
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.8242
                       Mean reward: 10.67
               Mean episode length: 787.16
Episode_Reward/track_lin_vel_xy_exp: 0.5918
Episode_Reward/track_ang_vel_z_exp: 0.3234
       Episode_Reward/lin_vel_z_l2: -0.0502
      Episode_Reward/ang_vel_xy_l2: -0.0521
     Episode_Reward/dof_torques_l2: -0.0578
         Episode_Reward/dof_acc_l2: -0.1398
     Episode_Reward/action_rate_l2: -0.0601
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0079
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4261
Metrics/base_velocity/error_vel_xy: 0.4514
Metrics/base_velocity/error_vel_yaw: 0.3455
      Episode_Termination/time_out: 0.5994
  Episode_Termination/base_contact: 0.4006
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 2.60s
                      Time elapsed: 00:54:38
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1290/1400 [0m                     

                       Computation: 9647 steps/s (collection: 2.285s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0193
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.8332
                       Mean reward: 10.90
               Mean episode length: 789.99
Episode_Reward/track_lin_vel_xy_exp: 0.5979
Episode_Reward/track_ang_vel_z_exp: 0.3324
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0575
         Episode_Reward/dof_acc_l2: -0.1243
     Episode_Reward/action_rate_l2: -0.0597
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0114
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4262
Metrics/base_velocity/error_vel_xy: 0.4471
Metrics/base_velocity/error_vel_yaw: 0.3250
      Episode_Termination/time_out: 0.5964
  Episode_Termination/base_contact: 0.4036
--------------------------------------------------------------------------------
                   Total timesteps: 31727616
                    Iteration time: 2.55s
                      Time elapsed: 00:54:40
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1291/1400 [0m                     

                       Computation: 9539 steps/s (collection: 2.312s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 5.8454
                       Mean reward: 10.76
               Mean episode length: 761.30
Episode_Reward/track_lin_vel_xy_exp: 0.5064
Episode_Reward/track_ang_vel_z_exp: 0.2849
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0440
     Episode_Reward/dof_torques_l2: -0.0498
         Episode_Reward/dof_acc_l2: -0.1215
     Episode_Reward/action_rate_l2: -0.0526
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0075
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4273
Metrics/base_velocity/error_vel_xy: 0.3933
Metrics/base_velocity/error_vel_yaw: 0.2861
      Episode_Termination/time_out: 0.5949
  Episode_Termination/base_contact: 0.4051
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.58s
                      Time elapsed: 00:54:43
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1292/1400 [0m                     

                       Computation: 9708 steps/s (collection: 2.275s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0183
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.8381
                       Mean reward: 9.69
               Mean episode length: 721.55
Episode_Reward/track_lin_vel_xy_exp: 0.5023
Episode_Reward/track_ang_vel_z_exp: 0.2799
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0556
         Episode_Reward/dof_acc_l2: -0.1309
     Episode_Reward/action_rate_l2: -0.0544
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0079
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4114
Metrics/base_velocity/error_vel_xy: 0.4250
Metrics/base_velocity/error_vel_yaw: 0.3111
      Episode_Termination/time_out: 0.5905
  Episode_Termination/base_contact: 0.4095
--------------------------------------------------------------------------------
                   Total timesteps: 31776768
                    Iteration time: 2.53s
                      Time elapsed: 00:54:46
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1293/1400 [0m                     

                       Computation: 9668 steps/s (collection: 2.277s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0191
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 5.8393
                       Mean reward: 10.02
               Mean episode length: 746.16
Episode_Reward/track_lin_vel_xy_exp: 0.5781
Episode_Reward/track_ang_vel_z_exp: 0.3101
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0473
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1425
     Episode_Reward/action_rate_l2: -0.0580
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4084
Metrics/base_velocity/error_vel_xy: 0.4116
Metrics/base_velocity/error_vel_yaw: 0.3153
      Episode_Termination/time_out: 0.5933
  Episode_Termination/base_contact: 0.4067
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 2.54s
                      Time elapsed: 00:54:48
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1294/1400 [0m                     

                       Computation: 9586 steps/s (collection: 2.308s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 5.8480
                       Mean reward: 9.60
               Mean episode length: 725.43
Episode_Reward/track_lin_vel_xy_exp: 0.4391
Episode_Reward/track_ang_vel_z_exp: 0.2436
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0388
     Episode_Reward/dof_torques_l2: -0.0475
         Episode_Reward/dof_acc_l2: -0.1091
     Episode_Reward/action_rate_l2: -0.0468
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4109
Metrics/base_velocity/error_vel_xy: 0.3653
Metrics/base_velocity/error_vel_yaw: 0.2731
      Episode_Termination/time_out: 0.5918
  Episode_Termination/base_contact: 0.4082
--------------------------------------------------------------------------------
                   Total timesteps: 31825920
                    Iteration time: 2.56s
                      Time elapsed: 00:54:51
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1295/1400 [0m                     

                       Computation: 9699 steps/s (collection: 2.273s, learning 0.260s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.8572
                       Mean reward: 10.45
               Mean episode length: 760.78
Episode_Reward/track_lin_vel_xy_exp: 0.6640
Episode_Reward/track_ang_vel_z_exp: 0.3703
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0538
     Episode_Reward/dof_torques_l2: -0.0645
         Episode_Reward/dof_acc_l2: -0.1478
     Episode_Reward/action_rate_l2: -0.0673
      Episode_Reward/feet_air_time: -0.0104
 Episode_Reward/undesired_contacts: -0.0064
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4062
Metrics/base_velocity/error_vel_xy: 0.5077
Metrics/base_velocity/error_vel_yaw: 0.3708
      Episode_Termination/time_out: 0.5905
  Episode_Termination/base_contact: 0.4095
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.53s
                      Time elapsed: 00:54:53
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1296/1400 [0m                     

                       Computation: 9521 steps/s (collection: 2.318s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.8695
                       Mean reward: 10.10
               Mean episode length: 742.46
Episode_Reward/track_lin_vel_xy_exp: 0.6334
Episode_Reward/track_ang_vel_z_exp: 0.3430
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0506
     Episode_Reward/dof_torques_l2: -0.0624
         Episode_Reward/dof_acc_l2: -0.1589
     Episode_Reward/action_rate_l2: -0.0641
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0118
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4115
Metrics/base_velocity/error_vel_xy: 0.4448
Metrics/base_velocity/error_vel_yaw: 0.3437
      Episode_Termination/time_out: 0.5914
  Episode_Termination/base_contact: 0.4086
--------------------------------------------------------------------------------
                   Total timesteps: 31875072
                    Iteration time: 2.58s
                      Time elapsed: 00:54:56
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1297/1400 [0m                     

                       Computation: 9581 steps/s (collection: 2.309s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.8737
                       Mean reward: 10.78
               Mean episode length: 782.03
Episode_Reward/track_lin_vel_xy_exp: 0.5797
Episode_Reward/track_ang_vel_z_exp: 0.3157
       Episode_Reward/lin_vel_z_l2: -0.0328
      Episode_Reward/ang_vel_xy_l2: -0.0433
     Episode_Reward/dof_torques_l2: -0.0542
         Episode_Reward/dof_acc_l2: -0.1179
     Episode_Reward/action_rate_l2: -0.0569
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4226
Metrics/base_velocity/error_vel_xy: 0.3916
Metrics/base_velocity/error_vel_yaw: 0.2987
      Episode_Termination/time_out: 0.5927
  Episode_Termination/base_contact: 0.4073
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 2.56s
                      Time elapsed: 00:54:58
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1298/1400 [0m                     

                       Computation: 9497 steps/s (collection: 2.331s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.8808
                       Mean reward: 10.15
               Mean episode length: 733.39
Episode_Reward/track_lin_vel_xy_exp: 0.5017
Episode_Reward/track_ang_vel_z_exp: 0.2715
       Episode_Reward/lin_vel_z_l2: -0.0353
      Episode_Reward/ang_vel_xy_l2: -0.0434
     Episode_Reward/dof_torques_l2: -0.0510
         Episode_Reward/dof_acc_l2: -0.1197
     Episode_Reward/action_rate_l2: -0.0521
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4293
Metrics/base_velocity/error_vel_xy: 0.3939
Metrics/base_velocity/error_vel_yaw: 0.3128
      Episode_Termination/time_out: 0.5942
  Episode_Termination/base_contact: 0.4058
--------------------------------------------------------------------------------
                   Total timesteps: 31924224
                    Iteration time: 2.59s
                      Time elapsed: 00:55:01
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1299/1400 [0m                     

                       Computation: 9539 steps/s (collection: 2.313s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 5.8888
                       Mean reward: 10.43
               Mean episode length: 747.27
Episode_Reward/track_lin_vel_xy_exp: 0.6047
Episode_Reward/track_ang_vel_z_exp: 0.3270
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.0553
         Episode_Reward/dof_acc_l2: -0.1267
     Episode_Reward/action_rate_l2: -0.0590
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0080
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4295
Metrics/base_velocity/error_vel_xy: 0.3912
Metrics/base_velocity/error_vel_yaw: 0.2970
      Episode_Termination/time_out: 0.5928
  Episode_Termination/base_contact: 0.4072
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.58s
                      Time elapsed: 00:55:03
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1300/1400 [0m                     

                       Computation: 9575 steps/s (collection: 2.309s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.8844
                       Mean reward: 10.05
               Mean episode length: 727.96
Episode_Reward/track_lin_vel_xy_exp: 0.5177
Episode_Reward/track_ang_vel_z_exp: 0.2861
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.0433
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.1154
     Episode_Reward/action_rate_l2: -0.0534
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0071
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4338
Metrics/base_velocity/error_vel_xy: 0.3948
Metrics/base_velocity/error_vel_yaw: 0.2995
      Episode_Termination/time_out: 0.5931
  Episode_Termination/base_contact: 0.4069
--------------------------------------------------------------------------------
                   Total timesteps: 31973376
                    Iteration time: 2.57s
                      Time elapsed: 00:55:06
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1301/1400 [0m                     

                       Computation: 9640 steps/s (collection: 2.293s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.8790
                       Mean reward: 10.56
               Mean episode length: 770.78
Episode_Reward/track_lin_vel_xy_exp: 0.5581
Episode_Reward/track_ang_vel_z_exp: 0.3200
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.0546
     Episode_Reward/dof_torques_l2: -0.0659
         Episode_Reward/dof_acc_l2: -0.1493
     Episode_Reward/action_rate_l2: -0.0625
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0074
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4361
Metrics/base_velocity/error_vel_xy: 0.5224
Metrics/base_velocity/error_vel_yaw: 0.3784
      Episode_Termination/time_out: 0.5940
  Episode_Termination/base_contact: 0.4062
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 2.55s
                      Time elapsed: 00:55:09
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1302/1400 [0m                     

                       Computation: 9612 steps/s (collection: 2.298s, learning 0.259s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 5.8732
                       Mean reward: 10.31
               Mean episode length: 768.38
Episode_Reward/track_lin_vel_xy_exp: 0.4936
Episode_Reward/track_ang_vel_z_exp: 0.2742
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.0525
         Episode_Reward/dof_acc_l2: -0.1212
     Episode_Reward/action_rate_l2: -0.0514
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0101
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4390
Metrics/base_velocity/error_vel_xy: 0.4130
Metrics/base_velocity/error_vel_yaw: 0.3138
      Episode_Termination/time_out: 0.5922
  Episode_Termination/base_contact: 0.4088
--------------------------------------------------------------------------------
                   Total timesteps: 32022528
                    Iteration time: 2.56s
                      Time elapsed: 00:55:11
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1303/1400 [0m                     

                       Computation: 9494 steps/s (collection: 2.334s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 5.8761
                       Mean reward: 9.41
               Mean episode length: 731.83
Episode_Reward/track_lin_vel_xy_exp: 0.4873
Episode_Reward/track_ang_vel_z_exp: 0.2702
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0449
     Episode_Reward/dof_torques_l2: -0.0497
         Episode_Reward/dof_acc_l2: -0.1207
     Episode_Reward/action_rate_l2: -0.0512
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0124
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4358
Metrics/base_velocity/error_vel_xy: 0.4012
Metrics/base_velocity/error_vel_yaw: 0.3017
      Episode_Termination/time_out: 0.5953
  Episode_Termination/base_contact: 0.4056
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.59s
                      Time elapsed: 00:55:14
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1304/1400 [0m                     

                       Computation: 9555 steps/s (collection: 2.310s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.8768
                       Mean reward: 9.24
               Mean episode length: 705.19
Episode_Reward/track_lin_vel_xy_exp: 0.4743
Episode_Reward/track_ang_vel_z_exp: 0.2675
       Episode_Reward/lin_vel_z_l2: -0.0334
      Episode_Reward/ang_vel_xy_l2: -0.0413
     Episode_Reward/dof_torques_l2: -0.0472
         Episode_Reward/dof_acc_l2: -0.1076
     Episode_Reward/action_rate_l2: -0.0487
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0106
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4378
Metrics/base_velocity/error_vel_xy: 0.3761
Metrics/base_velocity/error_vel_yaw: 0.2738
      Episode_Termination/time_out: 0.5933
  Episode_Termination/base_contact: 0.4077
--------------------------------------------------------------------------------
                   Total timesteps: 32071680
                    Iteration time: 2.57s
                      Time elapsed: 00:55:16
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1305/1400 [0m                     

                       Computation: 9700 steps/s (collection: 2.277s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 5.8704
                       Mean reward: 10.25
               Mean episode length: 745.71
Episode_Reward/track_lin_vel_xy_exp: 0.6528
Episode_Reward/track_ang_vel_z_exp: 0.3574
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.0484
     Episode_Reward/dof_torques_l2: -0.0596
         Episode_Reward/dof_acc_l2: -0.1401
     Episode_Reward/action_rate_l2: -0.0645
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4357
Metrics/base_velocity/error_vel_xy: 0.4389
Metrics/base_velocity/error_vel_yaw: 0.3201
      Episode_Termination/time_out: 0.5935
  Episode_Termination/base_contact: 0.4075
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 2.53s
                      Time elapsed: 00:55:19
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1306/1400 [0m                     

                       Computation: 9549 steps/s (collection: 2.317s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.8785
                       Mean reward: 10.36
               Mean episode length: 737.17
Episode_Reward/track_lin_vel_xy_exp: 0.5453
Episode_Reward/track_ang_vel_z_exp: 0.2992
       Episode_Reward/lin_vel_z_l2: -0.0359
      Episode_Reward/ang_vel_xy_l2: -0.0460
     Episode_Reward/dof_torques_l2: -0.0568
         Episode_Reward/dof_acc_l2: -0.1305
     Episode_Reward/action_rate_l2: -0.0569
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0084
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4338
Metrics/base_velocity/error_vel_xy: 0.4221
Metrics/base_velocity/error_vel_yaw: 0.3202
      Episode_Termination/time_out: 0.5956
  Episode_Termination/base_contact: 0.4054
--------------------------------------------------------------------------------
                   Total timesteps: 32120832
                    Iteration time: 2.57s
                      Time elapsed: 00:55:21
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1307/1400 [0m                     

                       Computation: 9737 steps/s (collection: 2.269s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.8835
                       Mean reward: 10.08
               Mean episode length: 730.84
Episode_Reward/track_lin_vel_xy_exp: 0.4415
Episode_Reward/track_ang_vel_z_exp: 0.2493
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0447
     Episode_Reward/dof_torques_l2: -0.0470
         Episode_Reward/dof_acc_l2: -0.1080
     Episode_Reward/action_rate_l2: -0.0483
      Episode_Reward/feet_air_time: -0.0073
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4382
Metrics/base_velocity/error_vel_xy: 0.4091
Metrics/base_velocity/error_vel_yaw: 0.3025
      Episode_Termination/time_out: 0.5986
  Episode_Termination/base_contact: 0.4024
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.52s
                      Time elapsed: 00:55:24
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1308/1400 [0m                     

                       Computation: 9616 steps/s (collection: 2.300s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.8692
                       Mean reward: 10.34
               Mean episode length: 749.63
Episode_Reward/track_lin_vel_xy_exp: 0.6733
Episode_Reward/track_ang_vel_z_exp: 0.3609
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0606
         Episode_Reward/dof_acc_l2: -0.1419
     Episode_Reward/action_rate_l2: -0.0643
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4506
Metrics/base_velocity/error_vel_xy: 0.4317
Metrics/base_velocity/error_vel_yaw: 0.3430
      Episode_Termination/time_out: 0.6010
  Episode_Termination/base_contact: 0.3999
--------------------------------------------------------------------------------
                   Total timesteps: 32169984
                    Iteration time: 2.56s
                      Time elapsed: 00:55:26
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1309/1400 [0m                     

                       Computation: 9645 steps/s (collection: 2.292s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 5.8614
                       Mean reward: 9.83
               Mean episode length: 728.55
Episode_Reward/track_lin_vel_xy_exp: 0.5401
Episode_Reward/track_ang_vel_z_exp: 0.2973
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.0456
     Episode_Reward/dof_torques_l2: -0.0484
         Episode_Reward/dof_acc_l2: -0.1115
     Episode_Reward/action_rate_l2: -0.0531
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4600
Metrics/base_velocity/error_vel_xy: 0.3863
Metrics/base_velocity/error_vel_yaw: 0.2949
      Episode_Termination/time_out: 0.5993
  Episode_Termination/base_contact: 0.4017
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 2.55s
                      Time elapsed: 00:55:29
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1310/1400 [0m                     

                       Computation: 9592 steps/s (collection: 2.310s, learning 0.252s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.8558
                       Mean reward: 10.58
               Mean episode length: 767.85
Episode_Reward/track_lin_vel_xy_exp: 0.5782
Episode_Reward/track_ang_vel_z_exp: 0.3130
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0474
     Episode_Reward/dof_torques_l2: -0.0534
         Episode_Reward/dof_acc_l2: -0.1233
     Episode_Reward/action_rate_l2: -0.0579
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0113
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4518
Metrics/base_velocity/error_vel_xy: 0.4050
Metrics/base_velocity/error_vel_yaw: 0.3168
      Episode_Termination/time_out: 0.5960
  Episode_Termination/base_contact: 0.4050
--------------------------------------------------------------------------------
                   Total timesteps: 32219136
                    Iteration time: 2.56s
                      Time elapsed: 00:55:32
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1311/1400 [0m                     

                       Computation: 9609 steps/s (collection: 2.301s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.8593
                       Mean reward: 10.70
               Mean episode length: 776.15
Episode_Reward/track_lin_vel_xy_exp: 0.6466
Episode_Reward/track_ang_vel_z_exp: 0.3450
       Episode_Reward/lin_vel_z_l2: -0.0514
      Episode_Reward/ang_vel_xy_l2: -0.0538
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.1343
     Episode_Reward/action_rate_l2: -0.0628
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0074
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4532
Metrics/base_velocity/error_vel_xy: 0.4425
Metrics/base_velocity/error_vel_yaw: 0.3600
      Episode_Termination/time_out: 0.5988
  Episode_Termination/base_contact: 0.4022
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.56s
                      Time elapsed: 00:55:34
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1312/1400 [0m                     

                       Computation: 9708 steps/s (collection: 2.274s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 5.8633
                       Mean reward: 10.74
               Mean episode length: 785.04
Episode_Reward/track_lin_vel_xy_exp: 0.5979
Episode_Reward/track_ang_vel_z_exp: 0.3286
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.0556
     Episode_Reward/dof_torques_l2: -0.0634
         Episode_Reward/dof_acc_l2: -0.1532
     Episode_Reward/action_rate_l2: -0.0627
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4559
Metrics/base_velocity/error_vel_xy: 0.5020
Metrics/base_velocity/error_vel_yaw: 0.3894
      Episode_Termination/time_out: 0.5975
  Episode_Termination/base_contact: 0.4035
--------------------------------------------------------------------------------
                   Total timesteps: 32268288
                    Iteration time: 2.53s
                      Time elapsed: 00:55:37
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1313/1400 [0m                     

                       Computation: 9536 steps/s (collection: 2.315s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.8637
                       Mean reward: 11.76
               Mean episode length: 827.10
Episode_Reward/track_lin_vel_xy_exp: 0.6439
Episode_Reward/track_ang_vel_z_exp: 0.3515
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.0498
     Episode_Reward/dof_torques_l2: -0.0582
         Episode_Reward/dof_acc_l2: -0.1411
     Episode_Reward/action_rate_l2: -0.0634
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4636
Metrics/base_velocity/error_vel_xy: 0.4346
Metrics/base_velocity/error_vel_yaw: 0.3329
      Episode_Termination/time_out: 0.5971
  Episode_Termination/base_contact: 0.4039
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 2.58s
                      Time elapsed: 00:55:39
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1314/1400 [0m                     

                       Computation: 9686 steps/s (collection: 2.282s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.8694
                       Mean reward: 10.75
               Mean episode length: 778.50
Episode_Reward/track_lin_vel_xy_exp: 0.5230
Episode_Reward/track_ang_vel_z_exp: 0.2931
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0467
     Episode_Reward/dof_torques_l2: -0.0564
         Episode_Reward/dof_acc_l2: -0.1246
     Episode_Reward/action_rate_l2: -0.0558
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0083
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4618
Metrics/base_velocity/error_vel_xy: 0.4556
Metrics/base_velocity/error_vel_yaw: 0.3356
      Episode_Termination/time_out: 0.5960
  Episode_Termination/base_contact: 0.4049
--------------------------------------------------------------------------------
                   Total timesteps: 32317440
                    Iteration time: 2.54s
                      Time elapsed: 00:55:42
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1315/1400 [0m                     

                       Computation: 9847 steps/s (collection: 2.232s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 5.8810
                       Mean reward: 10.62
               Mean episode length: 770.64
Episode_Reward/track_lin_vel_xy_exp: 0.5274
Episode_Reward/track_ang_vel_z_exp: 0.2872
       Episode_Reward/lin_vel_z_l2: -0.0334
      Episode_Reward/ang_vel_xy_l2: -0.0474
     Episode_Reward/dof_torques_l2: -0.0541
         Episode_Reward/dof_acc_l2: -0.1220
     Episode_Reward/action_rate_l2: -0.0539
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0090
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4667
Metrics/base_velocity/error_vel_xy: 0.4341
Metrics/base_velocity/error_vel_yaw: 0.3625
      Episode_Termination/time_out: 0.5974
  Episode_Termination/base_contact: 0.4036
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.50s
                      Time elapsed: 00:55:44
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1316/1400 [0m                     

                       Computation: 9853 steps/s (collection: 2.231s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.8746
                       Mean reward: 9.73
               Mean episode length: 711.47
Episode_Reward/track_lin_vel_xy_exp: 0.5370
Episode_Reward/track_ang_vel_z_exp: 0.2804
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0496
         Episode_Reward/dof_acc_l2: -0.1177
     Episode_Reward/action_rate_l2: -0.0518
      Episode_Reward/feet_air_time: -0.0074
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4685
Metrics/base_velocity/error_vel_xy: 0.3375
Metrics/base_velocity/error_vel_yaw: 0.2924
      Episode_Termination/time_out: 0.5952
  Episode_Termination/base_contact: 0.4058
--------------------------------------------------------------------------------
                   Total timesteps: 32366592
                    Iteration time: 2.49s
                      Time elapsed: 00:55:47
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1317/1400 [0m                     

                       Computation: 9858 steps/s (collection: 2.236s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.8841
                       Mean reward: 9.80
               Mean episode length: 720.10
Episode_Reward/track_lin_vel_xy_exp: 0.5745
Episode_Reward/track_ang_vel_z_exp: 0.3216
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0504
     Episode_Reward/dof_torques_l2: -0.0597
         Episode_Reward/dof_acc_l2: -0.1455
     Episode_Reward/action_rate_l2: -0.0596
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4725
Metrics/base_velocity/error_vel_xy: 0.4835
Metrics/base_velocity/error_vel_yaw: 0.3542
      Episode_Termination/time_out: 0.5931
  Episode_Termination/base_contact: 0.4079
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 2.49s
                      Time elapsed: 00:55:49
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1318/1400 [0m                     

                       Computation: 9874 steps/s (collection: 2.225s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.8856
                       Mean reward: 10.68
               Mean episode length: 761.96
Episode_Reward/track_lin_vel_xy_exp: 0.6195
Episode_Reward/track_ang_vel_z_exp: 0.3364
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.0512
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.1398
     Episode_Reward/action_rate_l2: -0.0613
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4847
Metrics/base_velocity/error_vel_xy: 0.4563
Metrics/base_velocity/error_vel_yaw: 0.3442
      Episode_Termination/time_out: 0.5951
  Episode_Termination/base_contact: 0.4058
--------------------------------------------------------------------------------
                   Total timesteps: 32415744
                    Iteration time: 2.49s
                      Time elapsed: 00:55:52
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1319/1400 [0m                     

                       Computation: 9835 steps/s (collection: 2.249s, learning 0.250s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 5.8803
                       Mean reward: 10.57
               Mean episode length: 762.79
Episode_Reward/track_lin_vel_xy_exp: 0.5186
Episode_Reward/track_ang_vel_z_exp: 0.2835
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0417
     Episode_Reward/dof_torques_l2: -0.0515
         Episode_Reward/dof_acc_l2: -0.1179
     Episode_Reward/action_rate_l2: -0.0520
      Episode_Reward/feet_air_time: -0.0072
 Episode_Reward/undesired_contacts: -0.0091
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4921
Metrics/base_velocity/error_vel_xy: 0.3681
Metrics/base_velocity/error_vel_yaw: 0.2831
      Episode_Termination/time_out: 0.5966
  Episode_Termination/base_contact: 0.4043
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.50s
                      Time elapsed: 00:55:54
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1320/1400 [0m                     

                       Computation: 9835 steps/s (collection: 2.235s, learning 0.263s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.8722
                       Mean reward: 10.34
               Mean episode length: 774.47
Episode_Reward/track_lin_vel_xy_exp: 0.5142
Episode_Reward/track_ang_vel_z_exp: 0.2987
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.0541
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1437
     Episode_Reward/action_rate_l2: -0.0576
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0114
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4941
Metrics/base_velocity/error_vel_xy: 0.5304
Metrics/base_velocity/error_vel_yaw: 0.3606
      Episode_Termination/time_out: 0.5955
  Episode_Termination/base_contact: 0.4054
--------------------------------------------------------------------------------
                   Total timesteps: 32464896
                    Iteration time: 2.50s
                      Time elapsed: 00:55:57
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1321/1400 [0m                     

                       Computation: 9857 steps/s (collection: 2.241s, learning 0.252s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 5.8592
                       Mean reward: 9.70
               Mean episode length: 751.63
Episode_Reward/track_lin_vel_xy_exp: 0.5160
Episode_Reward/track_ang_vel_z_exp: 0.3018
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.0517
     Episode_Reward/dof_torques_l2: -0.0569
         Episode_Reward/dof_acc_l2: -0.1393
     Episode_Reward/action_rate_l2: -0.0574
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0066
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4909
Metrics/base_velocity/error_vel_xy: 0.5146
Metrics/base_velocity/error_vel_yaw: 0.3467
      Episode_Termination/time_out: 0.5909
  Episode_Termination/base_contact: 0.4100
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 2.49s
                      Time elapsed: 00:55:59
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1322/1400 [0m                     

                       Computation: 9785 steps/s (collection: 2.254s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 5.8566
                       Mean reward: 9.87
               Mean episode length: 752.53
Episode_Reward/track_lin_vel_xy_exp: 0.5846
Episode_Reward/track_ang_vel_z_exp: 0.3119
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0459
     Episode_Reward/dof_torques_l2: -0.0576
         Episode_Reward/dof_acc_l2: -0.1352
     Episode_Reward/action_rate_l2: -0.0577
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4901
Metrics/base_velocity/error_vel_xy: 0.3905
Metrics/base_velocity/error_vel_yaw: 0.3156
      Episode_Termination/time_out: 0.5921
  Episode_Termination/base_contact: 0.4089
--------------------------------------------------------------------------------
                   Total timesteps: 32514048
                    Iteration time: 2.51s
                      Time elapsed: 00:56:02
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1323/1400 [0m                     

                       Computation: 9855 steps/s (collection: 2.233s, learning 0.261s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.8689
                       Mean reward: 11.04
               Mean episode length: 793.83
Episode_Reward/track_lin_vel_xy_exp: 0.6256
Episode_Reward/track_ang_vel_z_exp: 0.3337
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.0496
     Episode_Reward/dof_torques_l2: -0.0562
         Episode_Reward/dof_acc_l2: -0.1351
     Episode_Reward/action_rate_l2: -0.0601
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4950
Metrics/base_velocity/error_vel_xy: 0.4002
Metrics/base_velocity/error_vel_yaw: 0.3175
      Episode_Termination/time_out: 0.5957
  Episode_Termination/base_contact: 0.4053
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.49s
                      Time elapsed: 00:56:04
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1324/1400 [0m                     

                       Computation: 9932 steps/s (collection: 2.217s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.8593
                       Mean reward: 11.27
               Mean episode length: 798.76
Episode_Reward/track_lin_vel_xy_exp: 0.6230
Episode_Reward/track_ang_vel_z_exp: 0.3408
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0592
         Episode_Reward/dof_acc_l2: -0.1292
     Episode_Reward/action_rate_l2: -0.0618
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0103
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4949
Metrics/base_velocity/error_vel_xy: 0.4622
Metrics/base_velocity/error_vel_yaw: 0.3442
      Episode_Termination/time_out: 0.5956
  Episode_Termination/base_contact: 0.4054
--------------------------------------------------------------------------------
                   Total timesteps: 32563200
                    Iteration time: 2.47s
                      Time elapsed: 00:56:07
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1325/1400 [0m                     

                       Computation: 9778 steps/s (collection: 2.252s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 5.8454
                       Mean reward: 11.70
               Mean episode length: 830.12
Episode_Reward/track_lin_vel_xy_exp: 0.6349
Episode_Reward/track_ang_vel_z_exp: 0.3558
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.0524
     Episode_Reward/dof_torques_l2: -0.0626
         Episode_Reward/dof_acc_l2: -0.1497
     Episode_Reward/action_rate_l2: -0.0643
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4912
Metrics/base_velocity/error_vel_xy: 0.4896
Metrics/base_velocity/error_vel_yaw: 0.3520
      Episode_Termination/time_out: 0.5979
  Episode_Termination/base_contact: 0.4031
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 2.51s
                      Time elapsed: 00:56:09
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1326/1400 [0m                     

                       Computation: 9859 steps/s (collection: 2.235s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.8367
                       Mean reward: 12.06
               Mean episode length: 838.40
Episode_Reward/track_lin_vel_xy_exp: 0.6681
Episode_Reward/track_ang_vel_z_exp: 0.3616
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.0515
     Episode_Reward/dof_torques_l2: -0.0587
         Episode_Reward/dof_acc_l2: -0.1368
     Episode_Reward/action_rate_l2: -0.0641
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0078
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5012
Metrics/base_velocity/error_vel_xy: 0.4294
Metrics/base_velocity/error_vel_yaw: 0.3266
      Episode_Termination/time_out: 0.6018
  Episode_Termination/base_contact: 0.3991
--------------------------------------------------------------------------------
                   Total timesteps: 32612352
                    Iteration time: 2.49s
                      Time elapsed: 00:56:12
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1327/1400 [0m                     

                       Computation: 9791 steps/s (collection: 2.254s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 5.8375
                       Mean reward: 12.03
               Mean episode length: 842.70
Episode_Reward/track_lin_vel_xy_exp: 0.6444
Episode_Reward/track_ang_vel_z_exp: 0.3504
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0502
     Episode_Reward/dof_torques_l2: -0.0608
         Episode_Reward/dof_acc_l2: -0.1343
     Episode_Reward/action_rate_l2: -0.0633
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0067
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5150
Metrics/base_velocity/error_vel_xy: 0.4434
Metrics/base_velocity/error_vel_yaw: 0.3423
      Episode_Termination/time_out: 0.6036
  Episode_Termination/base_contact: 0.3974
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.51s
                      Time elapsed: 00:56:14
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1328/1400 [0m                     

                       Computation: 9830 steps/s (collection: 2.244s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.8181
                       Mean reward: 11.82
               Mean episode length: 847.70
Episode_Reward/track_lin_vel_xy_exp: 0.6246
Episode_Reward/track_ang_vel_z_exp: 0.3487
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.0526
     Episode_Reward/dof_torques_l2: -0.0619
         Episode_Reward/dof_acc_l2: -0.1469
     Episode_Reward/action_rate_l2: -0.0639
      Episode_Reward/feet_air_time: -0.0099
 Episode_Reward/undesired_contacts: -0.0107
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5197
Metrics/base_velocity/error_vel_xy: 0.4958
Metrics/base_velocity/error_vel_yaw: 0.3546
      Episode_Termination/time_out: 0.6041
  Episode_Termination/base_contact: 0.3969
--------------------------------------------------------------------------------
                   Total timesteps: 32661504
                    Iteration time: 2.50s
                      Time elapsed: 00:56:17
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1329/1400 [0m                     

                       Computation: 9829 steps/s (collection: 2.244s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 5.8043
                       Mean reward: 12.21
               Mean episode length: 864.60
Episode_Reward/track_lin_vel_xy_exp: 0.6886
Episode_Reward/track_ang_vel_z_exp: 0.3774
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.0526
     Episode_Reward/dof_torques_l2: -0.0624
         Episode_Reward/dof_acc_l2: -0.1448
     Episode_Reward/action_rate_l2: -0.0664
      Episode_Reward/feet_air_time: -0.0104
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5304
Metrics/base_velocity/error_vel_xy: 0.4854
Metrics/base_velocity/error_vel_yaw: 0.3563
      Episode_Termination/time_out: 0.6090
  Episode_Termination/base_contact: 0.3920
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 2.50s
                      Time elapsed: 00:56:19
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1330/1400 [0m                     

                       Computation: 9803 steps/s (collection: 2.248s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.8047
                       Mean reward: 12.12
               Mean episode length: 852.25
Episode_Reward/track_lin_vel_xy_exp: 0.6086
Episode_Reward/track_ang_vel_z_exp: 0.3410
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.0499
     Episode_Reward/dof_torques_l2: -0.0615
         Episode_Reward/dof_acc_l2: -0.1427
     Episode_Reward/action_rate_l2: -0.0622
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5303
Metrics/base_velocity/error_vel_xy: 0.4568
Metrics/base_velocity/error_vel_yaw: 0.3202
      Episode_Termination/time_out: 0.6174
  Episode_Termination/base_contact: 0.3836
--------------------------------------------------------------------------------
                   Total timesteps: 32710656
                    Iteration time: 2.51s
                      Time elapsed: 00:56:22
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1331/1400 [0m                     

                       Computation: 9751 steps/s (collection: 2.265s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.8208
                       Mean reward: 12.23
               Mean episode length: 849.91
Episode_Reward/track_lin_vel_xy_exp: 0.6678
Episode_Reward/track_ang_vel_z_exp: 0.3647
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.0517
     Episode_Reward/dof_torques_l2: -0.0604
         Episode_Reward/dof_acc_l2: -0.1437
     Episode_Reward/action_rate_l2: -0.0643
      Episode_Reward/feet_air_time: -0.0101
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5449
Metrics/base_velocity/error_vel_xy: 0.4470
Metrics/base_velocity/error_vel_yaw: 0.3356
      Episode_Termination/time_out: 0.6182
  Episode_Termination/base_contact: 0.3827
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.52s
                      Time elapsed: 00:56:24
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1332/1400 [0m                     

                       Computation: 9924 steps/s (collection: 2.213s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.8256
                       Mean reward: 11.61
               Mean episode length: 830.85
Episode_Reward/track_lin_vel_xy_exp: 0.6537
Episode_Reward/track_ang_vel_z_exp: 0.3613
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.0548
     Episode_Reward/dof_torques_l2: -0.0661
         Episode_Reward/dof_acc_l2: -0.1638
     Episode_Reward/action_rate_l2: -0.0672
      Episode_Reward/feet_air_time: -0.0100
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5571
Metrics/base_velocity/error_vel_xy: 0.4946
Metrics/base_velocity/error_vel_yaw: 0.3701
      Episode_Termination/time_out: 0.6172
  Episode_Termination/base_contact: 0.3838
--------------------------------------------------------------------------------
                   Total timesteps: 32759808
                    Iteration time: 2.48s
                      Time elapsed: 00:56:27
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1333/1400 [0m                     

                       Computation: 9753 steps/s (collection: 2.261s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.8240
                       Mean reward: 11.99
               Mean episode length: 832.55
Episode_Reward/track_lin_vel_xy_exp: 0.5645
Episode_Reward/track_ang_vel_z_exp: 0.3064
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.0506
     Episode_Reward/dof_torques_l2: -0.0578
         Episode_Reward/dof_acc_l2: -0.1340
     Episode_Reward/action_rate_l2: -0.0570
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0076
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5625
Metrics/base_velocity/error_vel_xy: 0.4272
Metrics/base_velocity/error_vel_yaw: 0.3455
      Episode_Termination/time_out: 0.6201
  Episode_Termination/base_contact: 0.3809
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 2.52s
                      Time elapsed: 00:56:29
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1334/1400 [0m                     

                       Computation: 9831 steps/s (collection: 2.247s, learning 0.252s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.8128
                       Mean reward: 11.44
               Mean episode length: 825.32
Episode_Reward/track_lin_vel_xy_exp: 0.5708
Episode_Reward/track_ang_vel_z_exp: 0.3106
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0481
     Episode_Reward/dof_torques_l2: -0.0531
         Episode_Reward/dof_acc_l2: -0.1270
     Episode_Reward/action_rate_l2: -0.0570
      Episode_Reward/feet_air_time: -0.0098
 Episode_Reward/undesired_contacts: -0.0100
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5625
Metrics/base_velocity/error_vel_xy: 0.4375
Metrics/base_velocity/error_vel_yaw: 0.3523
      Episode_Termination/time_out: 0.6242
  Episode_Termination/base_contact: 0.3768
--------------------------------------------------------------------------------
                   Total timesteps: 32808960
                    Iteration time: 2.50s
                      Time elapsed: 00:56:32
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1335/1400 [0m                     

                       Computation: 9752 steps/s (collection: 2.257s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.8069
                       Mean reward: 10.42
               Mean episode length: 799.94
Episode_Reward/track_lin_vel_xy_exp: 0.5175
Episode_Reward/track_ang_vel_z_exp: 0.2940
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0585
         Episode_Reward/dof_acc_l2: -0.1398
     Episode_Reward/action_rate_l2: -0.0565
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0098
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5633
Metrics/base_velocity/error_vel_xy: 0.5051
Metrics/base_velocity/error_vel_yaw: 0.3744
      Episode_Termination/time_out: 0.6215
  Episode_Termination/base_contact: 0.3795
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.52s
                      Time elapsed: 00:56:34
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1336/1400 [0m                     

                       Computation: 9769 steps/s (collection: 2.260s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.8071
                       Mean reward: 10.57
               Mean episode length: 801.39
Episode_Reward/track_lin_vel_xy_exp: 0.6321
Episode_Reward/track_ang_vel_z_exp: 0.3339
       Episode_Reward/lin_vel_z_l2: -0.0359
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0588
         Episode_Reward/dof_acc_l2: -0.1326
     Episode_Reward/action_rate_l2: -0.0611
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0101
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5698
Metrics/base_velocity/error_vel_xy: 0.4210
Metrics/base_velocity/error_vel_yaw: 0.3539
      Episode_Termination/time_out: 0.6259
  Episode_Termination/base_contact: 0.3751
--------------------------------------------------------------------------------
                   Total timesteps: 32858112
                    Iteration time: 2.52s
                      Time elapsed: 00:56:37
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1337/1400 [0m                     

                       Computation: 9778 steps/s (collection: 2.257s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.8139
                       Mean reward: 10.35
               Mean episode length: 787.99
Episode_Reward/track_lin_vel_xy_exp: 0.5571
Episode_Reward/track_ang_vel_z_exp: 0.3036
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0461
     Episode_Reward/dof_torques_l2: -0.0562
         Episode_Reward/dof_acc_l2: -0.1367
     Episode_Reward/action_rate_l2: -0.0571
      Episode_Reward/feet_air_time: -0.0084
 Episode_Reward/undesired_contacts: -0.0089
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5752
Metrics/base_velocity/error_vel_xy: 0.4364
Metrics/base_velocity/error_vel_yaw: 0.3337
      Episode_Termination/time_out: 0.6242
  Episode_Termination/base_contact: 0.3767
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 2.51s
                      Time elapsed: 00:56:39
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1338/1400 [0m                     

                       Computation: 9636 steps/s (collection: 2.287s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.8111
                       Mean reward: 10.91
               Mean episode length: 787.73
Episode_Reward/track_lin_vel_xy_exp: 0.6107
Episode_Reward/track_ang_vel_z_exp: 0.3257
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0461
     Episode_Reward/dof_torques_l2: -0.0574
         Episode_Reward/dof_acc_l2: -0.1293
     Episode_Reward/action_rate_l2: -0.0586
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5822
Metrics/base_velocity/error_vel_xy: 0.3784
Metrics/base_velocity/error_vel_yaw: 0.2986
      Episode_Termination/time_out: 0.6231
  Episode_Termination/base_contact: 0.3778
--------------------------------------------------------------------------------
                   Total timesteps: 32907264
                    Iteration time: 2.55s
                      Time elapsed: 00:56:42
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1339/1400 [0m                     

                       Computation: 9743 steps/s (collection: 2.266s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.7949
                       Mean reward: 10.99
               Mean episode length: 786.05
Episode_Reward/track_lin_vel_xy_exp: 0.5392
Episode_Reward/track_ang_vel_z_exp: 0.2970
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.0506
     Episode_Reward/dof_torques_l2: -0.0580
         Episode_Reward/dof_acc_l2: -0.1346
     Episode_Reward/action_rate_l2: -0.0562
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0095
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5882
Metrics/base_velocity/error_vel_xy: 0.4407
Metrics/base_velocity/error_vel_yaw: 0.3519
      Episode_Termination/time_out: 0.6240
  Episode_Termination/base_contact: 0.3770
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 2.52s
                      Time elapsed: 00:56:44
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1340/1400 [0m                     

                       Computation: 9667 steps/s (collection: 2.284s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.7761
                       Mean reward: 11.35
               Mean episode length: 790.08
Episode_Reward/track_lin_vel_xy_exp: 0.6209
Episode_Reward/track_ang_vel_z_exp: 0.3424
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.0504
     Episode_Reward/dof_torques_l2: -0.0612
         Episode_Reward/dof_acc_l2: -0.1379
     Episode_Reward/action_rate_l2: -0.0612
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5956
Metrics/base_velocity/error_vel_xy: 0.4422
Metrics/base_velocity/error_vel_yaw: 0.3235
      Episode_Termination/time_out: 0.6266
  Episode_Termination/base_contact: 0.3744
--------------------------------------------------------------------------------
                   Total timesteps: 32956416
                    Iteration time: 2.54s
                      Time elapsed: 00:56:47
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1341/1400 [0m                     

                       Computation: 9602 steps/s (collection: 2.305s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7657
                       Mean reward: 11.35
               Mean episode length: 814.48
Episode_Reward/track_lin_vel_xy_exp: 0.6193
Episode_Reward/track_ang_vel_z_exp: 0.3387
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.0498
     Episode_Reward/dof_torques_l2: -0.0606
         Episode_Reward/dof_acc_l2: -0.1545
     Episode_Reward/action_rate_l2: -0.0621
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6038
Metrics/base_velocity/error_vel_xy: 0.4524
Metrics/base_velocity/error_vel_yaw: 0.3442
      Episode_Termination/time_out: 0.6306
  Episode_Termination/base_contact: 0.3704
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 2.56s
                      Time elapsed: 00:56:50
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1342/1400 [0m                     

                       Computation: 9720 steps/s (collection: 2.273s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.7693
                       Mean reward: 11.35
               Mean episode length: 822.10
Episode_Reward/track_lin_vel_xy_exp: 0.5666
Episode_Reward/track_ang_vel_z_exp: 0.3105
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.0469
     Episode_Reward/dof_torques_l2: -0.0617
         Episode_Reward/dof_acc_l2: -0.1322
     Episode_Reward/action_rate_l2: -0.0586
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0099
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5999
Metrics/base_velocity/error_vel_xy: 0.4447
Metrics/base_velocity/error_vel_yaw: 0.3525
      Episode_Termination/time_out: 0.6302
  Episode_Termination/base_contact: 0.3707
--------------------------------------------------------------------------------
                   Total timesteps: 33005568
                    Iteration time: 2.53s
                      Time elapsed: 00:56:52
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1343/1400 [0m                     

                       Computation: 9923 steps/s (collection: 2.215s, learning 0.261s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.7904
                       Mean reward: 11.66
               Mean episode length: 839.91
Episode_Reward/track_lin_vel_xy_exp: 0.6086
Episode_Reward/track_ang_vel_z_exp: 0.3264
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0577
         Episode_Reward/dof_acc_l2: -0.1368
     Episode_Reward/action_rate_l2: -0.0594
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5989
Metrics/base_velocity/error_vel_xy: 0.4184
Metrics/base_velocity/error_vel_yaw: 0.3318
      Episode_Termination/time_out: 0.6314
  Episode_Termination/base_contact: 0.3691
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 2.48s
                      Time elapsed: 00:56:55
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1344/1400 [0m                     

                       Computation: 9760 steps/s (collection: 2.263s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 5.8027
                       Mean reward: 11.50
               Mean episode length: 821.68
Episode_Reward/track_lin_vel_xy_exp: 0.6123
Episode_Reward/track_ang_vel_z_exp: 0.3318
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0502
     Episode_Reward/dof_torques_l2: -0.0576
         Episode_Reward/dof_acc_l2: -0.1283
     Episode_Reward/action_rate_l2: -0.0607
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0078
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5995
Metrics/base_velocity/error_vel_xy: 0.4512
Metrics/base_velocity/error_vel_yaw: 0.3592
      Episode_Termination/time_out: 0.6339
  Episode_Termination/base_contact: 0.3661
--------------------------------------------------------------------------------
                   Total timesteps: 33054720
                    Iteration time: 2.52s
                      Time elapsed: 00:56:57
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1345/1400 [0m                     

                       Computation: 9742 steps/s (collection: 2.258s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 5.8002
                       Mean reward: 12.20
               Mean episode length: 852.28
Episode_Reward/track_lin_vel_xy_exp: 0.6545
Episode_Reward/track_ang_vel_z_exp: 0.3621
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.0551
     Episode_Reward/dof_torques_l2: -0.0635
         Episode_Reward/dof_acc_l2: -0.1509
     Episode_Reward/action_rate_l2: -0.0645
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5988
Metrics/base_velocity/error_vel_xy: 0.5023
Metrics/base_velocity/error_vel_yaw: 0.3584
      Episode_Termination/time_out: 0.6406
  Episode_Termination/base_contact: 0.3594
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 2.52s
                      Time elapsed: 00:57:00
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1346/1400 [0m                     

                       Computation: 9566 steps/s (collection: 2.313s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.7963
                       Mean reward: 11.25
               Mean episode length: 814.03
Episode_Reward/track_lin_vel_xy_exp: 0.5637
Episode_Reward/track_ang_vel_z_exp: 0.3064
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.0475
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.1315
     Episode_Reward/action_rate_l2: -0.0562
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0080
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6003
Metrics/base_velocity/error_vel_xy: 0.4327
Metrics/base_velocity/error_vel_yaw: 0.3314
      Episode_Termination/time_out: 0.6405
  Episode_Termination/base_contact: 0.3595
--------------------------------------------------------------------------------
                   Total timesteps: 33103872
                    Iteration time: 2.57s
                      Time elapsed: 00:57:02
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1347/1400 [0m                     

                       Computation: 9604 steps/s (collection: 2.303s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 5.7887
                       Mean reward: 10.85
               Mean episode length: 800.84
Episode_Reward/track_lin_vel_xy_exp: 0.5823
Episode_Reward/track_ang_vel_z_exp: 0.3190
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0474
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1281
     Episode_Reward/action_rate_l2: -0.0572
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6036
Metrics/base_velocity/error_vel_xy: 0.4269
Metrics/base_velocity/error_vel_yaw: 0.3213
      Episode_Termination/time_out: 0.6418
  Episode_Termination/base_contact: 0.3582
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 2.56s
                      Time elapsed: 00:57:05
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1348/1400 [0m                     

                       Computation: 9733 steps/s (collection: 2.266s, learning 0.259s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 5.7876
                       Mean reward: 10.04
               Mean episode length: 744.50
Episode_Reward/track_lin_vel_xy_exp: 0.4757
Episode_Reward/track_ang_vel_z_exp: 0.2651
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.0426
     Episode_Reward/dof_torques_l2: -0.0467
         Episode_Reward/dof_acc_l2: -0.1193
     Episode_Reward/action_rate_l2: -0.0476
      Episode_Reward/feet_air_time: -0.0079
 Episode_Reward/undesired_contacts: -0.0077
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6073
Metrics/base_velocity/error_vel_xy: 0.3662
Metrics/base_velocity/error_vel_yaw: 0.2663
      Episode_Termination/time_out: 0.6471
  Episode_Termination/base_contact: 0.3529
--------------------------------------------------------------------------------
                   Total timesteps: 33153024
                    Iteration time: 2.52s
                      Time elapsed: 00:57:07
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1349/1400 [0m                     

                       Computation: 9718 steps/s (collection: 2.273s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.7912
                       Mean reward: 10.81
               Mean episode length: 789.30
Episode_Reward/track_lin_vel_xy_exp: 0.5622
Episode_Reward/track_ang_vel_z_exp: 0.3117
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.0578
         Episode_Reward/dof_acc_l2: -0.1355
     Episode_Reward/action_rate_l2: -0.0572
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0113
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6097
Metrics/base_velocity/error_vel_xy: 0.4426
Metrics/base_velocity/error_vel_yaw: 0.3358
      Episode_Termination/time_out: 0.6475
  Episode_Termination/base_contact: 0.3525
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 2.53s
                      Time elapsed: 00:57:10
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1350/1400 [0m                     

                       Computation: 9767 steps/s (collection: 2.258s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 5.8000
                       Mean reward: 11.35
               Mean episode length: 819.13
Episode_Reward/track_lin_vel_xy_exp: 0.7564
Episode_Reward/track_ang_vel_z_exp: 0.3978
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.0556
     Episode_Reward/dof_torques_l2: -0.0706
         Episode_Reward/dof_acc_l2: -0.1702
     Episode_Reward/action_rate_l2: -0.0713
      Episode_Reward/feet_air_time: -0.0105
 Episode_Reward/undesired_contacts: -0.0075
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6048
Metrics/base_velocity/error_vel_xy: 0.4579
Metrics/base_velocity/error_vel_yaw: 0.3791
      Episode_Termination/time_out: 0.6537
  Episode_Termination/base_contact: 0.3463
--------------------------------------------------------------------------------
                   Total timesteps: 33202176
                    Iteration time: 2.52s
                      Time elapsed: 00:57:12
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1351/1400 [0m                     

                       Computation: 9818 steps/s (collection: 2.248s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7934
                       Mean reward: 11.35
               Mean episode length: 818.14
Episode_Reward/track_lin_vel_xy_exp: 0.5940
Episode_Reward/track_ang_vel_z_exp: 0.3166
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0485
     Episode_Reward/dof_torques_l2: -0.0562
         Episode_Reward/dof_acc_l2: -0.1356
     Episode_Reward/action_rate_l2: -0.0599
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0193
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5988
Metrics/base_velocity/error_vel_xy: 0.4341
Metrics/base_velocity/error_vel_yaw: 0.3529
      Episode_Termination/time_out: 0.6560
  Episode_Termination/base_contact: 0.3440
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 2.50s
                      Time elapsed: 00:57:15
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1352/1400 [0m                     

                       Computation: 9752 steps/s (collection: 2.262s, learning 0.258s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.7823
                       Mean reward: 12.13
               Mean episode length: 847.48
Episode_Reward/track_lin_vel_xy_exp: 0.6089
Episode_Reward/track_ang_vel_z_exp: 0.3342
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0546
         Episode_Reward/dof_acc_l2: -0.1459
     Episode_Reward/action_rate_l2: -0.0596
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6047
Metrics/base_velocity/error_vel_xy: 0.4376
Metrics/base_velocity/error_vel_yaw: 0.3300
      Episode_Termination/time_out: 0.6581
  Episode_Termination/base_contact: 0.3419
--------------------------------------------------------------------------------
                   Total timesteps: 33251328
                    Iteration time: 2.52s
                      Time elapsed: 00:57:17
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1353/1400 [0m                     

                       Computation: 9706 steps/s (collection: 2.270s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 5.7771
                       Mean reward: 12.65
               Mean episode length: 863.87
Episode_Reward/track_lin_vel_xy_exp: 0.6656
Episode_Reward/track_ang_vel_z_exp: 0.3477
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.0520
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.1397
     Episode_Reward/action_rate_l2: -0.0611
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6113
Metrics/base_velocity/error_vel_xy: 0.4181
Metrics/base_velocity/error_vel_yaw: 0.3336
      Episode_Termination/time_out: 0.6593
  Episode_Termination/base_contact: 0.3407
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 2.53s
                      Time elapsed: 00:57:20
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1354/1400 [0m                     

                       Computation: 9657 steps/s (collection: 2.290s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.7771
                       Mean reward: 12.35
               Mean episode length: 839.79
Episode_Reward/track_lin_vel_xy_exp: 0.5928
Episode_Reward/track_ang_vel_z_exp: 0.3185
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1334
     Episode_Reward/action_rate_l2: -0.0578
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6012
Metrics/base_velocity/error_vel_xy: 0.4168
Metrics/base_velocity/error_vel_yaw: 0.3209
      Episode_Termination/time_out: 0.6633
  Episode_Termination/base_contact: 0.3367
--------------------------------------------------------------------------------
                   Total timesteps: 33300480
                    Iteration time: 2.54s
                      Time elapsed: 00:57:22
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1355/1400 [0m                     

                       Computation: 9629 steps/s (collection: 2.301s, learning 0.251s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 5.7767
                       Mean reward: 11.87
               Mean episode length: 805.70
Episode_Reward/track_lin_vel_xy_exp: 0.5434
Episode_Reward/track_ang_vel_z_exp: 0.2933
       Episode_Reward/lin_vel_z_l2: -0.0367
      Episode_Reward/ang_vel_xy_l2: -0.0429
     Episode_Reward/dof_torques_l2: -0.0527
         Episode_Reward/dof_acc_l2: -0.1223
     Episode_Reward/action_rate_l2: -0.0529
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5993
Metrics/base_velocity/error_vel_xy: 0.3838
Metrics/base_velocity/error_vel_yaw: 0.2960
      Episode_Termination/time_out: 0.6629
  Episode_Termination/base_contact: 0.3371
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 2.55s
                      Time elapsed: 00:57:25
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1356/1400 [0m                     

                       Computation: 9731 steps/s (collection: 2.261s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 5.7760
                       Mean reward: 11.52
               Mean episode length: 810.99
Episode_Reward/track_lin_vel_xy_exp: 0.6591
Episode_Reward/track_ang_vel_z_exp: 0.3638
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.0538
     Episode_Reward/dof_torques_l2: -0.0624
         Episode_Reward/dof_acc_l2: -0.1466
     Episode_Reward/action_rate_l2: -0.0645
      Episode_Reward/feet_air_time: -0.0104
 Episode_Reward/undesired_contacts: -0.0105
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5946
Metrics/base_velocity/error_vel_xy: 0.4833
Metrics/base_velocity/error_vel_yaw: 0.3598
      Episode_Termination/time_out: 0.6678
  Episode_Termination/base_contact: 0.3322
--------------------------------------------------------------------------------
                   Total timesteps: 33349632
                    Iteration time: 2.53s
                      Time elapsed: 00:57:27
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1357/1400 [0m                     

                       Computation: 9822 steps/s (collection: 2.247s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0127
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 5.7759
                       Mean reward: 11.70
               Mean episode length: 835.37
Episode_Reward/track_lin_vel_xy_exp: 0.7042
Episode_Reward/track_ang_vel_z_exp: 0.3808
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.0589
     Episode_Reward/dof_torques_l2: -0.0670
         Episode_Reward/dof_acc_l2: -0.1562
     Episode_Reward/action_rate_l2: -0.0697
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6014
Metrics/base_velocity/error_vel_xy: 0.5197
Metrics/base_velocity/error_vel_yaw: 0.4160
      Episode_Termination/time_out: 0.6734
  Episode_Termination/base_contact: 0.3266
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 2.50s
                      Time elapsed: 00:57:30
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1358/1400 [0m                     

                       Computation: 9678 steps/s (collection: 2.282s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7736
                       Mean reward: 11.97
               Mean episode length: 856.03
Episode_Reward/track_lin_vel_xy_exp: 0.6404
Episode_Reward/track_ang_vel_z_exp: 0.3453
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.0483
     Episode_Reward/dof_torques_l2: -0.0604
         Episode_Reward/dof_acc_l2: -0.1369
     Episode_Reward/action_rate_l2: -0.0607
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6104
Metrics/base_velocity/error_vel_xy: 0.4070
Metrics/base_velocity/error_vel_yaw: 0.3140
      Episode_Termination/time_out: 0.6790
  Episode_Termination/base_contact: 0.3210
--------------------------------------------------------------------------------
                   Total timesteps: 33398784
                    Iteration time: 2.54s
                      Time elapsed: 00:57:32
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1359/1400 [0m                     

                       Computation: 9588 steps/s (collection: 2.304s, learning 0.259s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 5.7678
                       Mean reward: 12.12
               Mean episode length: 862.34
Episode_Reward/track_lin_vel_xy_exp: 0.6193
Episode_Reward/track_ang_vel_z_exp: 0.3492
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.0539
     Episode_Reward/dof_torques_l2: -0.0647
         Episode_Reward/dof_acc_l2: -0.1497
     Episode_Reward/action_rate_l2: -0.0637
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6028
Metrics/base_velocity/error_vel_xy: 0.5108
Metrics/base_velocity/error_vel_yaw: 0.3689
      Episode_Termination/time_out: 0.6817
  Episode_Termination/base_contact: 0.3183
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 2.56s
                      Time elapsed: 00:57:35
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1360/1400 [0m                     

                       Computation: 9624 steps/s (collection: 2.297s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.7718
                       Mean reward: 12.06
               Mean episode length: 862.02
Episode_Reward/track_lin_vel_xy_exp: 0.6270
Episode_Reward/track_ang_vel_z_exp: 0.3367
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.0511
     Episode_Reward/dof_torques_l2: -0.0622
         Episode_Reward/dof_acc_l2: -0.1443
     Episode_Reward/action_rate_l2: -0.0619
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0080
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5861
Metrics/base_velocity/error_vel_xy: 0.4441
Metrics/base_velocity/error_vel_yaw: 0.3593
      Episode_Termination/time_out: 0.6829
  Episode_Termination/base_contact: 0.3171
--------------------------------------------------------------------------------
                   Total timesteps: 33447936
                    Iteration time: 2.55s
                      Time elapsed: 00:57:38
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1361/1400 [0m                     

                       Computation: 9564 steps/s (collection: 2.313s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.7684
                       Mean reward: 11.60
               Mean episode length: 835.23
Episode_Reward/track_lin_vel_xy_exp: 0.6419
Episode_Reward/track_ang_vel_z_exp: 0.3539
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.1421
     Episode_Reward/action_rate_l2: -0.0633
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5863
Metrics/base_velocity/error_vel_xy: 0.4631
Metrics/base_velocity/error_vel_yaw: 0.3381
      Episode_Termination/time_out: 0.6819
  Episode_Termination/base_contact: 0.3181
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 2.57s
                      Time elapsed: 00:57:40
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1362/1400 [0m                     

                       Computation: 9722 steps/s (collection: 2.271s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 5.7605
                       Mean reward: 11.23
               Mean episode length: 822.43
Episode_Reward/track_lin_vel_xy_exp: 0.5813
Episode_Reward/track_ang_vel_z_exp: 0.3199
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.0552
     Episode_Reward/dof_torques_l2: -0.0630
         Episode_Reward/dof_acc_l2: -0.1562
     Episode_Reward/action_rate_l2: -0.0618
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0066
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5983
Metrics/base_velocity/error_vel_xy: 0.4881
Metrics/base_velocity/error_vel_yaw: 0.3742
      Episode_Termination/time_out: 0.6857
  Episode_Termination/base_contact: 0.3143
--------------------------------------------------------------------------------
                   Total timesteps: 33497088
                    Iteration time: 2.53s
                      Time elapsed: 00:57:43
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1363/1400 [0m                     

                       Computation: 9674 steps/s (collection: 2.277s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.7661
                       Mean reward: 11.74
               Mean episode length: 850.27
Episode_Reward/track_lin_vel_xy_exp: 0.6531
Episode_Reward/track_ang_vel_z_exp: 0.3502
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0621
         Episode_Reward/dof_acc_l2: -0.1447
     Episode_Reward/action_rate_l2: -0.0631
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6074
Metrics/base_velocity/error_vel_xy: 0.4349
Metrics/base_velocity/error_vel_yaw: 0.3449
      Episode_Termination/time_out: 0.6882
  Episode_Termination/base_contact: 0.3118
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 2.54s
                      Time elapsed: 00:57:45
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1364/1400 [0m                     

                       Computation: 9721 steps/s (collection: 2.275s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 5.7777
                       Mean reward: 12.07
               Mean episode length: 848.92
Episode_Reward/track_lin_vel_xy_exp: 0.6161
Episode_Reward/track_ang_vel_z_exp: 0.3341
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0489
     Episode_Reward/dof_torques_l2: -0.0588
         Episode_Reward/dof_acc_l2: -0.1381
     Episode_Reward/action_rate_l2: -0.0591
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6122
Metrics/base_velocity/error_vel_xy: 0.4225
Metrics/base_velocity/error_vel_yaw: 0.3199
      Episode_Termination/time_out: 0.6882
  Episode_Termination/base_contact: 0.3118
--------------------------------------------------------------------------------
                   Total timesteps: 33546240
                    Iteration time: 2.53s
                      Time elapsed: 00:57:48
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1365/1400 [0m                     

                       Computation: 9725 steps/s (collection: 2.264s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 5.7770
                       Mean reward: 11.43
               Mean episode length: 826.79
Episode_Reward/track_lin_vel_xy_exp: 0.6111
Episode_Reward/track_ang_vel_z_exp: 0.3463
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0520
     Episode_Reward/dof_torques_l2: -0.0600
         Episode_Reward/dof_acc_l2: -0.1497
     Episode_Reward/action_rate_l2: -0.0627
      Episode_Reward/feet_air_time: -0.0104
 Episode_Reward/undesired_contacts: -0.0131
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6147
Metrics/base_velocity/error_vel_xy: 0.5240
Metrics/base_velocity/error_vel_yaw: 0.3688
      Episode_Termination/time_out: 0.6884
  Episode_Termination/base_contact: 0.3116
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 2.53s
                      Time elapsed: 00:57:50
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1366/1400 [0m                     

                       Computation: 9671 steps/s (collection: 2.285s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 5.7823
                       Mean reward: 11.26
               Mean episode length: 818.21
Episode_Reward/track_lin_vel_xy_exp: 0.5909
Episode_Reward/track_ang_vel_z_exp: 0.3325
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.0515
     Episode_Reward/dof_torques_l2: -0.0620
         Episode_Reward/dof_acc_l2: -0.1440
     Episode_Reward/action_rate_l2: -0.0610
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0079
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6302
Metrics/base_velocity/error_vel_xy: 0.4726
Metrics/base_velocity/error_vel_yaw: 0.3410
      Episode_Termination/time_out: 0.6906
  Episode_Termination/base_contact: 0.3094
--------------------------------------------------------------------------------
                   Total timesteps: 33595392
                    Iteration time: 2.54s
                      Time elapsed: 00:57:53
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1367/1400 [0m                     

                       Computation: 9673 steps/s (collection: 2.283s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 5.7813
                       Mean reward: 11.29
               Mean episode length: 816.20
Episode_Reward/track_lin_vel_xy_exp: 0.4722
Episode_Reward/track_ang_vel_z_exp: 0.2577
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.0444
     Episode_Reward/dof_torques_l2: -0.0451
         Episode_Reward/dof_acc_l2: -0.1190
     Episode_Reward/action_rate_l2: -0.0469
      Episode_Reward/feet_air_time: -0.0076
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6376
Metrics/base_velocity/error_vel_xy: 0.3636
Metrics/base_velocity/error_vel_yaw: 0.2772
      Episode_Termination/time_out: 0.6934
  Episode_Termination/base_contact: 0.3066
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 2.54s
                      Time elapsed: 00:57:55
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1368/1400 [0m                     

                       Computation: 9857 steps/s (collection: 2.236s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0127
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 5.7740
                       Mean reward: 11.39
               Mean episode length: 824.18
Episode_Reward/track_lin_vel_xy_exp: 0.7013
Episode_Reward/track_ang_vel_z_exp: 0.3703
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.0528
     Episode_Reward/dof_torques_l2: -0.0667
         Episode_Reward/dof_acc_l2: -0.1570
     Episode_Reward/action_rate_l2: -0.0669
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6303
Metrics/base_velocity/error_vel_xy: 0.4422
Metrics/base_velocity/error_vel_yaw: 0.3550
      Episode_Termination/time_out: 0.6928
  Episode_Termination/base_contact: 0.3072
--------------------------------------------------------------------------------
                   Total timesteps: 33644544
                    Iteration time: 2.49s
                      Time elapsed: 00:57:58
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1369/1400 [0m                     

                       Computation: 9934 steps/s (collection: 2.216s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.7740
                       Mean reward: 11.62
               Mean episode length: 830.95
Episode_Reward/track_lin_vel_xy_exp: 0.6181
Episode_Reward/track_ang_vel_z_exp: 0.3368
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.0547
     Episode_Reward/dof_torques_l2: -0.0634
         Episode_Reward/dof_acc_l2: -0.1503
     Episode_Reward/action_rate_l2: -0.0621
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0071
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6286
Metrics/base_velocity/error_vel_xy: 0.4777
Metrics/base_velocity/error_vel_yaw: 0.3788
      Episode_Termination/time_out: 0.6924
  Episode_Termination/base_contact: 0.3076
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 2.47s
                      Time elapsed: 00:58:00
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1370/1400 [0m                     

                       Computation: 9863 steps/s (collection: 2.234s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 5.7742
                       Mean reward: 12.50
               Mean episode length: 871.88
Episode_Reward/track_lin_vel_xy_exp: 0.7033
Episode_Reward/track_ang_vel_z_exp: 0.3796
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.0577
     Episode_Reward/dof_torques_l2: -0.0625
         Episode_Reward/dof_acc_l2: -0.1572
     Episode_Reward/action_rate_l2: -0.0672
      Episode_Reward/feet_air_time: -0.0105
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6233
Metrics/base_velocity/error_vel_xy: 0.4798
Metrics/base_velocity/error_vel_yaw: 0.3701
      Episode_Termination/time_out: 0.6930
  Episode_Termination/base_contact: 0.3070
--------------------------------------------------------------------------------
                   Total timesteps: 33693696
                    Iteration time: 2.49s
                      Time elapsed: 00:58:03
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1371/1400 [0m                     

                       Computation: 9929 steps/s (collection: 2.222s, learning 0.253s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 5.7769
                       Mean reward: 12.88
               Mean episode length: 892.44
Episode_Reward/track_lin_vel_xy_exp: 0.6974
Episode_Reward/track_ang_vel_z_exp: 0.3787
       Episode_Reward/lin_vel_z_l2: -0.0465
      Episode_Reward/ang_vel_xy_l2: -0.0562
     Episode_Reward/dof_torques_l2: -0.0653
         Episode_Reward/dof_acc_l2: -0.1576
     Episode_Reward/action_rate_l2: -0.0674
      Episode_Reward/feet_air_time: -0.0105
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6320
Metrics/base_velocity/error_vel_xy: 0.4869
Metrics/base_velocity/error_vel_yaw: 0.3731
      Episode_Termination/time_out: 0.6967
  Episode_Termination/base_contact: 0.3033
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.47s
                      Time elapsed: 00:58:05
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1372/1400 [0m                     

                       Computation: 9796 steps/s (collection: 2.246s, learning 0.263s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 5.7718
                       Mean reward: 13.15
               Mean episode length: 900.68
Episode_Reward/track_lin_vel_xy_exp: 0.6706
Episode_Reward/track_ang_vel_z_exp: 0.3660
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.0527
     Episode_Reward/dof_torques_l2: -0.0631
         Episode_Reward/dof_acc_l2: -0.1451
     Episode_Reward/action_rate_l2: -0.0651
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6344
Metrics/base_velocity/error_vel_xy: 0.4675
Metrics/base_velocity/error_vel_yaw: 0.3458
      Episode_Termination/time_out: 0.6994
  Episode_Termination/base_contact: 0.3006
--------------------------------------------------------------------------------
                   Total timesteps: 33742848
                    Iteration time: 2.51s
                      Time elapsed: 00:58:08
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1373/1400 [0m                     

                       Computation: 9861 steps/s (collection: 2.237s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 5.7628
                       Mean reward: 12.09
               Mean episode length: 863.52
Episode_Reward/track_lin_vel_xy_exp: 0.6306
Episode_Reward/track_ang_vel_z_exp: 0.3483
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.0551
     Episode_Reward/dof_torques_l2: -0.0661
         Episode_Reward/dof_acc_l2: -0.1566
     Episode_Reward/action_rate_l2: -0.0642
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6472
Metrics/base_velocity/error_vel_xy: 0.4725
Metrics/base_velocity/error_vel_yaw: 0.3497
      Episode_Termination/time_out: 0.7000
  Episode_Termination/base_contact: 0.3000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 2.49s
                      Time elapsed: 00:58:10
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1374/1400 [0m                     

                       Computation: 9967 steps/s (collection: 2.211s, learning 0.255s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 5.7663
                       Mean reward: 11.73
               Mean episode length: 850.56
Episode_Reward/track_lin_vel_xy_exp: 0.6318
Episode_Reward/track_ang_vel_z_exp: 0.3530
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0652
         Episode_Reward/dof_acc_l2: -0.1539
     Episode_Reward/action_rate_l2: -0.0646
      Episode_Reward/feet_air_time: -0.0101
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6459
Metrics/base_velocity/error_vel_xy: 0.4871
Metrics/base_velocity/error_vel_yaw: 0.3467
      Episode_Termination/time_out: 0.7014
  Episode_Termination/base_contact: 0.2986
--------------------------------------------------------------------------------
                   Total timesteps: 33792000
                    Iteration time: 2.47s
                      Time elapsed: 00:58:13
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1375/1400 [0m                     

                       Computation: 9921 steps/s (collection: 2.215s, learning 0.262s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.7751
                       Mean reward: 11.90
               Mean episode length: 863.34
Episode_Reward/track_lin_vel_xy_exp: 0.6548
Episode_Reward/track_ang_vel_z_exp: 0.3622
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.0595
     Episode_Reward/dof_torques_l2: -0.0692
         Episode_Reward/dof_acc_l2: -0.1672
     Episode_Reward/action_rate_l2: -0.0661
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6560
Metrics/base_velocity/error_vel_xy: 0.5065
Metrics/base_velocity/error_vel_yaw: 0.3740
      Episode_Termination/time_out: 0.7037
  Episode_Termination/base_contact: 0.2963
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.48s
                      Time elapsed: 00:58:15
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1376/1400 [0m                     

                       Computation: 9890 steps/s (collection: 2.229s, learning 0.256s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 5.7859
                       Mean reward: 11.86
               Mean episode length: 870.00
Episode_Reward/track_lin_vel_xy_exp: 0.5717
Episode_Reward/track_ang_vel_z_exp: 0.3166
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.0543
     Episode_Reward/dof_torques_l2: -0.0620
         Episode_Reward/dof_acc_l2: -0.1597
     Episode_Reward/action_rate_l2: -0.0604
      Episode_Reward/feet_air_time: -0.0097
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6635
Metrics/base_velocity/error_vel_xy: 0.4842
Metrics/base_velocity/error_vel_yaw: 0.3695
      Episode_Termination/time_out: 0.7057
  Episode_Termination/base_contact: 0.2943
--------------------------------------------------------------------------------
                   Total timesteps: 33841152
                    Iteration time: 2.48s
                      Time elapsed: 00:58:18
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1377/1400 [0m                     

                       Computation: 9969 steps/s (collection: 2.200s, learning 0.265s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 5.7928
                       Mean reward: 11.30
               Mean episode length: 849.04
Episode_Reward/track_lin_vel_xy_exp: 0.5987
Episode_Reward/track_ang_vel_z_exp: 0.3509
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.0562
     Episode_Reward/dof_torques_l2: -0.0656
         Episode_Reward/dof_acc_l2: -0.1711
     Episode_Reward/action_rate_l2: -0.0649
      Episode_Reward/feet_air_time: -0.0106
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6635
Metrics/base_velocity/error_vel_xy: 0.5632
Metrics/base_velocity/error_vel_yaw: 0.3812
      Episode_Termination/time_out: 0.7066
  Episode_Termination/base_contact: 0.2934
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 2.47s
                      Time elapsed: 00:58:20
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1378/1400 [0m                     

                       Computation: 9888 steps/s (collection: 2.228s, learning 0.257s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 5.8014
                       Mean reward: 11.14
               Mean episode length: 847.06
Episode_Reward/track_lin_vel_xy_exp: 0.6489
Episode_Reward/track_ang_vel_z_exp: 0.3570
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.0550
     Episode_Reward/dof_torques_l2: -0.0662
         Episode_Reward/dof_acc_l2: -0.1550
     Episode_Reward/action_rate_l2: -0.0653
      Episode_Reward/feet_air_time: -0.0103
 Episode_Reward/undesired_contacts: -0.0061
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6611
Metrics/base_velocity/error_vel_xy: 0.4895
Metrics/base_velocity/error_vel_yaw: 0.3671
      Episode_Termination/time_out: 0.7071
  Episode_Termination/base_contact: 0.2929
--------------------------------------------------------------------------------
                   Total timesteps: 33890304
                    Iteration time: 2.49s
                      Time elapsed: 00:58:23
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1379/1400 [0m                     

                       Computation: 9791 steps/s (collection: 2.255s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 5.8144
                       Mean reward: 10.80
               Mean episode length: 837.68
Episode_Reward/track_lin_vel_xy_exp: 0.6233
Episode_Reward/track_ang_vel_z_exp: 0.3397
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.0495
     Episode_Reward/dof_torques_l2: -0.0592
         Episode_Reward/dof_acc_l2: -0.1437
     Episode_Reward/action_rate_l2: -0.0604
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6613
Metrics/base_velocity/error_vel_xy: 0.4348
Metrics/base_velocity/error_vel_yaw: 0.3360
      Episode_Termination/time_out: 0.7078
  Episode_Termination/base_contact: 0.2922
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.51s
                      Time elapsed: 00:58:25
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1380/1400 [0m                     

                       Computation: 9728 steps/s (collection: 2.275s, learning 0.252s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 5.8294
                       Mean reward: 11.45
               Mean episode length: 836.62
Episode_Reward/track_lin_vel_xy_exp: 0.6236
Episode_Reward/track_ang_vel_z_exp: 0.3459
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.0570
     Episode_Reward/dof_torques_l2: -0.0633
         Episode_Reward/dof_acc_l2: -0.1540
     Episode_Reward/action_rate_l2: -0.0631
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0094
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6672
Metrics/base_velocity/error_vel_xy: 0.4937
Metrics/base_velocity/error_vel_yaw: 0.3704
      Episode_Termination/time_out: 0.7063
  Episode_Termination/base_contact: 0.2937
--------------------------------------------------------------------------------
                   Total timesteps: 33939456
                    Iteration time: 2.53s
                      Time elapsed: 00:58:28
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1381/1400 [0m                     

                       Computation: 9900 steps/s (collection: 2.228s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 5.8367
                       Mean reward: 11.05
               Mean episode length: 825.43
Episode_Reward/track_lin_vel_xy_exp: 0.5657
Episode_Reward/track_ang_vel_z_exp: 0.3171
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0531
     Episode_Reward/dof_torques_l2: -0.0581
         Episode_Reward/dof_acc_l2: -0.1393
     Episode_Reward/action_rate_l2: -0.0590
      Episode_Reward/feet_air_time: -0.0100
 Episode_Reward/undesired_contacts: -0.0075
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6763
Metrics/base_velocity/error_vel_xy: 0.5000
Metrics/base_velocity/error_vel_yaw: 0.3663
      Episode_Termination/time_out: 0.7051
  Episode_Termination/base_contact: 0.2949
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 2.48s
                      Time elapsed: 00:58:30
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1382/1400 [0m                     

                       Computation: 9848 steps/s (collection: 2.238s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.8401
                       Mean reward: 11.23
               Mean episode length: 837.75
Episode_Reward/track_lin_vel_xy_exp: 0.6392
Episode_Reward/track_ang_vel_z_exp: 0.3581
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.0547
     Episode_Reward/dof_torques_l2: -0.0670
         Episode_Reward/dof_acc_l2: -0.1599
     Episode_Reward/action_rate_l2: -0.0658
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6699
Metrics/base_velocity/error_vel_xy: 0.4982
Metrics/base_velocity/error_vel_yaw: 0.3594
      Episode_Termination/time_out: 0.7077
  Episode_Termination/base_contact: 0.2923
--------------------------------------------------------------------------------
                   Total timesteps: 33988608
                    Iteration time: 2.50s
                      Time elapsed: 00:58:33
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1383/1400 [0m                     

                       Computation: 9608 steps/s (collection: 2.302s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 5.8567
                       Mean reward: 11.60
               Mean episode length: 847.18
Episode_Reward/track_lin_vel_xy_exp: 0.6715
Episode_Reward/track_ang_vel_z_exp: 0.3634
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.0536
     Episode_Reward/dof_torques_l2: -0.0631
         Episode_Reward/dof_acc_l2: -0.1547
     Episode_Reward/action_rate_l2: -0.0647
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6787
Metrics/base_velocity/error_vel_xy: 0.4474
Metrics/base_velocity/error_vel_yaw: 0.3416
      Episode_Termination/time_out: 0.7105
  Episode_Termination/base_contact: 0.2895
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.56s
                      Time elapsed: 00:58:35
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1384/1400 [0m                     

                       Computation: 9834 steps/s (collection: 2.242s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 5.8577
                       Mean reward: 11.87
               Mean episode length: 862.36
Episode_Reward/track_lin_vel_xy_exp: 0.6454
Episode_Reward/track_ang_vel_z_exp: 0.3462
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.0544
     Episode_Reward/dof_torques_l2: -0.0723
         Episode_Reward/dof_acc_l2: -0.1600
     Episode_Reward/action_rate_l2: -0.0657
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0216
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6833
Metrics/base_velocity/error_vel_xy: 0.4608
Metrics/base_velocity/error_vel_yaw: 0.3749
      Episode_Termination/time_out: 0.7119
  Episode_Termination/base_contact: 0.2881
--------------------------------------------------------------------------------
                   Total timesteps: 34037760
                    Iteration time: 2.50s
                      Time elapsed: 00:58:38
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1385/1400 [0m                     

                       Computation: 9703 steps/s (collection: 2.278s, learning 0.254s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.8616
                       Mean reward: 11.79
               Mean episode length: 858.01
Episode_Reward/track_lin_vel_xy_exp: 0.6204
Episode_Reward/track_ang_vel_z_exp: 0.3478
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.0521
     Episode_Reward/dof_torques_l2: -0.0607
         Episode_Reward/dof_acc_l2: -0.1411
     Episode_Reward/action_rate_l2: -0.0635
      Episode_Reward/feet_air_time: -0.0100
 Episode_Reward/undesired_contacts: -0.0186
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6915
Metrics/base_velocity/error_vel_xy: 0.5077
Metrics/base_velocity/error_vel_yaw: 0.3712
      Episode_Termination/time_out: 0.7084
  Episode_Termination/base_contact: 0.2916
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 2.53s
                      Time elapsed: 00:58:40
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1386/1400 [0m                     

                       Computation: 9706 steps/s (collection: 2.277s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 5.8630
                       Mean reward: 11.82
               Mean episode length: 859.44
Episode_Reward/track_lin_vel_xy_exp: 0.6193
Episode_Reward/track_ang_vel_z_exp: 0.3535
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.0542
     Episode_Reward/dof_torques_l2: -0.0667
         Episode_Reward/dof_acc_l2: -0.1631
     Episode_Reward/action_rate_l2: -0.0656
      Episode_Reward/feet_air_time: -0.0106
 Episode_Reward/undesired_contacts: -0.0117
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6997
Metrics/base_velocity/error_vel_xy: 0.5127
Metrics/base_velocity/error_vel_yaw: 0.3532
      Episode_Termination/time_out: 0.7092
  Episode_Termination/base_contact: 0.2908
--------------------------------------------------------------------------------
                   Total timesteps: 34086912
                    Iteration time: 2.53s
                      Time elapsed: 00:58:43
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1387/1400 [0m                     

                       Computation: 9822 steps/s (collection: 2.237s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.8735
                       Mean reward: 11.50
               Mean episode length: 838.27
Episode_Reward/track_lin_vel_xy_exp: 0.6258
Episode_Reward/track_ang_vel_z_exp: 0.3449
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0548
     Episode_Reward/dof_torques_l2: -0.0618
         Episode_Reward/dof_acc_l2: -0.1582
     Episode_Reward/action_rate_l2: -0.0627
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7041
Metrics/base_velocity/error_vel_xy: 0.4968
Metrics/base_velocity/error_vel_yaw: 0.3742
      Episode_Termination/time_out: 0.7090
  Episode_Termination/base_contact: 0.2910
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.50s
                      Time elapsed: 00:58:45
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1388/1400 [0m                     

                       Computation: 9713 steps/s (collection: 2.274s, learning 0.256s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 5.8715
                       Mean reward: 12.21
               Mean episode length: 848.09
Episode_Reward/track_lin_vel_xy_exp: 0.6745
Episode_Reward/track_ang_vel_z_exp: 0.3602
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.0505
     Episode_Reward/dof_torques_l2: -0.0620
         Episode_Reward/dof_acc_l2: -0.1426
     Episode_Reward/action_rate_l2: -0.0640
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6947
Metrics/base_velocity/error_vel_xy: 0.4092
Metrics/base_velocity/error_vel_yaw: 0.3198
      Episode_Termination/time_out: 0.7171
  Episode_Termination/base_contact: 0.2829
--------------------------------------------------------------------------------
                   Total timesteps: 34136064
                    Iteration time: 2.53s
                      Time elapsed: 00:58:48
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1389/1400 [0m                     

                       Computation: 9836 steps/s (collection: 2.242s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.8668
                       Mean reward: 11.33
               Mean episode length: 797.89
Episode_Reward/track_lin_vel_xy_exp: 0.5080
Episode_Reward/track_ang_vel_z_exp: 0.2811
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.0427
     Episode_Reward/dof_torques_l2: -0.0513
         Episode_Reward/dof_acc_l2: -0.1311
     Episode_Reward/action_rate_l2: -0.0516
      Episode_Reward/feet_air_time: -0.0075
 Episode_Reward/undesired_contacts: -0.0098
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6982
Metrics/base_velocity/error_vel_xy: 0.3650
Metrics/base_velocity/error_vel_yaw: 0.2674
      Episode_Termination/time_out: 0.7167
  Episode_Termination/base_contact: 0.2833
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 2.50s
                      Time elapsed: 00:58:50
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1390/1400 [0m                     

                       Computation: 9944 steps/s (collection: 2.209s, learning 0.262s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 5.8720
                       Mean reward: 11.11
               Mean episode length: 781.04
Episode_Reward/track_lin_vel_xy_exp: 0.4066
Episode_Reward/track_ang_vel_z_exp: 0.2343
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0398
     Episode_Reward/dof_torques_l2: -0.0481
         Episode_Reward/dof_acc_l2: -0.1004
     Episode_Reward/action_rate_l2: -0.0444
      Episode_Reward/feet_air_time: -0.0069
 Episode_Reward/undesired_contacts: -0.0157
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6927
Metrics/base_velocity/error_vel_xy: 0.3916
Metrics/base_velocity/error_vel_yaw: 0.2930
      Episode_Termination/time_out: 0.7179
  Episode_Termination/base_contact: 0.2821
--------------------------------------------------------------------------------
                   Total timesteps: 34185216
                    Iteration time: 2.47s
                      Time elapsed: 00:58:53
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1391/1400 [0m                     

                       Computation: 9755 steps/s (collection: 2.264s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0162
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 5.8761
                       Mean reward: 10.74
               Mean episode length: 778.55
Episode_Reward/track_lin_vel_xy_exp: 0.6861
Episode_Reward/track_ang_vel_z_exp: 0.3753
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.0584
     Episode_Reward/dof_torques_l2: -0.0647
         Episode_Reward/dof_acc_l2: -0.1643
     Episode_Reward/action_rate_l2: -0.0676
      Episode_Reward/feet_air_time: -0.0095
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6995
Metrics/base_velocity/error_vel_xy: 0.4834
Metrics/base_velocity/error_vel_yaw: 0.3606
      Episode_Termination/time_out: 0.7186
  Episode_Termination/base_contact: 0.2814
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.52s
                      Time elapsed: 00:58:55
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1392/1400 [0m                     

                       Computation: 9711 steps/s (collection: 2.276s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 5.8901
                       Mean reward: 10.77
               Mean episode length: 791.14
Episode_Reward/track_lin_vel_xy_exp: 0.5673
Episode_Reward/track_ang_vel_z_exp: 0.3122
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.0487
     Episode_Reward/dof_torques_l2: -0.0587
         Episode_Reward/dof_acc_l2: -0.1388
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7031
Metrics/base_velocity/error_vel_xy: 0.4316
Metrics/base_velocity/error_vel_yaw: 0.3216
      Episode_Termination/time_out: 0.7197
  Episode_Termination/base_contact: 0.2803
--------------------------------------------------------------------------------
                   Total timesteps: 34234368
                    Iteration time: 2.53s
                      Time elapsed: 00:58:58
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1393/1400 [0m                     

                       Computation: 9575 steps/s (collection: 2.303s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 5.8933
                       Mean reward: 10.66
               Mean episode length: 800.50
Episode_Reward/track_lin_vel_xy_exp: 0.5729
Episode_Reward/track_ang_vel_z_exp: 0.3290
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.0529
     Episode_Reward/dof_torques_l2: -0.0635
         Episode_Reward/dof_acc_l2: -0.1513
     Episode_Reward/action_rate_l2: -0.0622
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0150
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7145
Metrics/base_velocity/error_vel_xy: 0.5060
Metrics/base_velocity/error_vel_yaw: 0.3458
      Episode_Termination/time_out: 0.7196
  Episode_Termination/base_contact: 0.2804
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 2.57s
                      Time elapsed: 00:59:00
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1394/1400 [0m                     

                       Computation: 9721 steps/s (collection: 2.273s, learning 0.255s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 5.8950
                       Mean reward: 10.55
               Mean episode length: 804.89
Episode_Reward/track_lin_vel_xy_exp: 0.5589
Episode_Reward/track_ang_vel_z_exp: 0.3138
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.0495
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1552
     Episode_Reward/action_rate_l2: -0.0598
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7234
Metrics/base_velocity/error_vel_xy: 0.4575
Metrics/base_velocity/error_vel_yaw: 0.3447
      Episode_Termination/time_out: 0.7177
  Episode_Termination/base_contact: 0.2823
--------------------------------------------------------------------------------
                   Total timesteps: 34283520
                    Iteration time: 2.53s
                      Time elapsed: 00:59:03
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1395/1400 [0m                     

                       Computation: 9628 steps/s (collection: 2.295s, learning 0.257s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 5.8883
                       Mean reward: 10.55
               Mean episode length: 794.04
Episode_Reward/track_lin_vel_xy_exp: 0.6855
Episode_Reward/track_ang_vel_z_exp: 0.3596
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.0523
     Episode_Reward/dof_torques_l2: -0.0643
         Episode_Reward/dof_acc_l2: -0.1446
     Episode_Reward/action_rate_l2: -0.0647
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7196
Metrics/base_velocity/error_vel_xy: 0.4059
Metrics/base_velocity/error_vel_yaw: 0.3349
      Episode_Termination/time_out: 0.7138
  Episode_Termination/base_contact: 0.2862
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.55s
                      Time elapsed: 00:59:06
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1396/1400 [0m                     

                       Computation: 9548 steps/s (collection: 2.310s, learning 0.264s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 5.8783
                       Mean reward: 10.43
               Mean episode length: 783.98
Episode_Reward/track_lin_vel_xy_exp: 0.5520
Episode_Reward/track_ang_vel_z_exp: 0.3132
       Episode_Reward/lin_vel_z_l2: -0.0422
      Episode_Reward/ang_vel_xy_l2: -0.0510
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1515
     Episode_Reward/action_rate_l2: -0.0590
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7254
Metrics/base_velocity/error_vel_xy: 0.4710
Metrics/base_velocity/error_vel_yaw: 0.3272
      Episode_Termination/time_out: 0.7087
  Episode_Termination/base_contact: 0.2913
--------------------------------------------------------------------------------
                   Total timesteps: 34332672
                    Iteration time: 2.57s
                      Time elapsed: 00:59:08
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1397/1400 [0m                     

                       Computation: 9553 steps/s (collection: 2.307s, learning 0.265s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 5.8709
                       Mean reward: 10.76
               Mean episode length: 792.62
Episode_Reward/track_lin_vel_xy_exp: 0.5898
Episode_Reward/track_ang_vel_z_exp: 0.3263
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.0513
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1486
     Episode_Reward/action_rate_l2: -0.0607
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7196
Metrics/base_velocity/error_vel_xy: 0.4432
Metrics/base_velocity/error_vel_yaw: 0.3249
      Episode_Termination/time_out: 0.7072
  Episode_Termination/base_contact: 0.2928
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 2.57s
                      Time elapsed: 00:59:11
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1398/1400 [0m                     

                       Computation: 9568 steps/s (collection: 2.311s, learning 0.258s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 5.8748
                       Mean reward: 10.40
               Mean episode length: 793.73
Episode_Reward/track_lin_vel_xy_exp: 0.5823
Episode_Reward/track_ang_vel_z_exp: 0.3227
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.0531
     Episode_Reward/dof_torques_l2: -0.0612
         Episode_Reward/dof_acc_l2: -0.1564
     Episode_Reward/action_rate_l2: -0.0615
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7172
Metrics/base_velocity/error_vel_xy: 0.4568
Metrics/base_velocity/error_vel_yaw: 0.3358
      Episode_Termination/time_out: 0.7061
  Episode_Termination/base_contact: 0.2939
--------------------------------------------------------------------------------
                   Total timesteps: 34381824
                    Iteration time: 2.57s
                      Time elapsed: 00:59:13
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1399/1400 [0m                     

                       Computation: 9603 steps/s (collection: 2.299s, learning 0.260s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 5.8807
                       Mean reward: 10.76
               Mean episode length: 803.88
Episode_Reward/track_lin_vel_xy_exp: 0.6196
Episode_Reward/track_ang_vel_z_exp: 0.3432
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.0506
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1467
     Episode_Reward/action_rate_l2: -0.0627
      Episode_Reward/feet_air_time: -0.0091
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7144
Metrics/base_velocity/error_vel_xy: 0.4561
Metrics/base_velocity/error_vel_yaw: 0.3368
      Episode_Termination/time_out: 0.7008
  Episode_Termination/base_contact: 0.2992
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.56s
                      Time elapsed: 00:59:16
                               ETA: 00:00:02

Training time: 3601.71 seconds
